
















































































































[{"body":" The key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be interpreted as described in RFC 2119.\n Here is a list of Azure resource assumptions that are required for cloud provider Azure:\n All Azure resources MUST be under the same tenant. All virtual machine names MUST be the same as their hostname. Node LoadBalancer’s names SHOULD be following rules (\u003cclusterName\u003e is coming from --cluster-name configuration, default is kubernetes)  When enableMultipleStandardLoadBalancers is configured to false, LoadBalancer’s name SHOULD be \u003cclusterName\u003e for external type and \u003cclusterName\u003e-internal for internal type. When enableMultipleStandardLoadBalancers is configured to true, multiple standard load balancers SHOULD be provisioned:  All the virtual machines MUST be part of either VirtualMachineScaleSet (VMSS) or AvailabilitySet (VMAS). Each VMAS and VMSS SHOULD be put behind a different standard LoadBalancer. The primary LoadBalancer’s name SHOULD be \u003cclusterName\u003e for external type and \u003cclusterName\u003e-internal for internal type. Virtual machines that are part of primary VMAS (set by primaryAvailabilitySetName) or primary VMSS (set by primaryScaleSetName) SHOULD be added to primary LoadBalancer backend address pool. Other standard LoadBalancer’s name SHOULD be same as VMAS or VMSS name.     The cluster name set for kube-controller-manager --cluster-name=\u003ccluster-name\u003e MUST not end with -internal.  After the cluster is provisioned, cloud provider Azure MAY update the following Azure resources based on workloads:\n New routes would be added for each node if --configure-cloud-routes is enabled. New LoadBalancer (including external and internal) would be created if they’re not existing yet. Virtual machines and virtual machine scale sets would be added to LoadBalancer backend address pools if they’re not added yet. New public IPs and NSG rules would be added when LoadBalancer typed services are created.  ","categories":"","description":"Cloud provider assumptions on Azure resources that provisioning tools should follow.\n","excerpt":"Cloud provider assumptions on Azure resources that provisioning tools …","ref":"/cloud-provider-azure/topics/assumptions/","tags":"","title":"Cluster Provisioning Tools Contract"},{"body":"This doc describes cloud provider config file, which is to be used via the --cloud-config flag of azure-cloud-controller-manager.\nHere is a config file sample:\n{ \"cloud\":\"AzurePublicCloud\", \"tenantId\": \"0000000-0000-0000-0000-000000000000\", \"aadClientId\": \"0000000-0000-0000-0000-000000000000\", \"aadClientSecret\": \"0000000-0000-0000-0000-000000000000\", \"subscriptionId\": \"0000000-0000-0000-0000-000000000000\", \"resourceGroup\": \"\u003cname\u003e\", \"location\": \"eastus\", \"subnetName\": \"\u003cname\u003e\", \"securityGroupName\": \"\u003cname\u003e\", \"securityGroupResourceGroup\": \"\u003cname\u003e\", \"vnetName\": \"\u003cname\u003e\", \"vnetResourceGroup\": \"\u003cname\u003e\", \"routeTableName\": \"\u003cname\u003e\", \"primaryAvailabilitySetName\": \"\u003cname\u003e\", \"routeTableResourceGroup\": \"\u003cname\u003e\", \"cloudProviderBackoff\": false, \"useManagedIdentityExtension\": false, \"useInstanceMetadata\": true } Note: All values are of type string if not explicitly called out.\nAuth configs    Name Description Remark     cloud The cloud environment identifier Valid values could be found here. Default to AzurePublicCloud.   tenantID The AAD Tenant ID for the Subscription that the cluster is deployed in Required.   aadClientID The ClientID for an AAD application with RBAC access to talk to Azure RM APIs Used for service principal authn.   aadClientSecret The ClientSecret for an AAD application with RBAC access to talk to Azure RM APIs Used for service principal authn.   aadClientCertPath The path of a client certificate for an AAD application with RBAC access to talk to Azure RM APIs Used for client cert authn.   aadClientCertPassword The password of the client certificate for an AAD application with RBAC access to talk to Azure RM APIs Used for client cert authn.   useManagedIdentityExtension Use managed service identity for the virtual machine to access Azure ARM APIs Boolean type, default to false.   userAssignedIdentityID The Client ID of the user assigned MSI which is assigned to the underlying VMs Required for user-assigned managed identity.   subscriptionId The ID of the Azure Subscription that the cluster is deployed in Required.   identitySystem The identity system for AzureStack. Supported values are: ADFS Only used for AzureStack   networkResourceTenantID The AAD Tenant ID for the Subscription that the network resources are deployed in Optional. Supported since v1.18.0. Only used for hosting network resources in different AAD Tenant and Subscription than those for the cluster.   networkResourceSubscriptionID The ID of the Azure Subscription that the network resources are deployed in Optional. Supported since v1.18.0. Only used for hosting network resources in different AAD Tenant and Subscription than those for the cluster.    Note: Cloud provider currently supports three authentication methods, you can choose one combination of them:\n Managed Identity:  For system-assigned managed identity: set useManagedIdentityExtension to true For user-assigned managed identity: set useManagedIdentityExtension to true and also set userAssignedIdentityID   Service Principal: set aadClientID and aadClientSecret Client Certificate: set aadClientCertPath and aadClientCertPassword  If more than one value is set, the order is Managed Identity \u003e Service Principal \u003e Client Certificate.\nCluster config    Name Description Remark     resourceGroup The name of the resource group that the cluster is deployed in    location The location of the resource group that the cluster is deployed in    vnetName The name of the VNet that the cluster is deployed in    vnetResourceGroup The name of the resource group that the Vnet is deployed in    subnetName The name of the subnet that the cluster is deployed in    securityGroupName The name of the security group attached to the cluster’s subnet    securityGroupResourceGroup The name of the resource group that the security group is deployed in    routeTableName The name of the route table attached to the subnet that the cluster is deployed in Optional in 1.6   primaryAvailabilitySetName* The name of the availability set that should be used as the load balancer backend Optional   vmType The type of azure nodes. Candidate values are: vmss and standard Optional, default to standard   primaryScaleSetName* The name of the scale set that should be used as the load balancer backend Optional   cloudProviderBackoff Enable exponential backoff to manage resource request retries Boolean value, default to false   cloudProviderBackoffRetries Backoff retry limit Integer value, valid if cloudProviderBackoff is true   cloudProviderBackoffExponent Backoff exponent Float value, valid if cloudProviderBackoff is true   cloudProviderBackoffDuration Backoff duration Integer value, valid if cloudProviderBackoff is true   cloudProviderBackoffJitter Backoff jitter Float value, valid if cloudProviderBackoff is true   cloudProviderBackoffMode Backoff mode, supported values are “v2” and “default”. Note that “v2” has been deprecated since v1.18.0. Default to “default”   cloudProviderRateLimit Enable rate limiting Boolean value, default to false   cloudProviderRateLimitQPS Rate limit QPS (Read) Float value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitBucket Rate limit Bucket Size Integar value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitQPSWrite Rate limit QPS (Write) Float value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitBucketWrite Rate limit Bucket Size Integer value, valid if cloudProviderRateLimit is true   useInstanceMetadata Use instance metadata service where possible Boolean value, default to false   loadBalancerSku Sku of Load Balancer and Public IP. Candidate values are: basic and standard. Default to basic.   excludeMasterFromStandardLB ExcludeMasterFromStandardLB excludes master nodes from standard load balancer. Boolean value, default to true.   disableOutboundSNAT Disable outbound SNAT for SLB Default to false and available since v1.11.9, v1.12.7, v1.13.5 and v1.14.0   maximumLoadBalancerRuleCount Maximum allowed LoadBalancer Rule Count is the limit enforced by Azure Load balancer Integer value, default to 148   routeTableResourceGroup The resource group name for routeTable Default same as resourceGroup and available since v1.15.0   loadBalancerName Working together with loadBalancerResourceGroup to determine the LB name in a different resource group Since v1.18.0, default is cluster name setting on kube-controller-manager   loadBalancerResourceGroup The load balancer resource group name, which is different from node resource group Since v1.18.0, default is same as resourceGroup   disableAvailabilitySetNodes Disable supporting for AvailabilitySet virtual machines in vmss cluster. It should be only used when vmType is “vmss” and all the nodes (including master) are VMSS virtual machines Since v1.18.0, default is false   availabilitySetNodesCacheTTLInSeconds Cache TTL in seconds for availabilitySet Nodes Since v1.18.0, default is 900   vmssCacheTTLInSeconds Cache TTL in seconds for VMSS Since v1.18.0, default is 600   vmssVirtualMachinesCacheTTLInSeconds Cache TTL in seconds for VMSS virtual machines Since v1.18.0, default is 600   vmCacheTTLInSeconds Cache TTL in seconds for virtual machines Since v1.18.0, default is 60   loadBalancerCacheTTLInSeconds Cache TTL in seconds for load balancers Since v1.18.0, default is 120   nsgCacheTTLInSeconds Cache TTL in seconds for network security group Since v1.18.0, default is 120   routeTableCacheTTLInSeconds Cache TTL in seconds for route table Since v1.18.0, default is 120   disableAzureStackCloud DisableAzureStackCloud disables AzureStackCloud support. It should be used when setting Cloud with “AZURESTACKCLOUD” to customize ARM endpoints while the cluster is not running on AzureStack. Default is false. Optional. Supported since v1.20.0 in out-of-tree cloud provider Azure.   tags Tags that would be tagged onto the cloud provider managed resources, including lb, public IP, network security group and route table. Optional. Supported since v1.20.0.   tagsMap JSON-style tags, will be merged with tags Optional. Supported since v1.23.0.   systemTags Tag keys that should not be deleted when being updated. Optional. Supported since v1.21.0.   enableMultipleStandardLoadBalancers Enable multiple standard Load Balancers per cluster. Optional. Supported since v1.20.0   loadBalancerBackendPoolConfigurationType The type of the Load Balancer backend pool. Supported values are nodeIPConfiguration (default) and nodeIP Optional. Supported since v1.23.0   putVMSSVMBatchSize The number of requests the client sends concurrently in a batch when putting the VMSS VMs. Anything smaller than or equal to 0 means to update VMSS VMs one by one in sequence. Optional. Supported since v1.24.0.    primaryAvailabilitySetName If this is set, the Azure cloudprovider will only add nodes from that availability set to the load balancer backend pool. If this is not set, and multiple agent pools (availability sets) are used, then the cloudprovider will try to add all nodes to a single backend pool which is forbidden. In other words, if you use multiple agent pools (availability sets), you MUST set this field.\nprimaryScaleSetName If this is set, the Azure cloudprovider will only add nodes from that scale set to the load balancer backend pool. If this is not set, and multiple agent pools (scale sets) are used, then the cloudprovider will try to add all nodes to a single backend pool which is forbidden when using Load Balancer Basic SKU. In other words, if you use multiple agent pools (scale sets), and loadBalancerSku is set to basic you MUST set this field.\nexcludeMasterFromStandardLB Master nodes are not added to the backends of Azure Load Balancer (ALB) if excludeMasterFromStandardLB is set.\nBy default, if nodes are labeled with node-role.kubernetes.io/master, they would also be excluded from ALB. If you want to add the master nodes to ALB, excludeMasterFromStandardLB should be set to false and label node-role.kubernetes.io/master should be removed if it has already been applied.\nDynamically reloading cloud controller manager Since v1.21.0, Azure cloud provider supports reading the cloud config from Kubernetes secrets. The secret is a serialized version of azure.json file. When the secret is changed, the cloud controller manager will re-constructing itself without restarting the pod.\nTo enable this feature, set --enable-dynamic-reloading=true and configure the secret name, namespace and data key by --cloud-config-secret-name, --cloud-config-secret-namespace and --cloud-config-key. When initializing from secret, the --cloud-config should not be set.\n Note that the --enable-dynamic-reloading cannot be false if --cloud-config is empty. To build the cloud provider from classic config file, please explicitly specify the --cloud-config and do not set --enable-dynamic-reloading=true. In this manner, the cloud controller manager will not be updated when the config file is changed. You need to restart the pod to manually trigger the re-initialization.\n Since Azure cloud provider would read Kubernetes secrets, the following RBAC should also be configured:\n---apiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRolemetadata:labels:kubernetes.io/cluster-service:\"true\"name:system:azure-cloud-provider-secret-getterrules:- apiGroups:[\"\"]resources:[\"secrets\"]resourceNames:[\"azure-cloud-provider\"]verbs:- get---apiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRoleBindingmetadata:labels:kubernetes.io/cluster-service:\"true\"name:system:azure-cloud-provider-secret-getterroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:system:azure-cloud-provider-secret-gettersubjects:- kind:ServiceAccountname:azure-cloud-providernamespace:kube-systemIt is also supported to build the cloud controller manager from the cloud config file and reload dynamically. To use this way, turn on --enable-dynamic-reloading and set --cloud-config to an non-empty value.\nper client rate limiting Since v1.18.0, the original global rate limiting has been switched to per-client. A set of new rate limit configure options are introduced for each client, which includes:\n routeRateLimit SubnetsRateLimit InterfaceRateLimit RouteTableRateLimit LoadBalancerRateLimit PublicIPAddressRateLimit SecurityGroupRateLimit VirtualMachineRateLimit StorageAccountRateLimit DiskRateLimit SnapshotRateLimit VirtualMachineScaleSetRateLimit VirtualMachineSizeRateLimit  The original rate limiting options (“cloudProviderRateLimitBucket”, “cloudProviderRateLimitBucketWrite”, “cloudProviderRateLimitQPS”, “cloudProviderRateLimitQPSWrite”) are still supported, and they would be the default values if per-client rate limiting is not configured.\nHere is an example of per-client config:\n{ // default rate limit (enabled). \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitBucket\": 1, \"cloudProviderRateLimitBucketWrite\": 1, \"cloudProviderRateLimitQPS\": 1, \"cloudProviderRateLimitQPSWrite\": 1, \"virtualMachineScaleSetRateLimit\": { // VMSS specific (enabled). \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitBucket\": 2, \"CloudProviderRateLimitBucketWrite\": 2, \"cloudProviderRateLimitQPS\": 0, \"CloudProviderRateLimitQPSWrite\": 0 }, \"loadBalancerRateLimit\": { // LB specific (disabled) \"cloudProviderRatelimit\": false }, ... // other cloud provider configs } Run Kubelet without Azure identity When running Kubelet with kube-controller-manager, it also supports running without Azure identity since v1.15.0.\nBoth kube-controller-manager and kubelet should configure --cloud-provider=azure --cloud-config=/etc/kubernetes/cloud-config/azure.json, but the contents for azure.json are different:\n(1) For kube-controller-manager, refer the above part for setting azure.json.\n(2) For kubelet, useInstanceMetadata is required to be true and Azure identities are not required. A sample for Kubelet’s azure.json is\n{ \"useInstanceMetadata\": true, \"vmType\": \"vmss\" } Azure Stack Configuration Azure Stack has different API endpoints, depending on the Azure Stack deployment. These need to be provided to the Azure SDK and currently this is done by adding an extra json file with the arguments, as well as an environment variable pointing to this file.\nThere are several available presets, namely:\n AzureChinaCloud AzureGermanCloud AzurePublicCloud AzureUSGovernmentCloud  These are determined using cloud: \u003cPRESET\u003e described above in the description of azure.json.\nWhen cloud: AzureStackCloud, the extra environment variable used by the Azure SDK to find the Azure Stack configuration file is:\n AZURE_ENVIRONMENT_FILEPATH  The configuration parameters of this file:\n{ \"name\": \"AzureStackCloud\", \"managementPortalURL\": \"...\", \"publishSettingsURL\": \"...\", \"serviceManagementEndpoint\": \"...\", \"resourceManagerEndpoint\": \"...\", \"activeDirectoryEndpoint\": \"...\", \"galleryEndpoint\": \"...\", \"keyVaultEndpoint\": \"...\", \"graphEndpoint\": \"...\", \"serviceBusEndpoint\": \"...\", \"batchManagementEndpoint\": \"...\", \"storageEndpointSuffix\": \"...\", \"sqlDatabaseDNSSuffix\": \"...\", \"trafficManagerDNSSuffix\": \"...\", \"keyVaultDNSSuffix\": \"...\", \"serviceBusEndpointSuffix\": \"...\", \"serviceManagementVMDNSSuffix\": \"...\", \"resourceManagerVMDNSSuffix\": \"...\", \"containerRegistryDNSSuffix\": \"...\", \"cosmosDBDNSSuffix\": \"...\", \"tokenAudience\": \"...\", \"resourceIdentifiers\": { \"graph\": \"...\", \"keyVault\": \"...\", \"datalake\": \"...\", \"batch\": \"...\", \"operationalInsights\": \"...\" } } The full list of existing settings for the AzureChinaCloud, AzureGermanCloud, AzurePublicCloud and AzureUSGovernmentCloud is available in the source code at https://github.com/Azure/go-autorest/blob/master/autorest/azure/environments.go#L51.\nHost Network Resources in different AAD Tenant and Subscription Since v1.18.0, Azure cloud provider supports hosting network resources (Virtual Network, Network Security Group, Route Table, Load Balancer and Public IP) in different AAD Tenant and Subscription than those for the cluster. To enable this feature, set networkResourceTenantID and networkResourceSubscriptionID in auth config. Note that the value of them need to be different than value of tenantID and subscriptionID.\nWith this feature enabled, network resources of the cluster will be created in networkResourceSubscriptionID in networkResourceTenantID, and rest resources of the cluster still remain in subscriptionID in tenantID. Properties which specify the resource groups of network resources are compatible with this feature. For example, Virtual Network will be created in vnetResourceGroup in networkResourceSubscriptionID in networkResourceTenantID.\nFor authentication methods, only Service Principal supports this feature, and aadClientID and aadClientSecret are used to authenticate with those two AAD Tenants and Subscriptions. Managed Identity and Client Certificate doesn’t support this feature. Azure Stack doesn’t support this feature.\nCurrent default rate-limiting values The following are the default rate limiting values configured in AKS and AKS-Engine clusters prior to Kubernetes version v1.18.0.\n\"cloudProviderBackoff\": true, \"cloudProviderBackoffRetries\": 6, \"cloudProviderBackoffDuration\": 5, \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitQPS\": 10, \"cloudProviderRateLimitBucket\": 100, \"cloudProviderRatelimitQPSWrite\": 10, \"cloudProviderRatelimitBucketWrite\": 100, For v1.18.0+ refer to per client rate limit config\n","categories":"","description":"The configurations for Cloud Provider Azure.\n","excerpt":"The configurations for Cloud Provider Azure.\n","ref":"/cloud-provider-azure/install/configs/","tags":"","title":"Configure Cloud Provider"},{"body":"Thanks for taking the time to join our community and start contributing!\nThe Contributor Guide provides detailed instructions on how to get your ideas and bug fixes seen and accepted.\nPlease remember to sign the CNCF CLA and read and observe the Code of Conduct.\n","categories":"","description":"Developer guidance.\n","excerpt":"Developer guidance.\n","ref":"/cloud-provider-azure/contribute/contributing/","tags":"","title":"Contributing"},{"body":"Switch to the project root directory and run the following command to build both CCM and CNM images:\nmake image If you want to build only one of them, try make build-ccm-image or ARCH=amd64 make build-node-image-linux.\nTo push the images to your own image registry, you can specify the registry and image tag while building:\nIMAGE_REGISTRY=\u003cimage registry name\u003e IMAGE_TAG=\u003ctag name\u003e make image After building, you can push them to your image registry by make push.\nPlease follow here to build multi-arch image\n","categories":"","description":"Deploy a cluster with customized CCM or CNM images.\n","excerpt":"Deploy a cluster with customized CCM or CNM images.\n","ref":"/cloud-provider-azure/development/custom-images/","tags":"","title":"Deploy with Customized Images"},{"body":"To deploy an In-tree Cloud Provider Azure, all you need to do is deploy a cluster using AKS-Engine with the API model defined here. The AKS-Engine will automatically deploy the Kubernetes components needed and you don’t have to deploy them manually. However, customization is possible by modifying the manifests in /etc/kubernetes on master node. Here are the examples:\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nTo customize kubelet, you need to modify the starting command like here.\n","categories":"","description":"Deploy a cluster with In-tree Cloud Provider Azure.\n","excerpt":"Deploy a cluster with In-tree Cloud Provider Azure.\n","ref":"/cloud-provider-azure/example/in-tree/","tags":"","title":"Deploy with In-tree Cloud Provider Azure"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cloud-provider-azure/blog/releases/","tags":"","title":"Release Notes"},{"body":"azure-cloud-controller-manager is a Kubernetes component which provides interoperability with Azure API, and will be used by Kubernetes clusters running on Azure. It runs together with other components to provide the Kubernetes cluster’s control plane.\nUsing cloud-controller-manager is a new alpha feature for Kubernetes since v1.14. cloud-controller-manager runs cloud provider related controller loops, which used to be run by controller-manager.\nazure-cloud-controller-manager is a specialization of cloud-controller-manager. It depends on cloud-controller-manager app and azure cloud provider.\nDeployment To deploy Azure cloud controller manager, the following components need to be configured.\nkubelet    Flag Value Remark     --cloud-provider external cloud-provider should be set external   --azure-container-registry-config /etc/kubernetes/cloud-config/azure.json Used for Azure credential provider    kube-controller-manager    Flag Value Remark     --cloud-provider external cloud-provider should be set external   --external-cloud-volume-plugin azure Optional*    *: Since cloud controller manager does not support volume controllers, it will not provide volume capabilities compared to using previous built-in cloud provider case. You can add this flag to turn on volume controller for in-tree cloud providers. This option is likely to be removed with in-tree cloud providers in future.\nkube-apiserver Do not set flag --cloud-provider.\nazure-cloud-controller-manager azure-cloud-controller-manager should be run as Deployment with multiple replicas or Kubelet static Pods on each master Node.\n   Flag Value Remark     --cloud-provider azure cloud-provider should be set azure   --cloud-config /etc/kubernetes/cloud-config/azure.json Path for cloud provider config   --controllers *,-cloud-node cloud node controller should be disabled   --configure-cloud-routes “false” for Azure CNI and “true” for other network plugins Used for non-AzureCNI clusters    For other flags such as --allocate-node-cidrs, --cluster-cidr and --cluster-name, they are moved from kube-controller-manager. If you are migrating from kube-controller-manager, they should be set to same value.\nFor details of those flags, please refer to this doc.\nazure-cloud-node-manager azure-cloud-node-manager should be run as daemonsets on both Windows and Linux nodes, and the following configurations should be set:\n   Flag Value Remark     --node-name The node name for the Pod Kubernetes Downward API could be used to get Pod’s name   --wait-routes only set to true when --configure-cloud-routes=true in cloud-controller-manager Used for non-AzureCNI clusters    Please refer examples here for sample deployment manifests for above components.\nAlternatively, you can use aks-engine to deploy a Kubernetes cluster running with cloud-controller-manager. It supports deploying Kubernetes azure-cloud-controller-manager for Kubernetes v1.16+.\nAzureDisk and AzureFile AzureDisk and AzureFile volume plugins are not supported with in-tree cloud provider (See kubernetes/kubernetes#71018 for explanations).\nHence, azuredisk-csi-driver and azurefile-csi-driver should be used for persistent volumes. Please refer the installation guides here and here for their deployments.\nChange default storage class Follow the steps bellow if you want change the current default storage class to AzureDisk CSI driver.\nFirst, delete the default storage class:\nkubectl delete storageclass default Then create a new storage class named default:\ncat \u003c\u003cEOF | kubectl apply -f- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.beta.kubernetes.io/is-default-class: \"true\" name: default provisioner: disk.csi.azure.com parameters: skuname: Standard_LRS # available values: Standard_LRS, Premium_LRS, StandardSSD_LRS and UltraSSD_LRS kind: managed # value \"dedicated\", \"shared\" are deprecated since it's using unmanaged disk cachingMode: ReadOnly reclaimPolicy: Delete volumeBindingMode: Immediate EOF ","categories":"","description":"The configurations for using Azure Cloud Controller Manager.\n","excerpt":"The configurations for using Azure Cloud Controller Manager.\n","ref":"/cloud-provider-azure/install/azure-ccm/","tags":"","title":"Deploy Cloud Controller Manager"},{"body":"The way Azure defines a LoadBalancer is different from GCE or AWS. Azure’s LB can have multiple frontend IP refs. GCE and AWS only allow one, if you want more, you would need multiple LBs. Since Public IP’s are not part of the LB in Azure, an NSG is not part of the LB in Azure either. However, you cannot delete them in parallel, a Public IP can only be deleted after the LB’s frontend IP ref is removed.\nThe different Azure Resources such as LB, Public IP, and NSG are the same tier of Azure resources and circular dependencies need to be avoided. In other words, they should only depend on service state.\nBy default the basic SKU is selected for a load balancer. Services can be annotated to allow auto selection of available load balancers. Service annotations can also be used to provide specific availability sets that host the load balancers. Note that in case of auto selection or specific availability set selection, services are currently not auto-reassigned to an available loadbalancer when the availability set is lost in case of downtime or cluster scale down.\nLoadBalancer annotations Below is a list of annotations supported for Kubernetes services with type LoadBalancer:\n   Annotation Value Description Kubernetes Version     service.beta.kubernetes.io/azure-load-balancer-internal true or false Specify whether the load balancer should be internal. It’s defaulting to public if not set. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-internal-subnet Name of the subnet Specify which subnet the internal load balancer should be bound to. It’s defaulting to the subnet configured in cloud config file if not set. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-mode auto, {vmset-name} Specify the Azure load balancer selection algorithm based on vm sets (VMSS or VMAS). There are currently three possible load balancer selection modes : default, auto or “{vmset-name}”. This is only working for basic LB or multiple standard LB (see below for how it works) v1.10.0 and later   service.beta.kubernetes.io/azure-dns-label-name Name of the PIP DNS label Specify the DNS label name for the service’s public IP address (PIP). If it is set to empty string, DNS in PIP would be deleted. Because of a bug, before v1.15.10/v1.16.7/v1.17.3, the DNS label on PIP would also be deleted if the annotation is not specified. v1.15.0 and later   service.beta.kubernetes.io/azure-shared-securityrule true or false Specify that the service should be exposed using an Azure security rule that may be shared with another service, trading specificity of rules for an increase in the number of services that can be exposed. This relies on the Azure “augmented security rules” feature. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-resource-group Name of the PIP resource group Specify the resource group of the service’s PIP that are not in the same resource group as the cluster. v1.10.0 and later   service.beta.kubernetes.io/azure-allowed-service-tags List of allowed service tags Specify a list of allowed service tags separated by comma. v1.11.0 and later   service.beta.kubernetes.io/azure-load-balancer-tcp-idle-timeout TCP idle timeouts in minutes Specify the time, in minutes, for TCP connection idle timeouts to occur on the load balancer. Default and minimum value is 4. Maximum value is 30. Must be an integer. v1.11.4, v1.12.0 and later   service.beta.kubernetes.io/azure-pip-name Name of PIP Specify the PIP that will be applied to load balancer v1.16 and later   service.beta.kubernetes.io/azure-pip-tags Tags of the PIP Specify the tags of the PIP that will be associated to the load balancer typed service. Doc v1.20 and later   service.beta.kubernetes.io/azure-load-balancer-health-probe-interval Health probe interval Refer to the detailed docs here v1.21 and later with out-of-tree cloud provider   service.beta.kubernetes.io/azure-load-balancer-health-probe-num-of-probe The minimum number of unhealthy responses of health probe Refer to the detailed docs here v1.21 and later with out-of-tree cloud provider   service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path Request path of the health probe Refer to the detailed docs here v1.20 and later with out-of-tree cloud provider   service.beta.kubernetes.io/port_{port}_health-probe_interval Health probe interval {port} is port number of service. Refer to the detailed docs here v1.21 and later with out-of-tree cloud provider   service.beta.kubernetes.io/port_{port}_health-probe_num-of-probe The minimum number of unhealthy responses of health probe {port} is port number of service. Refer to the detailed docs here v1.21 and later with out-of-tree cloud provider   service.beta.kubernetes.io/port_{port}_health-probe_request-path Request path of the health probe {port} is port number of service. Refer to the detailed docs here v1.20 and later with out-of-tree cloud provider   service.beta.kubernetes.io/azure-load-balancer-enable-high-availability-ports Enable high availability ports on internal SLB HA ports is required when applications require IP fragments v1.20 and later   service.beta.kubernetes.io/azure-deny-all-except-load-balancer-source-ranges true or false Deny all traffic to the service. This is helpful when the service.Spec.LoadBalancerSourceRanges is set to an internal load balancer typed service. When set the loadBalancerSourceRanges field on the service in order to whitelist ip src addresses, although the generated NSG has added the rules for loadBalancerSourceRanges, the default rule (65000) will allow any vnet traffic, basically meaning the whitelist is of no use. This annotation solves this issue. v1.21 and later   service.beta.kubernetes.io/azure-additional-public-ips External public IPs besides the service’s own public IP It is mainly used for global VIP on Azure cross-region LoadBalancer v1.20 and later with out-of-tree cloud provider    Please note that\n When loadBalancerSourceRanges have been set on service spec, service.beta.kubernetes.io/azure-allowed-service-tags won’t work because of DROP iptables rules from kube-proxy. The CIDRs from service tags should be merged into loadBalancerSourceRanges to make it work.  Load balancer selection modes There are currently three possible load balancer selection modes :\n Default mode - service has no annotation (“service.beta.kubernetes.io/azure-load-balancer-mode”). In this case the Loadbalancer of the primary Availability set is selected “auto” mode - service is annotated with __auto__ value. In this case, services would be associated with the Loadbalancer with the minimum number of rules. “{vmset-name}” mode - service is annotated with the name of a VMSS/VMAS. In this case, only load balancers of the specified VMSS/VMAS would be selected, and services would be associated with the one with the minimum number of rules.   Note that the “auto” mode is valid only if the service is newly created. It is not allowed to change the annotation value to __auto__ of an existed service.\n The selection mode for a load balancer only works for basic load balancers or multiple standard load balancers. Following is the detailed information of allowed number of VMSS/VMAS in a load balancer.\n Standard SKU supports any virtual machine in a single virtual network, including a mix of virtual machines, availability sets, and virtual machine scale sets. So all the nodes would be added to the same standard LB backend pool with a max size of 1000. Basic SKU only supports virtual machines in a single availability set, or a virtual machine scale set. Only nodes with the same availability set or virtual machine scale set would be added to the basic LB backend pool.  LoadBalancer SKUs Azure cloud provider supports both basic and standard SKU load balancers, which can be set via loadBalancerSku option in cloud config file. A list of differences between these two SKUs can be found here.\n Note that the public IPs used in load balancer frontend configurations should be the same SKU. That is a standard SKU public IP for standard load balancer and a basic SKU public IP for a basic load balancer.\n Azure doesn’t support a network interface joining load balancers with different SKUs, hence migration dynamically between them is not supported.\n If you do require migration, please delete all services with type LoadBalancer (or change to other type)\n Outbound connectivity Outbound connectivity is also different between the two load balancer SKUs:\n  For the basic SKU, the outbound connectivity is opened by default. If multiple frontends are set, then the outbound IP is selected randomly (and configurable) from them.\n  For the standard SKU, the outbound connectivity is disabled by default. There are two ways to open the outbound connectivity: use a standard public IP with the standard load balancer or define outbound rules.\n  Standard LoadBalancer Because the load balancer in a Kubernetes cluster is managed by the Azure cloud provider, and it may change dynamically (e.g. the public load balancer would be deleted if no services defined with type LoadBalancer), outbound rules are the recommended path if you want to ensure the outbound connectivity for all nodes.\n Especially note:\n  In the context of outbound connectivity, a single standalone VM, all the VM’s in an Availability Set, all the instances in a VMSS behave as a group. This means, if a single VM in an Availability Set is associated with a Standard SKU, all VM instances within this Availability Set now behave by the same rules as if they are associated with Standard SKU, even if an individual instance is not directly associated with it.\n  Public IP’s used as instance-level public IP are mutually exclusive with outbound rules.\n   Here is the recommended way to define the outbound rules when using separate provisioning tools:\n Create a separate IP (or multiple IPs for scale) in a standard SKU for outbound rules. Make use of the allocatedOutboundPorts parameter to allocate sufficient ports for your desired scenario scale. Create a separate pool definition for outbound, and ensure all virtual machines or VMSS virtual machines are in this pool. Azure cloud provider will manage the load balancer rules with another pool, so that provisioning tools and the Azure cloud provider won’t affect each other. Define inbound with load balancing rules and inbound NAT rules as needed, and set disableOutboundSNAT to true on the load balancing rule(s). Don’t rely on the side effect from these rules for outbound connectivity. It makes it messier than it needs to be and limits your options. Use inbound NAT rules to create port forwarding mappings for SSH access to the VM’s rather than burning public IPs per instance.  Exclude nodes from the load balancer  Excluding nodes from Azure LoadBalancer is supported since v1.20.0.\n The kubernetes controller manager supports excluding nodes from the load balancer backend pools by enabling the feature gate ServiceNodeExclusion. To exclude nodes from Azure LoadBalancer, label node.kubernetes.io/exclude-from-external-load-balancers=true should be added to the nodes.\n  To use the feature, the feature gate ServiceNodeExclusion should be on (enabled by default since its beta on v1.19).\n  The labeled nodes would be excluded from the LB in the next LB reconcile loop, which needs one or more LB typed services to trigger. Basically, users could trigger the update by creating a service. If there are one or more LB typed services existing, no extra operations are needed.\n  To re-include the nodes, just remove the label and the update would be operated in the next LB reconcile loop.\n  Using SCTP SCTP protocol services are only supported on internal standard LoadBalancer, hence annotation service.beta.kubernetes.io/azure-load-balancer-internal: \"true\" should be added to SCTP protocol services. See below for an example:\napiVersion:v1kind:Servicemetadata:name:sctpserviceannotations:service.beta.kubernetes.io/azure-load-balancer-internal:\"true\"spec:type:LoadBalancerselector:app:sctpserverports:- name:sctpserverprotocol:SCTPport:30102targetPort:30102Multiple Standard LoadBalancer per cluster  This feature is supported since v1.20.0.\n There is only one external and one internal Standard Load Balancer (SLB) at most per cluster. Set enableMultipleStandardLoadBalancers=true in the cloud config if you want to turn on the multiple SLB mode. Similar to the basic LB, there will be a 1:1 mapping between each SLB and VMSS/VMAS. The SLB of the primary VMSS/VMAS will be named after clusterName in the cloud config (in AKS, it would be kubernetes) while the name of those belonging to non-primary VMSS/VMAS will be the name of the corresponding vmSet.\n If the cluster provisioning tools like ASK-Engine and CAPZ don’t proactively create a dedicated SLB for each VMSS/VMAS when enabling multiple SLB, only the primary SLB would be created. You could manually trigger the creation by setting the service annotation service.beta.kubernetes.io/azure-load-balancer-mode to bind the service to that VMSS/VMAS. The dedicated SLB would be created once the service reconcile loop is done. Unlike the primary SLB, there is no default outbound rules/IPs for the non-primary SLBs. That means the SLB would be deleted once all the services referencing it are deleted.\n Choose which SLB to use The service annotation service.beta.kubernetes.io/azure-load-balancer-mode will be respected as long as enableMultipleStandardLoadBalancers=true when using standard LB, and the usage is the same as it is in the basic LB clusters. Specifically, there are three selection mode: default to select the primary SLB; __auto__ to select the SLB with minimum rules and vmSetName to select the dedicated SLB of that VMSS/VMAS.\nOutbound Connections of non-primary VMSS/VMAS The outbound rules of the non-primary SLB are not managed by cloud provider azure. Instead, it should be managed by cluster provisioning tools. For now, there is no outbound configuration for the non-primary VMSS/VMAS, but we plan to support customized outbound configurations in AKS and CAPZ in the future.\nSharing the primary SLB with multiple VMSS/VMAS  This feature is supported since v1.21.0\n For each non-primary VMSS/VMAS, one can determine to use dedicated SLB or share the primary SLB. If the VMSS/VMAS names are in the cloud config nodepoolsWithoutDedicatedSLB, those would join the backend pool of the primary SLB while the others would remain to have dedicated SLBs. If the VMSS/VMAS supposed to share the primary SLB owns a dedicated SLB, the dedicated one would be deleted, and the VMSS/VMAS would be joint the primary SLB’s backend pool.\nCustom Load Balancer health probe As documented here, Tcp, Http and Https are three protocols supported by load balancer service.\nCurrently, the default protocol of the health probe varies among services with different transport protocols, app protocols, annotations and external traffic policies.\n for local services, HTTP and /healthz would be used. The health probe will query NodeHealthPort rather than actual backend service for cluster TCP services, TCP would be used. for cluster UDP services, no health probes.  Since v1.20, two service annotations service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path are introduced, which determine the new health probe behavior. If the spec.ports.appProtocol is set, both local and cluster TCP services would use the specified health probe protocol. If the service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path is set, the specified request path would be used instead of /healthz. Note that the request path would be ignored when using TCP or the spec.ports.appProtocol is empty. More specifically:\n   loadbalancer sku externalTrafficPolicy spec.ports.Protocol spec.ports.AppProtocol service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path protocol request path     standard local any any any http /healthz   standard cluster udp any any null null   standard cluster tcp  (ignored) tcp null   standard cluster tcp tcp (ignored) tcp null   standard cluster tcp http/https  http/https /   standard cluster tcp http/https /custom-path http/https /custom-path   standard cluster tcp unsupported protocol /custom-path tcp null (For backward compatibility)   basic local any any any http /healthz   basic cluster tcp  (ignored) tcp null   basic cluster tcp tcp (ignored) tcp null   basic cluster tcp http  http /   basic cluster tcp http /custom-path http /custom-path   basic cluster tcp unsupported protocol /custom-path tcp null (For backward compatibility)    Since v1.21, two service annotations service.beta.kubernetes.io/azure-load-balancer-health-probe-interval and load-balancer-health-probe-num-of-probe are introduced, which customize the configuration of health probe. If service.beta.kubernetes.io/azure-load-balancer-health-probe-interval is not set, Default value of 5 is applied. If load-balancer-health-probe-num-of-probe is not set, Default value of 2 is applied. And total probe should be less than 120 seconds.\nCustom Load Balancer health probe for port Because MixedProtocolLBService feature is in alpha stage, Ports in one service may have different probe configurations. Following annotations are introduced to customize probe configuration for one port.\n   port specific annotation global probe annotation     service.beta.kubernetes.io/port_{port}_health-probe_request-path service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path   service.beta.kubernetes.io/port_{port}_health-probe_num-of-probe service.beta.kubernetes.io/azure-load-balancer-health-probe-num-of-probe   service.beta.kubernetes.io/port_{port}_health-probe_interval service.beta.kubernetes.io/azure-load-balancer-health-probe-interval    For following manifest, probe rule for port httpsserver is different from the one for httpserver because annoations for port httpsserver are specified.\napiVersion:v1kind:Servicemetadata:name:appserviceannotations:service.beta.kubernetes.io/azure-load-balancer-health-probe-num-of-probe:\"5\"service.beta.kubernetes.io/port_443_health-probe_num-of-probe:\"4\"spec:type:LoadBalancerselector:app:serverports:- name:httpserverprotocol:TCPport:80targetPort:30102- name:httpsserverprotocol:TCPappProtocol:HTTPSport:443targetPort:30104Configure Load Balancer backend  This feature is supported since v1.23.0\n The backend pool type can be configured by specifying loadBalancerBackendPoolConfigurationType in the cloud configuration file. There are three possible values:\n nodeIPConfiguration (default). In this case we attach nodes to the LB by calling the VMSS/NIC API to associate the corresponding node IP configuration with the LB backend pool. nodeIP. In this case we attach nodes to the LB by calling the LB API to add the node private IP addresses to the LB backend pool. podIP (not supported yet). In this case we do not attach nodes to the LB. Instead we directly adding pod IPs to the LB backend pool.  Load balancer limits The limits of the load balancer related resources are listed below:\nStandard Load Balancer\n   Resource Limit     Load balancers 1,000   Rules per resource 1,500   Rules per NIC (across all IPs on a NIC) 300   Frontend IP configurations 600   Backend pool size 1,000 IP configurations, single virtual network   Backend resources per Load Balancer 150   High-availability ports 1 per internal frontend   Outbound rules per Load Balancer 600   Load Balancers per VM 2 (1 Public and 1 internal)    The limit is up to 150 resources, in any combination of standalone virtual machine resources, availability set resources, and virtual machine scale-set placement groups.\nBasic Load Balancer\n   Resource Limit     Load balancers 1,000   Rules per resource 250   Rules per NIC (across all IPs on a NIC) 300   Frontend IP configurations 200   Backend pool size 300 IP configurations, single availability set   Availability sets per Load Balancer 1   Load Balancers per VM 2 (1 Public and 1 internal)     There is a restriction of 300 rules per NIC, hence for single SLB mode 300 services are allowed at most. If more services are required, try to enable multiple SLBs.\n ","categories":"","description":"Azure LoadBalancer basics.\n","excerpt":"Azure LoadBalancer basics.\n","ref":"/cloud-provider-azure/topics/loadbalancer/","tags":"","title":"Azure LoadBalancer"},{"body":"cloud-provider-azure uses go modules for Go dependency management.\nUsage Run make update-dependencies whenever vendored dependencies change. This takes a minute to complete.\nRun make update-mocks whenever implementations for pkg/azureclients change.\nUpdating dependencies New dependencies causes golang to recompute the minor version used for each major version of each dependency. And golang automatically removes dependencies that nothing imports any more.\nTo upgrade to the latest version for all direct and indirect dependencies of the current module:\n run go get -u \u003cpackage\u003e to use the latest minor or patch releases run go get -u=patch \u003cpackage\u003e to use the latest patch releases run go get \u003cpackage\u003e@VERSION to use the specified version  You can also manually editing go.mod and update the versions in require and replace parts.\nBecause of staging in Kubernetes, manually go.mod updating is required for Kubernetes and its staging packages. In cloud-provider-azure, their versions are set in replace part, e.g.\nreplace ( ... k8s.io/kubernetes =\u003e k8s.io/kubernetes v0.0.0-20190815230911-4e7fd98763aa ) To update their versions, you need switch to $GOPATH/src/k8s.io/kubernetes, checkout to the version you want upgrade to, and finally run the following commands to get the go modules expected version:\ncommit=$(TZ=UTC git --no-pager show --quiet --abbrev=12 --date='format-local:%Y%m%d%H%M%S' --format=\"%cd-%h\") echo \"v0.0.0-$commit\" After this, replace all kubernetes and staging versions (e.g. v0.0.0-20190815230911-4e7fd98763aa in above example) in go.mod.\nAlways run hack/update-dependencies.sh after changing go.mod by any of these methods (or adding new imports).\nSee golang’s go.mod, Using Go Modules and Kubernetes Go modules docs for more details.\nUpdating mocks mockgen v1.6.0 is used to generate mocks.\nmockgen -copyright_file=\u003ccopyright file\u003e -source=\u003cazureclient source\u003e -package=\u003cmock package\u003e ","categories":"","description":"Manage Cloud Provider Azure dependencies using go modules.\n","excerpt":"Manage Cloud Provider Azure dependencies using go modules.\n","ref":"/cloud-provider-azure/development/dependencies/","tags":"","title":"Dependency Management"},{"body":"NOTE This page only applies after Azure cloud provider implementation code has been moved to this repository.\nThere are some ongoing issues and pull requests addressing the Azure cloud provider in Kubernetes repository.\nWhen we turned to use the standalone cloud provider in this repository, those issues and pull requests should also be moved.\nHere are some notes for issues and pull requests migration.\nIssue migration If issue applies only to Azure cloud provider, please close it and create a new one in this repository.\nIf issue also involves other component, leave it there, but do create a new issue in this repository to track counterpater in Azure cloud provider.\nIn both cases, leave a link to the new created issue in the old issue.\nPull request migration Basically we have migrated code from k8s.io/legacy-cloud-providers/azure/ to github.com/sigs.k8s.io/cloud-provider-azure/pkg/provider.\nThe following steps describe how to port an existing PR from kubernetes repository to this repository.\n Generate pull request patch  In your kubernetes repository, run following to generate a patch for your PR.\n PR_ID: Pull Request ID in kubernetes repository UPSTREAM_BRANCH: Branch name pointing to upstream, basically the branch with url https://github.com/kubernetes/kubernetes.git or https://k8s.io/kubernetes  PR_ID= UPSTREAM_BRANCH=origin PR_BRANCH_LOCAL=PR$PR_ID git fetch $UPSTREAM_BRANCH pull/$PR_ID/head:$PR_BRANCH_LOCAL MERGE_BASE=$(git merge-base $UPSTREAM_BRANCH/master $PR_BRANCH_LOCAL) PATCH_FILE=/tmp/${PR_ID}.patch git diff $MERGE_BASE $PR_BRANCH_LOCAL \u003e $PATCH_FILE git branch -D $PR_BRANCH_LOCAL Transform the patch and apply  Switch to kubernetes-azure-cloud-controller-manager repo. Apply the patch:\nhack/transform-patch.pl $PATCH_FILE | git apply If any of file in the patch does not fall under Azure cloud provider directory, the transform script will prompt a warning.\n","categories":"","description":"Developer guidance for how to contribute using issues and PRs.\n","excerpt":"Developer guidance for how to contribute using issues and PRs.\n","ref":"/cloud-provider-azure/contribute/issues-and-pull-requests-migration/","tags":"","title":"Issues and pull requests migration"},{"body":"","categories":"","description":"Known Issues of Azure cloud provider.\n","excerpt":"Known Issues of Azure cloud provider.\n","ref":"/cloud-provider-azure/faq/known-issues/","tags":"","title":"Known Issues"},{"body":"The AKS-engine supports deploying clusters with customized Cloud Controller Manager (CCM) and Cloud Node Manager (CNM) images. The API model is defined here. Follow this guide to build your CCM and CNM images and fill them into the AKS-engine API model.\nTo manually deploy an out-of-tree cluster, we need to deploy the following manifests. Note that there are some restrictions when setting config flags. To get more infomation, checkout this doc.\ncloud-controller-manager\ncloud-node-manager\nkube-apiserver\nkube-controller-manager\nkube-shedular\nkubelet command\n","categories":"","description":"Deploy a cluster with Out-of-tree Cloud Provider Azure.\n","excerpt":"Deploy a cluster with Out-of-tree Cloud Provider Azure.\n","ref":"/cloud-provider-azure/example/out-of-tree/","tags":"","title":"Deploy with Out-of-tree Cloud Provider Azure"},{"body":"Azure cloud provider requires a set of permissions to manage the Azure resources. Here is a list of all permissions and reasons of why they’re required.\n// Required to create, delete or update LoadBalancer for LoadBalancer service Microsoft.Network/loadBalancers/delete Microsoft.Network/loadBalancers/read Microsoft.Network/loadBalancers/write // Required to allow query, create or delete public IPs for LoadBalancer service Microsoft.Network/publicIPAddresses/delete Microsoft.Network/publicIPAddresses/read Microsoft.Network/publicIPAddresses/write // Required if public IPs from another resource group are used for LoadBalancer service // This is because of the linked access check when adding the public IP to LB frontendIPConfiguration Microsoft.Network/publicIPAddresses/join/action // Required to create or delete security rules for LoadBalancer service Microsoft.Network/networkSecurityGroups/read Microsoft.Network/networkSecurityGroups/write // Required to create, delete or update AzureDisks Microsoft.Compute/disks/delete Microsoft.Compute/disks/read Microsoft.Compute/disks/write Microsoft.Compute/locations/DiskOperations/read // Required to create, update or delete storage accounts for AzureFile or AzureDisk Microsoft.Storage/storageAccounts/delete Microsoft.Storage/storageAccounts/listKeys/action Microsoft.Storage/storageAccounts/read Microsoft.Storage/storageAccounts/write Microsoft.Storage/operations/read // Required to create, delete or update routeTables and routes for nodes Microsoft.Network/routeTables/read Microsoft.Network/routeTables/routes/delete Microsoft.Network/routeTables/routes/read Microsoft.Network/routeTables/routes/write Microsoft.Network/routeTables/write // Required to query information for VM (e.g. zones, faultdomain, size and data disks) Microsoft.Compute/virtualMachines/read // Required to attach AzureDisks to VM Microsoft.Compute/virtualMachines/write // Required to query information for vmssVM (e.g. zones, faultdomain, size and data disks) Microsoft.Compute/virtualMachineScaleSets/read Microsoft.Compute/virtualMachineScaleSets/virtualMachines/read Microsoft.Compute/virtualMachineScaleSets/virtualmachines/instanceView/read // Required to add VM to LoadBalancer backendAddressPools Microsoft.Network/networkInterfaces/write // Required to add vmss to LoadBalancer backendAddressPools Microsoft.Compute/virtualMachineScaleSets/write // Required to attach AzureDisks and add vmssVM to LB Microsoft.Compute/virtualMachineScaleSets/virtualmachines/write // Required to upgrade VMSS models to latest for all instances // only needed for Kubernetes 1.11.0-1.11.9, 1.12.0-1.12.8, 1.13.0-1.13.5, 1.14.0-1.14.1 Microsoft.Compute/virtualMachineScaleSets/manualupgrade/action // Required to query internal IPs and loadBalancerBackendAddressPools for VM Microsoft.Network/networkInterfaces/read // Required to query internal IPs and loadBalancerBackendAddressPools for vmssVM microsoft.Compute/virtualMachineScaleSets/virtualMachines/networkInterfaces/read // Required to get public IPs for vmssVM Microsoft.Compute/virtualMachineScaleSets/virtualMachines/networkInterfaces/ipconfigurations/publicipaddresses/read // Required to check whether subnet existing for ILB in another resource group Microsoft.Network/virtualNetworks/read Microsoft.Network/virtualNetworks/subnets/read // Required to create, update or delete snapshots for AzureDisk Microsoft.Compute/snapshots/delete Microsoft.Compute/snapshots/read Microsoft.Compute/snapshots/write // Required to get vm sizes for getting AzureDisk volume limit Microsoft.Compute/locations/vmSizes/read Microsoft.Compute/locations/operations/read ","categories":"","description":"Permissions required to set up Azure resources.\n","excerpt":"Permissions required to set up Azure resources.\n","ref":"/cloud-provider-azure/topics/azure-permissions/","tags":"","title":"Azure Permissions"},{"body":"Azure disk Container Storage Interface (CSI) Storage Plugin is moved to https://github.com/kubernetes-sigs/azuredisk-csi-driver. Please check the github link for the documentation.\n","categories":"","description":"Deploy AzureDisk CSI driver to Cloud Provider Azure. \n","excerpt":"Deploy AzureDisk CSI driver to Cloud Provider Azure. \n","ref":"/cloud-provider-azure/install/azuredisk/","tags":"","title":"Deploy AzureDisk CSI Driver"},{"body":"","categories":"","description":"E2E tests guidance.\n","excerpt":"E2E tests guidance.\n","ref":"/cloud-provider-azure/development/e2e/","tags":"","title":"E2E tests"},{"body":"Release source There are two major code change sources for this project, either may push forward a new release for Kubernetes azure-cloud-controller-manager:\n  Changes in Kubernetes cloud-controller-manager, which happens in Kubernetes repository Since this project dependes on Kubernetes cloud-controller-manager, we’ll periodically sync changes from Kubernetes upstream repository. When upstream shipped a new release tag, we may consider publishing a new release\n  Changes in Azure cloud provider, which happens directly in this repository Azure cloud provider also accepts new features and bug changes. In cases when a security fix is required or when the changes accumulated to certain amount, we may also consider publishing a new release, even if there is no change from Kubernetes upstream.\n  Versioning This project is a Kubernetes component whereas the functionalities and APIs all go with Kubernetes upstream project, thus we will use same versioning mechanism of Kubernetes, with some subtle differences for Azure cloud provider and non-Kubernetes changes.\nThe basic rule is:\n Every release version follows Semantic Versioning, in the form of MAJOR.MINOR.PATCH For MAJOR.MINOR, it keeps same value as the Kubernetes upstream For PATCH, it is calculated independently:  If upstream Kubernetes has a new a patch release, which introduces change in cloud-controller-manager or any component we depend on, then sync the change and increase the PATCH number. If any code change happens in Azure cloud provider or other dependency projects, which becomes eligible for a new release, then increase the PATCH number.    References:\n Kubernetes Release Versioning Semantic Versioning  Branch and version scheme This project uses golang’s vendoring mechanism for managing dependencies (see Dependency management for detail). When talking about ‘sync from Kubernetes upstream’, it actually means vendoring Kubernetes repository code under the vendor directory.\nDuring each sync from upstream, it is usually fine to sync to latest commit. But if there is a new tagged commit in upstream that we haven’t vendored, we should sync to that tagged commit first, and apply a version tag correspondingly if applicable. The version tag mechanism is a bit different on master branch and releasing branch, please see below for detail.\nThe upstream syncing change should be made in a single Pull Request. If in some case, the upstream change causes a test break, then the pull requests should not be merged until follow up fix commits are added.\nFor example, if upstream change adds a new cloud provider interface, syncing the upstream change may raise a test break, and we should add the implementation (even no-op) in same pull request.\nmaster branch This is the main development branch for merging pull requests. When upgrading dependencies, it will sync from Kubernetes upstream’s master branch.\nFixes to releasing branches should be merged in master branch first, and then ported to corresponding release branch.\nVersion tags:\n X.Y.0-alpha.0  This is initial tag for a new release, it will be applied when a release branch is created. See below for detail   X.Y.0-alpha.W, W \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.0-alpha.W in Kubernetes upstream    releasing branch For release X.Y, the branch will have name release-X.Y. When upgrading dependencies, it will sync with Kubernetes upstream’s release-X.Y branch. Release branch would be created when upstream release branch is created and first X.Y.0-beta.0 tag is applied.\nVersion tags:\n X.Y.0-beta.0  X.Y.0-beta.0 would be tagged at first independent commit on release branch, the corresponding separation point commit on master would be tagged X.Y+1.0-alpha.0 No new feature changes are allowed from this time on   X.Y.0-beta.W, W \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.0-beta.W in Kubernetes upstream   X.Y.0  This is the final release version. When upstream X.Y.0 tag rolls out, we will begin prepare X.Y.0 release After merging upstream X.Y.0 tag commit, we will run full test cycle to ensure the Azure cloud provider works well before release:  If any test fails, prepare fixes first. If the fix also applies to master branch, then also apply it to master. Rerun full test cycle till all tests got passed stablely Finally, apply X.Y.0 to latest commit of releasing branch   X.Y.1-beta.0 will be tagged at the same commit   X.Y.Z, Z \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.Z in Kubernetes upstream Testing and release process follows same rule as X.Y.0    CI and dev version scheme We use git-describe as versioning source, please check version for detail.\nIn this case, for commits that does not have a certain tag, the result version would be something like ‘v0.1.0-alpha.0-25-gd7999d10’.\n","categories":"","description":"Introduce rules related to release.\n","excerpt":"Introduce rules related to release.\n","ref":"/cloud-provider-azure/contribute/release-versioning/","tags":"","title":"Release Versioning"},{"body":"Feature Status: Alpha since v1.12.\nKubernetes v1.12 adds support for Azure availability zones (AZ). Nodes in availability zone will be added with label failure-domain.beta.kubernetes.io/zone=\u003cregion\u003e-\u003cAZ\u003e and topology-aware provisioning is added for Azure managed disks storage class.\nTOC:\n Availability Zones  Pre-requirements Node labels Load Balancer Managed Disks  StorageClass examples PV examples   Appendix Reference    Pre-requirements Because only standard load balancer is supported with AZ, it is a prerequisite to enable AZ for the cluster. It should be configured in Azure cloud provider configure file (e.g. /etc/kubernetes/cloud-config/azure.json):\n{ \"loadBalancerSku\": \"standard\", ... } If topology-aware provisioning feature is used, feature gate VolumeScheduling should be enabled on master components (e.g. kube-apiserver, kube-controller-manager and kube-scheduler).\nNode labels Both zoned and unzoned nodes are supported, but the value of node label failure-domain.beta.kubernetes.io/zone are different:\n For zoned nodes, the value is \u003cregion\u003e-\u003cAZ\u003e, e.g. centralus-1. For unzoned nodes, the value is faultDomain, e.g. 0.  e.g.\n$ kubectl get nodes --show-labelsNAME STATUS AGE VERSION LABELSkubernetes-node12 Ready 6m v1.11 failure-domain.beta.kubernetes.io/region=centralus,failure-domain.beta.kubernetes.io/zone=centralus-1,...Load Balancer loadBalancerSku has been set to standard in cloud provider configure file, so standard load balancer and standard public IPs will be provisioned automatically for services with type LoadBalancer. Both load balancer and public IPs are zone redundant.\nManaged Disks Zone-aware and topology-aware provisioning are supported for Azure managed disks. To support these features, a few options are added in AzureDisk storage class:\n zoned: indicates whether new disks are provisioned with AZ. Default is true. allowedTopologies: indicates which topologies are allowed for topology-aware provisioning. Only can be set if zoned is not false.  StorageClass examples An example of zone-aware provisioning storage class is:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:labels:kubernetes.io/cluster-service:\"true\"name:managed-premiumparameters:kind:Managedstorageaccounttype:Premium_LRSzoned:\"true\"provisioner:kubernetes.io/azure-diskvolumeBindingMode:WaitForFirstConsumerAnother example of topology-aware provisioning storage class is:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:labels:kubernetes.io/cluster-service:\"true\"name:managed-premiumparameters:kind:Managedstorageaccounttype:Premium_LRSprovisioner:kubernetes.io/azure-diskvolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:failure-domain.beta.kubernetes.io/zonevalues:- centralus-1- centralus-2PV examples When feature gate VolumeScheduling disabled, no NodeAffinity set for zoned PV:\n$ kubectl describe pv Name: pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c Labels: failure-domain.beta.kubernetes.io/region=southeastasia failure-domain.beta.kubernetes.io/zone=southeastasia-2 Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: default Status: Bound Claim: default/pvc-azuredisk Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [southeastasia-2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c DiskURI: /subscriptions/\u003csubscription-id\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e When feature gate VolumeScheduling enabled, NodeAffinity will be populated for zoned PV:\n$ kubectl describe pv Name: pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c Labels: failure-domain.beta.kubernetes.io/region=southeastasia failure-domain.beta.kubernetes.io/zone=southeastasia-2 Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: default Status: Bound Claim: default/pvc-azuredisk Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [southeastasia-2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c DiskURI: /subscriptions/\u003csubscription-id\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e While unzoned disks are not able to attach in zoned nodes, NodeAffinity will also be set for them so that they will only be scheduled to unzoned nodes:\n$ kubectl describe pv pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Name: pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: azuredisk-unzoned Status: Bound Claim: default/unzoned-pvc Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [0] Term 1: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [1] Term 2: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c DiskURI: /subscriptions/\u003csubscription\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e Appendix Note that unlike most cases, fault domain and availability zones mean different on Azure:\n A Fault Domain (FD) is essentially a rack of servers. It consumes subsystems like network, power, cooling etc. Availability Zones are unique physical locations within an Azure region. Each zone is made up of one or more data centers equipped with independent power, cooling, and networking.  An Availability Zone in an Azure region is a combination of a fault domain, and an update domain (Same like FD, but for updates. When upgrading a deployment, it is carried out one update domain at a time). For example, if you create three or more VMs across three zones in an Azure region, your VMs are effectively distributed across three fault domains and three update domains.\nReference See design docs for AZ in KEP for Azure availability zones.\n","categories":"","description":"Use availability zones in provider azure.\n","excerpt":"Use availability zones in provider azure.\n","ref":"/cloud-provider-azure/topics/availability-zones/","tags":"","title":"Use Availability Zones"},{"body":"Azure file Container Storage Interface (CSI) Storage Plugin is moved to https://github.com/kubernetes-sigs/azurefile-csi-driver. Please check the github link for the documentation.\n","categories":"","description":"Deploy AzureFile CSI driver to Cloud Provider Azure. \n","excerpt":"Deploy AzureFile CSI driver to Cloud Provider Azure. \n","ref":"/cloud-provider-azure/install/azurefile/","tags":"","title":"Deploy AzureFile CSI Driver"},{"body":"AKS-Engine Please refer to step 1-3 in e2e-tests for how to deploy a cluster by AKS-Engine.\nCluster API Provider Azure (CAPZ) Please run the following command in the root directory of the repo.\nmake deploy-cluster Customizations are supported by environment variables:\n   Environment variables required description default     AZURE_SUBSCRIPTION_ID true subscription ID    AZURE_TENANT_ID true tenant ID    AZURE_CLIENT_ID true client ID with permission    AZURE_CLIENT_SECRET true client secret    CLUSTER_NAME true name of the cluster    AZURE_RESOURCE_GROUP true name of the resource group to be deployed (auto generated if not existed)    MANAGEMENT_CLUSTER_NAME false name of the kind management cluster capi   WORKLOAD_CLUSTER_TEMPLATE false path to the cluster-api template tests/k8s-azure-manifest/cluster-api/vmss-multi-nodepool.yaml   CUSTOMIZED_CLOUD_CONFIG_TEMPLATE false customized cloud provider configs    AZURE_CLUSTER_IDENTITY_SECRET_NAME false name of the cluster identity secret cluster-identity-secret   AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE false namespace of the cluster identity secret default   CLUSTER_IDENTITY_NAME false name of the AzureClusterIdentity CRD cluster-identity   CONTROL_PLANE_MACHINE_COUNT false number of the control plane nodes 1   WORKER_MACHINE_COUNT false number of the worker nodes 2   AZURE_CONTROL_PLANE_MACHINE_TYPE false VM SKU of the control plane nodes Standard_D4s_v3   AZURE_NODE_MACHINE_TYPE false VM SKU of the worker nodes Standard_D2s_v3   AZURE_LOCATION false region of the cluster resources westus2   AZURE_CLOUD_CONTROLLER_MANAGER_IMG false image of the cloud-controller-manager mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.23.1   AZURE_CLOUD_NODE_MANAGER_IMG false image of the cloud-node-manager mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v1.23.1   KUBERNETES_VERSION false Kubernetes components version v1.23.0   LB_SKU false LoadBalancer SKU, Standard or Basic Standard   ENABLE_MULTI_SLB false Enable multiple standard LoadBalancers per cluster false   LB_BACKEND_POOL_CONFIG_TYPE false LoadBalancer backend pool configuration type, nodeIPConfiguration, nodeIP or podIP nodeIPConfiguration   PUT_VMSS_VM_BATCH_SIZE false Batch size when updating VMSS VM concurrently 0   AZURE_SSH_PUBLIC_KEY false SSH public key to connecet to the VMs \"\"    ","categories":"","description":"Deploy Kubernetes clusters\n","excerpt":"Deploy Kubernetes clusters\n","ref":"/cloud-provider-azure/development/deploy-cluster/","tags":"","title":"Deploy clusters"},{"body":"This is the staging area of the design docs prior to or under development. Once the feature is done, the corresponding design doc would be moved to Topics.\n","categories":"","description":"Design Docs and KEPs related to this project.\n","excerpt":"Design Docs and KEPs related to this project.\n","ref":"/cloud-provider-azure/development/design-docs/","tags":"","title":"Design Docs and KEPs"},{"body":"Security Announcements Join the kubernetes-security-announce group for security and vulnerability announcements.\nYou can also subscribe to an RSS feed of the above using this link.\nReporting a Vulnerability Instructions for reporting a vulnerability can be found on the Kubernetes Security and Disclosure Information page.\nSupported Versions Information about supported Kubernetes versions can be found on the Kubernetes version and version skew support policy page on the Kubernetes website.\n","categories":"","description":"Security Policies.\n","excerpt":"Security Policies.\n","ref":"/cloud-provider-azure/contribute/security/","tags":"","title":"Security Policy"},{"body":"Feature status: Alpha since v1.12.\nKubernetes v1.12 adds support for cross resource group (RG) nodes and unmanaged (such as on-prem) nodes in Azure cloud provider. A few assumptions are made for such nodes:\n Cross-RG nodes are in same region and set with required labels (as clarified in the following part) Nodes will not be part of the load balancer managed by cloud provider Both node and container networking should be configured properly by provisioning tools AzureDisk is supported for Azure cross-RG nodes, but not for on-prem nodes  TOC:\n Cross resource group nodes  Pre-requirements Cross-RG nodes Unmanaged nodes Reference    Pre-requirements Because cross-RG nodes and unmanaged nodes won’t be added to Azure load balancer backends, feature gate ServiceNodeExclusion should be enabled for master components (e.g. kube-controller-manager).\nCross-RG nodes Cross-RG nodes should register themselves with required labels together with cloud provider:\n node.kubernetes.io/exclude-balancer, which is used to exclude the node from load balancer.  alpha.service-controller.kubernetes.io/exclude-balancer=true should be used if the cluster version is below v1.16.0.   kubernetes.azure.com/resource-group=\u003crg-name\u003e, which provides external RG and is used to get node information. cloud provider config  --cloud-provider=azure when using kube-controller-manager --cloud-provider=external when using cloud-controller-manager    For example,\nkubelet ... \\  --cloud-provider=azure \\  --cloud-config=/etc/kubernetes/cloud-config/azure.json \\  --node-labels=node.kubernetes.io/exclude-balancer=true,kubernetes.azure.com/resource-group=\u003crg-name\u003e Unmanaged nodes On-prem nodes are different from Azure nodes, all Azure coupled features (such as load balancers and Azure managed disks) are not supported for them. To prevent the node being deleted, Azure cloud provider will always assumes the node existing.\nOn-prem nodes should register themselves with labels node.kubernetes.io/exclude-balancer=true and kubernetes.azure.com/managed=false:\n node.kubernetes.io/exclude-balancer=true, which is used to exclude the node from load balancer. kubernetes.azure.com/managed=false, which indicates the node is on-prem or on other clouds.  For example,\nkubelet ...\\  --cloud-provider= \\  --node-labels=node.kubernetes.io/exclude-balancer=true,kubernetes.azure.com/managed=false Reference See design docs for cross resource group nodes in KEP 20180809-cross-resource-group-nodes.\n","categories":"","description":"Deploy cross resource group nodes.\n","excerpt":"Deploy cross resource group nodes.\n","ref":"/cloud-provider-azure/topics/cross-resource-group-nodes/","tags":"","title":"Deploy Cross Resource Group Nodes"},{"body":"multi-arch image Currently, only Linux multi-arch cloud-node-manager image is supported as a result of customer requests and windows limitations. Supported Linux archs are defined by ALL_ARCH.linux in Makefile, and Windows os versions are by ALL_OSVERSIONS.windows.\nWindows multi-arch image limitation Images nanoserver and servercore are referenced to build a Windows image, but as current officially published servercore images does not support non-amd64 image, and only Windows server 1809 has the support of non-amd64 for nanoserver, amd64 is the only supported arch for a range of Windows OS version so far. This issue is tracked here\nhand-on examples To build and publish the multi-arch image for node manager\nIMAGE_REGISTRY=\u003cregistry\u003e make build-all-node-images IMAGE_REGISTRY=\u003cregistry\u003e make push-multi-arch-node-manager-image To build a specific Linux arch image for node manager\nIMAGE_REGISTRY=\u003cregistry\u003e ARCH=amd64 make build-node-image-linux To build specific Windows OS and arch image for node manager\nIMAGE_REGISTRY=\u003cregistry\u003e OUTPUT_TYPE=registry ARCH=amd64 WINDOWS_OSVERSION=1809 build-node-image-windows The OUTPUT_TYPE registry here means the built image will be published to the registry, this is necessary to build a Windows image from a Linux working environment. An alternative is to export the image tarball to a local destination, like OUTPUT_TYPE=docker,dest=dstdir/azure-cloud-node-manager.tar. For more info about docker buildx output type, please check out here\n","categories":"","description":"Image building.\n","excerpt":"Image building.\n","ref":"/cloud-provider-azure/development/image-building/","tags":"","title":"Image building"},{"body":"To be completed.\n","categories":"","description":"Future Plans.\n","excerpt":"Future Plans.\n","ref":"/cloud-provider-azure/development/future/","tags":"","title":"Future Plans"},{"body":" This feature is supported since v1.20.0.\n Provider Azure supports sharing one IP address among multiple load balancer typed external or internal services. To share an IP address among multiple public services, a public IP resource is needed. This public IP could be created in advance or let the cloud provider provision it when creating the first external service. Specifically, Azure would create a public IP resource automatically when an external service is discovered.\napiVersion:v1kind:Servicemetadata:name:nginxnamespace:defaultspec:ports:- port:80protocol:TCPtargetPort:80selector:app:nginxtype:LoadBalancerNote that the loadBalancerIP is not set, or Azure would find a pre-allocated public IP with the address. After obtaining the IP address of the service, you could create other services using this address.\napiVersion:v1kind:Servicemetadata:name:httpsnamespace:defaultspec:loadBalancerIP:1.2.3.4# the IP address could be the same as it is of `nginx` serviceports:- port:443protocol:TCPtargetPort:443selector:app:httpstype:LoadBalancerNote that if you specify the loadBalancerIP but there is no corresponding public IP pre-allocated, an error would be reported.\nDNS Even if multiple services can refer to one public IP, the DNS label cannot be re-used. The public IP would have the label kubernetes-dns-label-service: \u003csvcName\u003e to indicate which service is binding to the DNS label. In this case if there is another service sharing this specific IP address trying to refer to the DNS label, an error would be reported.\n The DNS name on the public IP won’t be deleted after the service with the DNS annotation being deleted, because the cloud provider don’t know if the DNS was set by the user or not.\n Restrictions The cloud provider azure manages the lifecycle of the system-created public IPs. By default, there are two kinds of system managed tags: kubernetes-cluster-name and service (see the picture below). The controller manager would add the service name to the service if a service is trying to refer to the public IP, and remove the name from the service if the service is deleted. The public IP would be deleted if there is no service in the tag service. However, according to the docs of azure tags, there are several restrictions:\n  Each resource, resource group, and subscription can have a maximum of 50 tag name/value pairs. If you need to apply more tags than the maximum allowed number, use a JSON string for the tag value. The JSON string can contain many values that are applied to a single tag name. A resource group or subscription can contain many resources that each have 50 tag name/value pairs.\n  The tag name is limited to 512 characters, and the tag value is limited to 256 characters. For storage accounts, the tag name is limited to 128 characters, and the tag value is limited to 256 characters.\n  Based to that, we suggest to use static public IPs when there are more than 10 services sharing the IP address.\n","categories":"","description":"Bind one IP address to multiple services.\n","excerpt":"Bind one IP address to multiple services.\n","ref":"/cloud-provider-azure/topics/shared-ip/","tags":"","title":"Multiple Services Sharing One IP Address"},{"body":" This feature is supported since v1.20.0.\n We could use tags to organize your Azure resources and management hierarchy. Cloud Provider Azure supports tagging managed resource through configuration file or service annotation.\nSpecifically, the shared resources (load balancer, route table and security group) could be tagged by setting tags in azure.json:\n{ \"tags\": \"a=b,c=d\" } the controller manager would parse this configuration and tag the shared resources once restarted.\nThe non-shared resource (public IP) could be tagged by setting tags in azure.json or service annotation service.beta.kubernetes.io/azure-pip-tags. The format of the two is similar and the tags in the annotation would be considered first when there are conflicts between the configuration file and the annotation.\n The annotation service.beta.kubernetes.io/azure-pip-tags only works for managed public IPs. For BYO public IPs, the cloud provider would not apply any tags to them.\n When the configuration, file or annotation, is updated, the old ones would be updated if there are conflicts. For example, after updating {\"tags\": \"a=b,c=d\"} to {\"tags\": \"a=c,e=f\"}, the new tags would be a=c,c=d,e=f.\nIntegrating with system tags  This feature is supported since v1.21.0.\n Normally the controller manager don’t delete the existing tags even if they are not included in the new version of azure configuration files, because the controller manager doesn’t know which tags should be deleted and which should not (e.g., tags managed by cloud provider itself). We can leverage the config systemTags in the cloud configuration file to control what tags can be deleted. Here are the examples:\n   Tags SystemTags existing tags on resources new tags on resources     “a=b,c=d” \"\" {} {“a”: “b”, “c”: “d”}   “a=b,c=d” \"\" {“a”: “x”, “c”: “y”} {“a”: “b”, “c”: “d”}   “a=b,c=d” \"\" {“e”: “f”} {“a”: “b”, “c”: “d”, “e”: “f”} /* won’t delete e because the SystemTags is empty */   “c=d” “a” {“a”: “b”} {“a”: “b”, “c”: “d”} /* won’t delete a because it’s in the SystemTags */   “c=d” “x” {“a”: “b”} {“c”: “d”} /* will delete a because it’s not in Tags or SystemTags */     Please consider migrating existing “tags” to “tagsMap”, the support of “tags” configuration would be removed in a future release.\n Including special characters in tags  This feature is supported since v1.23.0.\n Normally we don’t support special characters such as = or , in key-value pairs. These characters will be treated as separator and will not be included in the key/value literal. To solve this problem, tagsMap is introduced since v1.23.0, in which a JSON-style tag is acceptable.\n{ \"tags\": \"a=b,c=d\", \"tagsMap\": {\"e\": \"f\", \"g=h\": \"i,j\"} } tags and tagsMap will be merged, and similarly, they are case-insensitive.\n","categories":"","description":"","excerpt":" This feature is supported since v1.20.0.\n We could use tags to …","ref":"/cloud-provider-azure/topics/tagging-resources/","tags":"","title":"Tagging resources managed by Cloud Provider Azure"},{"body":" Note: The Kubelet credential provider feature is still in alpha and shouldn’t be used in production environments. Please use --azure-container-registry-config=/etc/kubernetes/cloud-config/azure.json if you need pulling images from ACR in production.\n As part of [Out-of-Tree Credential Providers](enhancements/keps/sig-cloud-provider/2133-out-of-tree-credential-provider at master · kubernetes/enhancements (github.com)), the kubelet builtin image pulling from ACR (which could be enabled by setting kubelet --azure-container-registry-config=\u003cconfig-file\u003e) would be moved out-of-tree credential plugin acr-credential-provider. Please refer the original KEP for details.\nIn order to switch the kubelet credential provider to out-of-tree, you’ll have to\n Remove --azure-container-registry-config from kubelet configuration options. Add --feature-gates=KubeletCredentialProviders=true to kubelet configuration options. Create directory /var/lib/kubelet/credential-provider, download ‘acr-credential-provider’ binary to this directory and add --image-credential-provider-bin-dir=/var/lib/kubelet/credential-provider to kubelet configuration options. Create the following credential-provider-config.yaml file and add --image-credential-provider-config=/var/lib/kubelet/credential-provider-config.yaml to kubelet configuration options.  # cat /var/lib/kubelet/credential-provider-config.yamlkind:CredentialProviderConfigapiVersion:kubelet.config.k8s.io/v1alpha1providers:- name:acr-credential-providerapiVersion:credentialprovider.kubelet.k8s.io/v1alpha1defaultCacheDuration:10mmatchImages:- \"*.azurecr.io\"- \"*.azurecr.cn\"- \"*.azurecr.de\"- \"*.azurecr.us\"- \"*.azurecr.*\"# Only required for custom Azure cloud.args:- /etc/kubernetes/cloud-config/azure.json","categories":"","description":"Detailed steps to setup out-of-tree Kubelet Credential Provider.\n","excerpt":"Detailed steps to setup out-of-tree Kubelet Credential Provider.\n","ref":"/cloud-provider-azure/topics/credential-provider/","tags":"","title":"Kubelet Credential Provider"},{"body":" This feature is supported since v1.21.0.\n Background The in-tree Node IPAM controller only supports a fixed node CIDR mask size for all nodes, while in multiple node pool (VMSS) scenarios, different mask sizes are required for different node pools. There is a GCE-specific cloud CIDR allocator for a similar scenario, but that is not exposed in cloud provider API and it is planned to be moved out-of-tree.\nHence this docs proposes an out-of-tree node IPAM controller. Specifically, allocate different pod CIDRs based on different CIDR mask size for different node pools (VMSS or VMAS).\nUsage There are two kinds of CIDR allocator in the node IPAM controller, which are RangeAllocator and CloudAllocator. The RangeAllocator is the default one which allocates the pod CIDR for every node in the range of the cluster CIDR. The CloudAllocator allocates the pod CIDR for every node in the range of the CIDR on the corresponding VMSS or VMAS.\nThe pod CIDR mask size of each node that belongs to a specific VMSS or VMAS is set by a specific tag {\"kubernetesNodeCIDRMaskIPV4\": \"24\"} or {\"kubernetesNodeCIDRMaskIPV6\": \"64\"}. Note that the mask size tagging on the VMSS or VMAS must be within the cluster CIDR, or an error would be thrown.\nWhen the above tag doesn’t exist on VMSS/VMAS, the default mask size (24 for ipv4 and 64 for ipv6) would be used.\nTo turn on the out-of-tree node IPAM controller:\n Disable the in-tree node IPAM controller by setting --allocate-node-cidrs=false in kube-controller-manager. Enable the out-of-tree counterpart by setting --allocate-node-cidrs=true in cloud-controller-manager. To use RangeAllocator:  configure the --cluster-cidr, --service-cluster-ip-range and --node-cidr-mask-size; if you enable the ipv6 dualstack, setting --node-cidr-mask-size-ipv4 and --node-cidr-mask-size-ipv6 instead of --node-cidr-mask-size. An error would be reported if --node-cidr-mask-size and --node-cidr-mask-size-ipv4 (or --node-cidr-mask-size-ipv6) are set to non-zero values at a time. If only --node-cidr-mask-size is set, which is not recommended, the --node-cidr-mask-size-ipv4 and --node-cidr-mask-size-ipv6 would be set to this value by default.   To use CloudAllocator:  set the --cidr-allocator-type=CloudAllocator; configure mask sizes of each VMSS/VMAS by tagging {\"kubernetesNodeCIDRMaskIPV4\": \"custom-mask-size\"} and {\"kubernetesNodeCIDRMaskIPV4\": \"custom-mask-size\"} if necessary.    Configurations kube-controller-manager kube-controller-manager would be configured with option --allocate-node-cidrs=false to disable the in-tree node IPAM controller.\ncloud-controller-manager The following configurations from cloud-controller-manager would be used as default options:\n   name type default description     allocate-node-cidrs bool true Should CIDRs for Pods be allocated and set on the cloud provider.   cluster-cidr string “10.244.0.0/16” CIDR Range for Pods in cluster. Requires –allocate-node-cidrs to be true. It will be ignored when enabling dualstack.   service-cluster-ip-range string \"\" CIDR Range for Services in cluster, this would get excluded from the allocatable range. Requires –allocate-node-cidrs to be true.   node-cidr-mask-size int 24 Mask size for node cidr in cluster. Default is 24 for IPv4 and 64 for IPv6.   node-cidr-mask-size-ipv4 int 24 Mask size for IPv4 node cidr in dual-stack cluster. Default is 24.   node-cidr-mask-size-ipv6 int 64 Mask size for IPv6 node cidr in dual-stack cluster. Default is 64.   cidr-allocator-type string “RangeAllocator” The CIDR allocator type. “RangeAllocator” or “CloudAllocator”.    Limitations  We plan to integrate out-of-tree node ipam controller with aks-engine to provider a better experience. Before that, the manual configuration is required. It is not supported to change the custom mask size value on the tag once it is set. For now, there is no e2e test covering this feature, so there can be potential bugs. It is not recommended enabling it in the production environment.  ","categories":"","description":"Usage of out-of-tree Node IPAM allocator.","excerpt":"Usage of out-of-tree Node IPAM allocator.","ref":"/cloud-provider-azure/topics/ipam/","tags":"","title":"Node IPAM controller"},{"body":"Changes by Kind Uncategorized  Fix InstanceV2.InstanceExists: it should return false instead of ErrInstanceNotFound (#1511, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v63.2.0+incompatible → v63.3.0+incompatible github.com/Azure/go-autorest/autorest: v0.11.25 → v0.11.26 github.com/google/pprof: 94a9f03 → 1a94d86 github.com/ianlancetaylor/demangle: 28f6c0f → 5e5cf60 k8s.io/cloud-provider: v0.21.10 → v0.21.11  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.17","excerpt":"Cloud Provider Azure v1.0.17","ref":"/cloud-provider-azure/blog/2022/04/19/v1.0.17/","tags":"","title":"v1.0.17"},{"body":"Changes by Kind Uncategorized  Add azure private link service client interface to delete a private endpoint connection. (#1497, @k8s-infra-cherrypick-robot) Fix InstanceV2.InstanceExists: it should return false instead of ErrInstanceNotFound (#1512, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/go-autorest/autorest: v0.11.25 → v0.11.26 k8s.io/cloud-provider: v0.22.7 → v0.22.8  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.13","excerpt":"Cloud Provider Azure v1.1.13","ref":"/cloud-provider-azure/blog/2022/04/19/v1.1.13/","tags":"","title":"v1.1.13"},{"body":"Changes by Kind Uncategorized  Add azure private link service client interface to delete a private endpoint connection. (#1496, @k8s-infra-cherrypick-robot) Fix InstanceV2.InstanceExists: it should return false instead of ErrInstanceNotFound (#1513, @k8s-infra-cherrypick-robot)  Dependencies Added  github.com/google/martian: v2.1.0+incompatible rsc.io/binaryregexp: v0.2.0 rsc.io/quote/v3: v3.1.0 rsc.io/sampler: v1.3.0  Changed  github.com/Azure/go-autorest/autorest: v0.11.25 → v0.11.26 k8s.io/api: v0.23.3 → v0.23.5 k8s.io/apimachinery: v0.23.3 → v0.23.5 k8s.io/apiserver: v0.23.3 → v0.23.5 k8s.io/client-go: v0.23.3 → v0.23.5 k8s.io/cloud-provider: v0.23.3 → v0.23.5 k8s.io/component-base: v0.23.3 → v0.23.5 k8s.io/component-helpers: v0.23.3 → v0.23.5 k8s.io/controller-manager: v0.23.3 → v0.23.5 k8s.io/kubelet: v0.23.3 → v0.23.5 sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.27 → v0.0.30  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.10","excerpt":"Cloud Provider Azure v1.23.10","ref":"/cloud-provider-azure/blog/2022/04/19/v1.23.10/","tags":"","title":"v1.23.10"},{"body":"Changes by Kind Bug or Regression  The cloud provider will create TCP probe rule if the probe protocol is not supported. (#1397, @MartinForReal)  Uncategorized  Fix a bug: If users specify a subnet name like “a————————————————–z”, it leads to “InvalidResourceName” error. (#1466, @k8s-infra-cherrypick-robot) UserAgents can now be passed as part of the cloud provider config (#1422, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v62.3.0+incompatible → v63.2.0+incompatible github.com/Azure/go-autorest/autorest/mocks: v0.4.1 → v0.4.2 github.com/Azure/go-autorest/autorest: v0.11.24 → v0.11.25 github.com/google/pprof: 94a9f03 → 1ebb73c github.com/ianlancetaylor/demangle: 28f6c0f → 5e5cf60 github.com/onsi/ginkgo/v2: v2.0.0 → v2.1.3 github.com/onsi/gomega: v1.18.1 → v1.19.0 golang.org/x/net: 69e39ba → 27dd868 golang.org/x/term: 7de9c90 → 03fcf44  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.19","excerpt":"Cloud Provider Azure v0.7.19","ref":"/cloud-provider-azure/blog/2022/04/13/v0.7.19/","tags":"","title":"v0.7.19"},{"body":"Changes by Kind Bug or Regression  The cloud provider will create TCP probe rule if the probe protocol is not supported. (#1396, @MartinForReal)  Uncategorized  Chore: add mixed protocol service e2e test (#1438, @k8s-infra-cherrypick-robot) Chore: detect data race in unit tests (#1436, @k8s-infra-cherrypick-robot) Chore: enable mixed protocol service feature gates in e2e templates (#1440, @k8s-infra-cherrypick-robot) Chore: only reconciling routes in cloud controller manager (#1449, @k8s-infra-cherrypick-robot) Fix a bug: If users specify a subnet name like “a————————————————–z”, it leads to “InvalidResourceName” error. (#1465, @k8s-infra-cherrypick-robot) Fix: disk attach/detach failure when operation is preempted (#1461, @k8s-infra-cherrypick-robot) Fix: panic due to nil pointer (#1442, @k8s-infra-cherrypick-robot) Fix: report an error when route table name is not configured (#1429, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v62.3.0+incompatible → v63.2.0+incompatible github.com/Azure/go-autorest/autorest/mocks: v0.4.1 → v0.4.2 github.com/Azure/go-autorest/autorest: v0.11.24 → v0.11.25 github.com/onsi/ginkgo/v2: v2.0.0 → v2.1.3 github.com/onsi/gomega: v1.18.1 → v1.19.0 golang.org/x/net: 491a49a → 27dd868 golang.org/x/term: 6a3ed07 → 03fcf44  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.16","excerpt":"Cloud Provider Azure v1.0.16","ref":"/cloud-provider-azure/blog/2022/04/13/v1.0.16/","tags":"","title":"v1.0.16"},{"body":"Changes by Kind Bug or Regression  The cloud provider will create TCP probe rule if the probe protocol is not supported. (#1395, @MartinForReal)  Uncategorized  Chore: add mixed protocol service e2e test (#1437, @k8s-infra-cherrypick-robot) Chore: detect data race in unit tests (#1435, @k8s-infra-cherrypick-robot) Chore: enable mixed protocol service feature gates in e2e templates (#1439, @k8s-infra-cherrypick-robot) Fix a bug: If users specify a subnet name like “a————————————————–z”, it leads to “InvalidResourceName” error. (#1467, @k8s-infra-cherrypick-robot) Fix: disk attach/detach failure when operation is preempted (#1459, @k8s-infra-cherrypick-robot) Fix: panic due to nil pointer (#1441, @k8s-infra-cherrypick-robot) Fix: report an error when route table name is not configured (#1428, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v62.3.0+incompatible → v63.2.0+incompatible github.com/Azure/go-autorest/autorest/mocks: v0.4.1 → v0.4.2 github.com/Azure/go-autorest/autorest: v0.11.24 → v0.11.25 github.com/google/pprof: 94a9f03 → 1a94d86 github.com/ianlancetaylor/demangle: 28f6c0f → 5e5cf60 github.com/onsi/ginkgo/v2: v2.0.0 → v2.1.3 github.com/onsi/gomega: v1.18.1 → v1.19.0 golang.org/x/net: 491a49a → 27dd868 golang.org/x/term: 6a3ed07 → 03fcf44  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.12","excerpt":"Cloud Provider Azure v1.1.12","ref":"/cloud-provider-azure/blog/2022/04/13/v1.1.12/","tags":"","title":"v1.1.12"},{"body":"Changes by Kind Bug or Regression  The cloud provider will create TCP probe rule if the probe protocol is not supported. (#1394, @MartinForReal)  Uncategorized  Fix a bug: If users specify a subnet name like “a————————————————–z”, it leads to “InvalidResourceName” error. (#1464, @k8s-infra-cherrypick-robot) Fix: disk attach/detach failure when operation is preempted (#1458, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v62.3.0+incompatible → v63.2.0+incompatible github.com/Azure/go-autorest/autorest/mocks: v0.4.1 → v0.4.2 github.com/Azure/go-autorest/autorest: v0.11.24 → v0.11.25 github.com/google/pprof: 94a9f03 → cbba55b github.com/onsi/ginkgo/v2: v2.0.0 → v2.1.3 github.com/onsi/gomega: v1.18.1 → v1.19.0 golang.org/x/net: 491a49a → 27dd868 golang.org/x/term: 6886f2d → 03fcf44  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.9","excerpt":"Cloud Provider Azure v1.23.9","ref":"/cloud-provider-azure/blog/2022/04/13/v1.23.9/","tags":"","title":"v1.23.9"},{"body":"Changes by Kind Bug or Regression  Fix: only check the frontend IP config that is owned by the service (#1323, @nilo19)  Uncategorized  Fixed a bug in health probe generator. Probe related configuration is not updated due to variable scope (#1324, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.17","excerpt":"Cloud Provider Azure v0.7.17","ref":"/cloud-provider-azure/blog/2022/03/25/v0.7.17/","tags":"","title":"v0.7.17"},{"body":"Changes by Kind Bug or Regression  Fix: only check the frontend IP config that is owned by the service (#1323, @nilo19) Fix: update load balancer rule when probe changes (#1350, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.18","excerpt":"Cloud Provider Azure v0.7.18","ref":"/cloud-provider-azure/blog/2022/03/25/v0.7.18/","tags":"","title":"v0.7.18"},{"body":"Changes by Kind Bug or Regression  Fix: only check the frontend IP config that is owned by the service (#1321, @nilo19) Fix: remove agent pool LB if that vmSet is changed to primary vmSet fix: reuse previous private IP address when changing load balancers (#1299, @nilo19)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v62.2.0+incompatible → v62.3.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.14","excerpt":"Cloud Provider Azure v1.0.14","ref":"/cloud-provider-azure/blog/2022/03/25/v1.0.14/","tags":"","title":"v1.0.14"},{"body":"Changes by Kind Bug or Regression  Fix: only check the frontend IP config that is owned by the service (#1321, @nilo19) Fix: update load balancer rule when probe changes (#1349, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.15","excerpt":"Cloud Provider Azure v1.0.15","ref":"/cloud-provider-azure/blog/2022/03/25/v1.0.15/","tags":"","title":"v1.0.15"},{"body":"Changes by Kind Bug or Regression  Fix: only check the frontend IP config that is owned by the service (#1320, @nilo19) Fix: remove agent pool LB if that vmSet is changed to primary vmSet fix: reuse previous private IP address when changing load balancers (#1298, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v62.2.0+incompatible → v62.3.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.10","excerpt":"Cloud Provider Azure v1.1.10","ref":"/cloud-provider-azure/blog/2022/03/25/v1.1.10/","tags":"","title":"v1.1.10"},{"body":"Changes by Kind Bug or Regression  Fix: only check the frontend IP config that is owned by the service (#1320, @nilo19) Fix: update load balancer rule when probe changes (#1348, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.11","excerpt":"Cloud Provider Azure v1.1.11","ref":"/cloud-provider-azure/blog/2022/03/25/v1.1.11/","tags":"","title":"v1.1.11"},{"body":"Changes by Kind Bug or Regression  Fix: only check the frontend IP config that is owned by the service (#1322, @nilo19)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.7","excerpt":"Cloud Provider Azure v1.23.7","ref":"/cloud-provider-azure/blog/2022/03/25/v1.23.7/","tags":"","title":"v1.23.7"},{"body":"Changes by Kind Bug or Regression  Fix: only check the frontend IP config that is owned by the service (#1322, @nilo19) Fix: update load balancer rule when probe changes (#1347, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.8","excerpt":"Cloud Provider Azure v1.23.8","ref":"/cloud-provider-azure/blog/2022/03/25/v1.23.8/","tags":"","title":"v1.23.8"},{"body":"Dependencies Added Nothing has changed.\nChanged  cloud.google.com/go/firestore: v1.6.1 → v1.1.0 github.com/Azure/azure-sdk-for-go: v62.0.0+incompatible → v62.3.0+incompatible github.com/armon/go-metrics: v0.3.10 → f0300d1 github.com/armon/go-radix: v1.0.0 → 7fddfc3 github.com/census-instrumentation/opencensus-proto: v0.3.0 → v0.2.1 github.com/coreos/go-systemd/v22: v22.3.2 → v22.1.0 github.com/envoyproxy/go-control-plane: v0.10.1 → 5f8ba28 github.com/envoyproxy/protoc-gen-validate: v0.6.2 → v0.1.0 github.com/fatih/color: v1.13.0 → v1.7.0 github.com/godbus/dbus/v5: v5.0.4 → v5.0.3 github.com/grpc-ecosystem/grpc-gateway: v1.16.0 → v1.9.5 github.com/hashicorp/consul/api: v1.11.0 → v1.1.0 github.com/hashicorp/consul/sdk: v0.8.0 → v0.1.1 github.com/hashicorp/go-cleanhttp: v0.5.2 → v0.5.1 github.com/hashicorp/go-immutable-radix: v1.3.1 → v1.0.0 github.com/hashicorp/go-multierror: v1.1.0 → v1.0.0 github.com/hashicorp/go-rootcerts: v1.0.2 → v1.0.0 github.com/hashicorp/mdns: v1.0.4 → v1.0.0 github.com/hashicorp/memberlist: v0.3.0 → v0.1.3 github.com/hashicorp/serf: v0.9.6 → v0.8.2 github.com/magiconair/properties: v1.8.5 → v1.8.1 github.com/mattn/go-colorable: v0.1.12 → v0.0.9 github.com/mattn/go-isatty: v0.0.14 → v0.0.4 github.com/miekg/dns: v1.1.41 → v1.1.4 github.com/mitchellh/cli: v1.1.0 → v1.0.0 github.com/mitchellh/mapstructure: v1.4.3 → v1.1.2 github.com/pascaldekloe/goe: v0.1.0 → 57f6aae github.com/pelletier/go-toml: v1.9.4 → v1.2.0 github.com/posener/complete: v1.2.3 → v1.1.1 github.com/rogpeppe/fastuuid: v1.2.0 → 6724a57 github.com/spf13/afero: v1.6.0 → v1.2.2 github.com/spf13/cast: v1.4.1 → v1.3.0 github.com/spf13/cobra: v1.3.0 → v1.4.0 github.com/spf13/viper: v1.10.0 → v1.7.0 github.com/stretchr/testify: v1.7.0 → v1.7.1 golang.org/x/mod: v0.5.0 → v0.4.2 gopkg.in/ini.v1: v1.66.2 → v1.51.0  Removed  github.com/DataDog/datadog-go: v3.2.0+incompatible github.com/antihax/optional: v1.0.0 github.com/circonus-labs/circonus-gometrics: v2.3.1+incompatible github.com/circonus-labs/circonusllhist: v0.1.3 github.com/cncf/udpa/go: 04548b0 github.com/cncf/xds/go: a8f9461 github.com/hashicorp/go-hclog: v1.0.0 github.com/hashicorp/go-retryablehttp: v0.5.3 github.com/iancoleman/strcase: v0.2.0 github.com/kr/fs: v0.1.0 github.com/lyft/protoc-gen-star: v0.5.3 github.com/pkg/sftp: v1.10.1 github.com/sagikazarmark/crypt: v0.3.0 github.com/tv42/httpunix: b75d861 go.etcd.io/etcd/api/v3: v3.5.1 go.etcd.io/etcd/client/pkg/v3: v3.5.1 go.etcd.io/etcd/client/v2: v2.305.1 go.opentelemetry.io/proto/otlp: v0.7.0  ","categories":"","description":"Cloud Provider Azure v0.7.16","excerpt":"Cloud Provider Azure v0.7.16","ref":"/cloud-provider-azure/blog/2022/03/22/v0.7.16/","tags":"","title":"v0.7.16"},{"body":"Changes by Kind Bug or Regression  Fix: remove agent pool LB if that vmSet is changed to primary vmSet fix: reuse previous private IP address when changing load balancers (#1299, @nilo19)  Uncategorized  Chore: add verbose logs for latency and operation start timestamps (#1260, @feiskyer)  Dependencies Added  go.uber.org/goleak: v1.1.10  Changed  cloud.google.com/go/firestore: v1.6.1 → v1.1.0 cloud.google.com/go: v0.99.0 → v0.65.0 github.com/Azure/azure-sdk-for-go: v62.0.0+incompatible → v62.2.0+incompatible github.com/armon/go-metrics: v0.3.10 → f0300d1 github.com/armon/go-radix: v1.0.0 → 7fddfc3 github.com/census-instrumentation/opencensus-proto: v0.3.0 → v0.2.1 github.com/envoyproxy/go-control-plane: v0.10.1 → 5f8ba28 github.com/envoyproxy/protoc-gen-validate: v0.6.2 → v0.1.0 github.com/fatih/color: v1.13.0 → v1.7.0 github.com/google/martian/v3: v3.2.1 → v3.0.0 github.com/google/pprof: 4bb14d4 → 94a9f03 github.com/googleapis/gax-go/v2: v2.1.1 → v2.0.5 github.com/hashicorp/consul/api: v1.11.0 → v1.1.0 github.com/hashicorp/consul/sdk: v0.8.0 → v0.1.1 github.com/hashicorp/go-cleanhttp: v0.5.2 → v0.5.1 github.com/hashicorp/go-immutable-radix: v1.3.1 → v1.0.0 github.com/hashicorp/go-multierror: v1.1.0 → v1.0.0 github.com/hashicorp/go-rootcerts: v1.0.2 → v1.0.0 github.com/hashicorp/mdns: v1.0.4 → v1.0.0 github.com/hashicorp/memberlist: v0.3.0 → v0.1.3 github.com/hashicorp/serf: v0.9.6 → v0.8.2 github.com/magiconair/properties: v1.8.5 → v1.8.1 github.com/mattn/go-colorable: v0.1.12 → v0.0.9 github.com/mattn/go-isatty: v0.0.14 → v0.0.4 github.com/miekg/dns: v1.1.41 → v1.0.14 github.com/mitchellh/cli: v1.1.0 → v1.0.0 github.com/mitchellh/mapstructure: v1.4.3 → v1.1.2 github.com/pascaldekloe/goe: v0.1.0 → 57f6aae github.com/pelletier/go-toml: v1.9.4 → v1.2.0 github.com/posener/complete: v1.2.3 → v1.1.1 github.com/spf13/cast: v1.4.1 → v1.3.0 github.com/spf13/cobra: v1.3.0 → v1.4.0 github.com/spf13/jwalterweatherman: v1.1.0 → v1.0.0 github.com/spf13/viper: v1.10.0 → v1.7.0 github.com/stretchr/testify: v1.7.0 → v1.7.1 go.opencensus.io: v0.23.0 → v0.22.4 golang.org/x/lint: 6edffad → 738671d golang.org/x/mod: v0.5.0 → v0.4.2 golang.org/x/tools: v0.1.5 → v0.1.1 google.golang.org/api: v0.62.0 → v0.30.0 gopkg.in/ini.v1: v1.66.2 → v1.51.0 k8s.io/api: v0.21.10 → v0.21.11 k8s.io/apimachinery: v0.21.10 → v0.21.11 k8s.io/apiserver: v0.21.10 → v0.21.11 k8s.io/client-go: v0.21.10 → v0.21.11 k8s.io/component-base: v0.21.10 → v0.21.11 k8s.io/controller-manager: v0.21.10 → v0.21.11 k8s.io/utils: da69540 → 6203023 sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.27 → v0.0.30  Removed  github.com/DataDog/datadog-go: v3.2.0+incompatible github.com/circonus-labs/circonus-gometrics: v2.3.1+incompatible github.com/circonus-labs/circonusllhist: v0.1.3 github.com/cncf/udpa/go: 04548b0 github.com/cncf/xds/go: a8f9461 github.com/coreos/go-systemd/v22: v22.3.2 github.com/godbus/dbus/v5: v5.0.4 github.com/golang/snappy: v0.0.3 github.com/hashicorp/go-hclog: v1.0.0 github.com/hashicorp/go-retryablehttp: v0.5.3 github.com/iancoleman/strcase: v0.2.0 github.com/lyft/protoc-gen-star: v0.5.3 github.com/sagikazarmark/crypt: v0.3.0 github.com/tv42/httpunix: b75d861 go.etcd.io/etcd/api/v3: v3.5.1 go.etcd.io/etcd/client/pkg/v3: v3.5.1 go.etcd.io/etcd/client/v2: v2.305.1 go.opentelemetry.io/proto/otlp: v0.7.0 google.golang.org/grpc/cmd/protoc-gen-go-grpc: v1.1.0  ","categories":"","description":"Cloud Provider Azure v1.0.13","excerpt":"Cloud Provider Azure v1.0.13","ref":"/cloud-provider-azure/blog/2022/03/22/v1.0.13/","tags":"","title":"v1.0.13"},{"body":"Changes by Kind Bug or Regression  Fix: remove agent pool LB if that vmSet is changed to primary vmSet fix: reuse previous private IP address when changing load balancers (#1298, @k8s-infra-cherrypick-robot)  Uncategorized  Chore: add verbose logs for latency and operation start timestamps (#1259, @feiskyer)  Dependencies Added Nothing has changed.\nChanged  cloud.google.com/go/firestore: v1.6.1 → v1.1.0 cloud.google.com/go: v0.99.0 → v0.65.0 github.com/Azure/azure-sdk-for-go: v62.0.0+incompatible → v62.2.0+incompatible github.com/armon/go-metrics: v0.3.10 → f0300d1 github.com/armon/go-radix: v1.0.0 → 7fddfc3 github.com/census-instrumentation/opencensus-proto: v0.3.0 → v0.2.1 github.com/cncf/xds/go: a8f9461 → cb28da3 github.com/envoyproxy/go-control-plane: v0.10.1 → cf90f65 github.com/envoyproxy/protoc-gen-validate: v0.6.2 → v0.1.0 github.com/fatih/color: v1.13.0 → v1.7.0 github.com/google/martian/v3: v3.2.1 → v3.0.0 github.com/google/pprof: 4bb14d4 → 94a9f03 github.com/googleapis/gax-go/v2: v2.1.1 → v2.0.5 github.com/hashicorp/consul/api: v1.11.0 → v1.1.0 github.com/hashicorp/consul/sdk: v0.8.0 → v0.1.1 github.com/hashicorp/go-cleanhttp: v0.5.2 → v0.5.1 github.com/hashicorp/go-immutable-radix: v1.3.1 → v1.0.0 github.com/hashicorp/go-multierror: v1.1.0 → v1.0.0 github.com/hashicorp/go-rootcerts: v1.0.2 → v1.0.0 github.com/hashicorp/golang-lru: v0.5.4 → v0.5.1 github.com/hashicorp/mdns: v1.0.4 → v1.0.0 github.com/hashicorp/memberlist: v0.3.0 → v0.1.3 github.com/hashicorp/serf: v0.9.6 → v0.8.2 github.com/magiconair/properties: v1.8.5 → v1.8.1 github.com/mattn/go-colorable: v0.1.12 → v0.0.9 github.com/mattn/go-isatty: v0.0.14 → v0.0.3 github.com/miekg/dns: v1.1.41 → v1.0.14 github.com/mitchellh/cli: v1.1.0 → v1.0.0 github.com/mitchellh/mapstructure: v1.4.3 → v1.1.2 github.com/pascaldekloe/goe: v0.1.0 → 57f6aae github.com/pelletier/go-toml: v1.9.4 → v1.2.0 github.com/posener/complete: v1.2.3 → v1.1.1 github.com/spf13/afero: v1.6.0 → v1.2.2 github.com/spf13/cast: v1.4.1 → v1.3.0 github.com/spf13/cobra: v1.3.0 → v1.4.0 github.com/spf13/jwalterweatherman: v1.1.0 → v1.0.0 github.com/spf13/viper: v1.10.0 → v1.7.0 github.com/stretchr/testify: v1.7.0 → v1.7.1 go.etcd.io/etcd/client/v2: v2.305.1 → v2.305.0 go.opencensus.io: v0.23.0 → v0.22.4 golang.org/x/mod: v0.5.0 → v0.4.2 golang.org/x/tools: v0.1.5 → v0.1.2 google.golang.org/api: v0.62.0 → v0.30.0 gopkg.in/ini.v1: v1.66.2 → v1.51.0 k8s.io/api: v0.22.7 → v0.22.8 k8s.io/apimachinery: v0.22.7 → v0.22.8 k8s.io/apiserver: v0.22.7 → v0.22.8 k8s.io/client-go: v0.22.7 → v0.22.8 k8s.io/component-base: v0.22.7 → v0.22.8 k8s.io/controller-manager: v0.22.7 → v0.22.8 sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.27 → v0.0.30  Removed  github.com/DataDog/datadog-go: v3.2.0+incompatible github.com/circonus-labs/circonus-gometrics: v2.3.1+incompatible github.com/circonus-labs/circonusllhist: v0.1.3 github.com/golang/snappy: v0.0.3 github.com/hashicorp/go-hclog: v1.0.0 github.com/hashicorp/go-retryablehttp: v0.5.3 github.com/iancoleman/strcase: v0.2.0 github.com/kr/fs: v0.1.0 github.com/lyft/protoc-gen-star: v0.5.3 github.com/pkg/sftp: v1.10.1 github.com/sagikazarmark/crypt: v0.3.0 github.com/tv42/httpunix: b75d861 google.golang.org/grpc/cmd/protoc-gen-go-grpc: v1.1.0  ","categories":"","description":"Cloud Provider Azure v1.1.9","excerpt":"Cloud Provider Azure v1.1.9","ref":"/cloud-provider-azure/blog/2022/03/22/v1.1.9/","tags":"","title":"v1.1.9"},{"body":"Changes by Kind Bug or Regression  Fix: remove agent pool LB if that vmSet is changed to primary vmSet fix: reuse previous private IP address when changing load balancers (#1297, @k8s-infra-cherrypick-robot) Fix: skip reconcileSharedLoadBalancer if the service is being deleted (#1270, @nilo19)  Uncategorized  Chore: add verbose logs for latency and operation start timestamps (#1258, @k8s-infra-cherrypick-robot) Fix: do not delete backend pool when reconciling lb backend pools (#1217, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  cloud.google.com/go/firestore: v1.6.1 → v1.1.0 github.com/Azure/azure-sdk-for-go: v62.0.0+incompatible → v62.3.0+incompatible github.com/armon/go-metrics: v0.3.10 → f0300d1 github.com/armon/go-radix: v1.0.0 → 7fddfc3 github.com/census-instrumentation/opencensus-proto: v0.3.0 → v0.2.1 github.com/cncf/xds/go: a8f9461 → cb28da3 github.com/envoyproxy/go-control-plane: v0.10.1 → cf90f65 github.com/envoyproxy/protoc-gen-validate: v0.6.2 → v0.1.0 github.com/fatih/color: v1.13.0 → v1.7.0 github.com/googleapis/gax-go/v2: v2.1.1 → v2.0.5 github.com/hashicorp/consul/api: v1.11.0 → v1.1.0 github.com/hashicorp/consul/sdk: v0.8.0 → v0.1.1 github.com/hashicorp/go-cleanhttp: v0.5.2 → v0.5.1 github.com/hashicorp/go-immutable-radix: v1.3.1 → v1.0.0 github.com/hashicorp/go-multierror: v1.1.0 → v1.0.0 github.com/hashicorp/go-rootcerts: v1.0.2 → v1.0.0 github.com/hashicorp/golang-lru: v0.5.4 → v0.5.1 github.com/hashicorp/mdns: v1.0.4 → v1.0.0 github.com/hashicorp/memberlist: v0.3.0 → v0.1.3 github.com/hashicorp/serf: v0.9.6 → v0.8.2 github.com/mattn/go-colorable: v0.1.12 → v0.0.9 github.com/mattn/go-isatty: v0.0.14 → v0.0.3 github.com/miekg/dns: v1.1.41 → v1.0.14 github.com/mitchellh/cli: v1.1.0 → v1.0.0 github.com/mitchellh/mapstructure: v1.4.3 → v1.4.1 github.com/pascaldekloe/goe: v0.1.0 → 57f6aae github.com/pelletier/go-toml: v1.9.4 → v1.9.3 github.com/posener/complete: v1.2.3 → v1.1.1 github.com/spf13/cast: v1.4.1 → v1.3.1 github.com/spf13/cobra: v1.3.0 → v1.4.0 github.com/spf13/viper: v1.10.0 → v1.8.1 github.com/stretchr/testify: v1.7.0 → v1.7.1 go.etcd.io/etcd/client/v2: v2.305.1 → v2.305.0 golang.org/x/mod: v0.5.0 → v0.4.2 google.golang.org/api: v0.62.0 → v0.44.0 gopkg.in/ini.v1: v1.66.2 → v1.62.0  Removed  github.com/DataDog/datadog-go: v3.2.0+incompatible github.com/circonus-labs/circonus-gometrics: v2.3.1+incompatible github.com/circonus-labs/circonusllhist: v0.1.3 github.com/hashicorp/go-hclog: v1.0.0 github.com/hashicorp/go-retryablehttp: v0.5.3 github.com/iancoleman/strcase: v0.2.0 github.com/lyft/protoc-gen-star: v0.5.3 github.com/sagikazarmark/crypt: v0.3.0 github.com/tv42/httpunix: b75d861  ","categories":"","description":"Cloud Provider Azure v1.23.6","excerpt":"Cloud Provider Azure v1.23.6","ref":"/cloud-provider-azure/blog/2022/03/22/v1.23.6/","tags":"","title":"v1.23.6"},{"body":"Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v61.6.0+incompatible → v62.0.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.15","excerpt":"Cloud Provider Azure v0.7.15","ref":"/cloud-provider-azure/blog/2022/03/04/v0.7.15/","tags":"","title":"v0.7.15"},{"body":"Changes by Kind Uncategorized  Fix: remove outdated ipv4 route when the corresponding node is deleted (#1186, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v61.6.0+incompatible → v62.0.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.12","excerpt":"Cloud Provider Azure v1.0.12","ref":"/cloud-provider-azure/blog/2022/03/04/v1.0.12/","tags":"","title":"v1.0.12"},{"body":"Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v61.3.0+incompatible → v62.0.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.8","excerpt":"Cloud Provider Azure v1.1.8","ref":"/cloud-provider-azure/blog/2022/03/04/v1.1.8/","tags":"","title":"v1.1.8"},{"body":"Changes by Kind Uncategorized  Fix: do not delete backend pool when reconciling lb backend pools (#1217, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v61.6.0+incompatible → v62.0.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.5","excerpt":"Cloud Provider Azure v1.23.5","ref":"/cloud-provider-azure/blog/2022/03/04/v1.23.5/","tags":"","title":"v1.23.5"},{"body":"Changes by Kind Feature   Following configuration will be applied to the all ports of service.\n“service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol” “service.beta.kubernetes.io/azure-load-balancer-health-probe-interval” “service.beta.kubernetes.io/azure-load-balancer-health-probe-num-of-probe” “service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path”\nIf health probe is needed, user should specify AppProtocol in port object of Service spec. And following protocols are supported: http, tcp, https\nAdditional annotations are added. where port is the port number of port object\nservice.beta.kubernetes.io/port_{port}_health-probe_interval service.beta.kubernetes.io/port_{port}_health-probe_num-of-probe service.beta.kubernetes.io/port_{port}_health-probe_request-path\nPlease refer to docs. (#1131, @MartinForReal)\n  Bug or Regression  This code change fixes the bug that UDP services would trigger unnecessary LoadBalancer updates. The root cause is that a field not working for non-TCP protocols is considered. ref: #1090 (#1108, @lzhecheng)  Uncategorized  If spec.LoadBalancerSourceRanges is specified and lb is in internal mode, LB is open for public access.(close by default for security reasons) (#1124, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v61.4.0+incompatible → v61.6.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.14","excerpt":"Cloud Provider Azure v0.7.14","ref":"/cloud-provider-azure/blog/2022/02/23/v0.7.14/","tags":"","title":"v0.7.14"},{"body":"Changes by Kind Feature   Following configuration will be applied to the all ports of service.\n“service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol” “service.beta.kubernetes.io/azure-load-balancer-health-probe-interval” “service.beta.kubernetes.io/azure-load-balancer-health-probe-num-of-probe” “service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path”\nIf health probe is needed, user should specify AppProtocol in port object of Service spec. And following protocols are supported: http, tcp, https\nAdditional annotations are added. where port is the port number of port object\nservice.beta.kubernetes.io/port_{port}_health-probe_interval service.beta.kubernetes.io/port_{port}_health-probe_num-of-probe service.beta.kubernetes.io/port_{port}_health-probe_request-path\nPlease refer to docs. (#1130, @MartinForReal)\n  Bug or Regression  This code change fixes the bug that UDP services would trigger unnecessary LoadBalancer updates. The root cause is that a field not working for non-TCP protocols is considered. ref: #1090 (#1107, @lzhecheng)  Uncategorized  Fix: remove outdated ipv4 route when the corresponding node is deleted (#1186, @k8s-infra-cherrypick-robot) If spec.LoadBalancerSourceRanges is specified and lb is in internal mode, LB is open for public access.(close by default for security reasons) (#1123, @k8s-infra-cherrypick-robot)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v61.4.0+incompatible → v61.6.0+incompatible k8s.io/api: v0.21.9 → v0.21.10 k8s.io/apimachinery: v0.21.9 → v0.21.10 k8s.io/apiserver: v0.21.9 → v0.21.10 k8s.io/client-go: v0.21.9 → v0.21.10 k8s.io/cloud-provider: v0.21.9 → v0.21.10 k8s.io/component-base: v0.21.9 → v0.21.10 k8s.io/controller-manager: v0.21.9 → v0.21.10  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.11","excerpt":"Cloud Provider Azure v1.0.11","ref":"/cloud-provider-azure/blog/2022/02/23/v1.0.11/","tags":"","title":"v1.0.11"},{"body":"Changes by Kind Feature   Following configuration will be applied to the all ports of service.\n“service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol” “service.beta.kubernetes.io/azure-load-balancer-health-probe-interval” “service.beta.kubernetes.io/azure-load-balancer-health-probe-num-of-probe” “service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path”\nIf health probe is needed, user should specify AppProtocol in port object of Service spec. And following protocols are supported: http, tcp, https\nAdditional annotations are added. where port is the port number of port object\nservice.beta.kubernetes.io/port_{port}_health-probe_interval service.beta.kubernetes.io/port_{port}_health-probe_num-of-probe service.beta.kubernetes.io/port_{port}_health-probe_request-path\nPlease refer to docs. (#1128, @MartinForReal)\n  Uncategorized  If spec.LoadBalancerSourceRanges is specified and lb is in internal mode, LB is open for public access.(close by default for security reasons) (#1122, @k8s-infra-cherrypick-robot) This code change fixes the bug that UDP services would trigger unnecessary LoadBalancer updates. The root cause is that a field not working for non-TCP protocols is considered. ref: #1090 (#1106, @lzhecheng)  Dependencies Added Nothing has changed.\nChanged  github.com/stretchr/objx: v0.2.0 → v0.1.1 k8s.io/api: v0.22.6 → v0.22.7 k8s.io/apimachinery: v0.22.6 → v0.22.7 k8s.io/apiserver: v0.22.6 → v0.22.7 k8s.io/client-go: v0.22.6 → v0.22.7 k8s.io/cloud-provider: v0.22.6 → v0.22.7 k8s.io/component-base: v0.22.6 → v0.22.7 k8s.io/controller-manager: v0.22.6 → v0.22.7 k8s.io/utils: bdf08cb → 6203023  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.7","excerpt":"Cloud Provider Azure v1.1.7","ref":"/cloud-provider-azure/blog/2022/02/23/v1.1.7/","tags":"","title":"v1.1.7"},{"body":"Changes by Kind Feature   Feat: support changing LB backend pool type between nodeIP and nodeIP… (#1125, @nilo19)\n  Following configuration will be applied to the all ports of service.\n“service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol” “service.beta.kubernetes.io/azure-load-balancer-health-probe-interval” “service.beta.kubernetes.io/azure-load-balancer-health-probe-num-of-probe” “service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path”\nIf health probe is needed, user should specify AppProtocol in port object of Service spec. And following protocols are supported: http, tcp, https\nAdditional annotations are added. where port is the port number of port object\nservice.beta.kubernetes.io/port_{port}_health-probe_interval service.beta.kubernetes.io/port_{port}_health-probe_num-of-probe service.beta.kubernetes.io/port_{port}_health-probe_request-path\nPlease refer to docs. (#1126, @MartinForReal)\n  Uncategorized  Feat: Support controller health check in ccm (#1144, @k8s-infra-cherrypick-robot) If spec.LoadBalancerSourceRanges is specified and lb is in internal mode, LB is open for public access.(close by default for security reasons) (#1121, @k8s-infra-cherrypick-robot) Increase Azure ACR credential provider timeout (#1169, @k8s-infra-cherrypick-robot) This code change fixes the bug that UDP services would trigger unnecessary LoadBalancer updates. The root cause is that a field not working for non-TCP protocols is considered. ref: #1090 (#1105, @lzhecheng)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v61.4.0+incompatible → v61.6.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.4","excerpt":"Cloud Provider Azure v1.23.4","ref":"/cloud-provider-azure/blog/2022/02/23/v1.23.4/","tags":"","title":"v1.23.4"},{"body":"Changelog since v0.7.12 Changes by Kind Feature  Chore: upgrade github.com/gogo/protobuf to v1.3.2, golang.org/x/crypto to v0.0.0-20220112180741-5e0467b6c7ce and golang.org/x/text to v0.3.7 (#1036, @lodrem) Feat: support platformSubFaultDomain node label (#1081, @nilo19)  Bug or Regression  Fix: use zones in the pre-existing frontend IP configurations for internal LBs (#1089, @nilo19)  Dependencies Added  github.com/DataDog/datadog-go: v3.2.0+incompatible github.com/OneOfOne/xxhash: v1.2.2 github.com/antihax/optional: v1.0.0 github.com/cespare/xxhash: v1.1.0 github.com/circonus-labs/circonus-gometrics: v2.3.1+incompatible github.com/circonus-labs/circonusllhist: v0.1.3 github.com/cncf/udpa/go: 04548b0 github.com/cncf/xds/go: a8f9461 github.com/dgryski/go-sip13: e10d5fe github.com/go-task/slim-sprig: 348f09d github.com/golang-jwt/jwt/v4: v4.2.0 github.com/hashicorp/go-hclog: v1.0.0 github.com/hashicorp/go-retryablehttp: v0.5.3 github.com/iancoleman/strcase: v0.2.0 github.com/kr/fs: v0.1.0 github.com/lyft/protoc-gen-star: v0.5.3 github.com/nxadm/tail: v1.4.8 github.com/oklog/ulid: v1.3.1 github.com/onsi/ginkgo/v2: v2.0.0 github.com/pkg/sftp: v1.10.1 github.com/prometheus/tsdb: v0.7.1 github.com/sagikazarmark/crypt: v0.3.0 github.com/spaolacci/murmur3: f09979e github.com/tv42/httpunix: b75d861 github.com/ugorji/go: v1.1.4 go.etcd.io/etcd/api/v3: v3.5.1 go.etcd.io/etcd/client/pkg/v3: v3.5.1 go.etcd.io/etcd/client/v2: v2.305.1 go.opentelemetry.io/proto/otlp: v0.7.0 gopkg.in/yaml.v3: 496545a rsc.io/quote/v3: v3.1.0 rsc.io/sampler: v1.3.0  Changed  cloud.google.com/go/firestore: v1.1.0 → v1.6.1 github.com/Azure/azure-sdk-for-go: v53.1.0+incompatible → v61.4.0+incompatible github.com/Azure/go-autorest/autorest/adal: v0.9.10 → v0.9.18 github.com/Azure/go-autorest/autorest/mocks: v0.3.0 → v0.4.1 github.com/Azure/go-autorest/autorest/to: v0.2.0 → v0.4.0 github.com/Azure/go-autorest/autorest: v0.11.17 → v0.11.24 github.com/Azure/go-autorest/logger: v0.2.0 → v0.2.1 github.com/armon/go-metrics: f0300d1 → v0.3.10 github.com/armon/go-radix: 7fddfc3 → v1.0.0 github.com/beorn7/perks: v1.0.0 → v1.0.1 github.com/blang/semver: v3.5.0+incompatible → v3.5.1+incompatible github.com/census-instrumentation/opencensus-proto: v0.2.1 → v0.3.0 github.com/cespare/xxhash/v2: v2.1.1 → v2.1.2 github.com/coreos/go-systemd/v22: v22.1.0 → v22.3.2 github.com/coreos/pkg: 97fdf19 → 399ea9e github.com/cpuguy83/go-md2man/v2: v2.0.0 → v2.0.1 github.com/creack/pty: v1.1.9 → v1.1.7 github.com/envoyproxy/go-control-plane: 5f8ba28 → v0.10.1 github.com/envoyproxy/protoc-gen-validate: v0.1.0 → v0.6.2 github.com/evanphx/json-patch: v4.2.0+incompatible → v5.6.0+incompatible github.com/fatih/color: v1.7.0 → v1.13.0 github.com/fsnotify/fsnotify: v1.4.9 → v1.5.1 github.com/go-logr/logr: v0.2.0 → v0.4.0 github.com/godbus/dbus/v5: v5.0.3 → v5.0.4 github.com/gofrs/uuid: v4.0.0+incompatible → v4.2.0+incompatible github.com/gogo/protobuf: v1.3.1 → v1.3.2 github.com/golang/groupcache: 02826c3 → 41bb18b github.com/golang/mock: v1.3.1 → v1.6.0 github.com/golang/protobuf: v1.3.2 → v1.5.2 github.com/google/cadvisor: v0.38.5 → v0.38.8 github.com/google/go-cmp: v0.3.0 → v0.5.6 github.com/google/pprof: 1ebb73c → 94a9f03 github.com/google/uuid: v1.1.1 → v1.1.2 github.com/googleapis/gax-go/v2: v2.0.5 → v2.1.1 github.com/grpc-ecosystem/grpc-gateway: v1.9.5 → v1.16.0 github.com/hashicorp/consul/api: v1.1.0 → v1.11.0 github.com/hashicorp/consul/sdk: v0.1.1 → v0.8.0 github.com/hashicorp/go-cleanhttp: v0.5.1 → v0.5.2 github.com/hashicorp/go-immutable-radix: v1.0.0 → v1.3.1 github.com/hashicorp/go-multierror: v1.0.0 → v1.1.0 github.com/hashicorp/go-rootcerts: v1.0.0 → v1.0.2 github.com/hashicorp/golang-lru: v0.5.1 → v0.5.4 github.com/hashicorp/mdns: v1.0.0 → v1.0.4 github.com/hashicorp/memberlist: v0.1.3 → v0.3.0 github.com/hashicorp/serf: v0.8.2 → v0.9.6 github.com/ianlancetaylor/demangle: 5e5cf60 → 28f6c0f github.com/json-iterator/go: v1.1.8 → v1.1.12 github.com/kisielk/errcheck: v1.2.0 → v1.5.0 github.com/konsorten/go-windows-terminal-sequences: v1.0.1 → v1.0.3 github.com/kr/text: v0.2.0 → v0.1.0 github.com/magiconair/properties: v1.8.1 → v1.8.5 github.com/mattn/go-colorable: v0.0.9 → v0.1.12 github.com/mattn/go-isatty: v0.0.4 → v0.0.14 github.com/matttproud/golang_protobuf_extensions: v1.0.1 → c182aff github.com/miekg/dns: v1.1.4 → v1.1.41 github.com/mitchellh/cli: v1.0.0 → v1.1.0 github.com/mitchellh/mapstructure: v1.1.2 → v1.4.3 github.com/modern-go/reflect2: v1.0.1 → v1.0.2 github.com/onsi/ginkgo: v1.11.0 → v1.16.5 github.com/onsi/gomega: v1.7.0 → v1.18.1 github.com/opencontainers/go-digest: v1.0.0-rc1 → v1.0.0 github.com/pascaldekloe/goe: 57f6aae → v0.1.0 github.com/pelletier/go-toml: v1.2.0 → v1.9.4 github.com/pkg/errors: v0.8.1 → v0.9.1 github.com/posener/complete: v1.1.1 → v1.2.3 github.com/prometheus/procfs: v0.0.2 → v0.2.0 github.com/rogpeppe/fastuuid: 6724a57 → v1.2.0 github.com/rubiojr/go-vhd: 0bfd3b3 → 02e2102 github.com/russross/blackfriday/v2: v2.0.1 → v2.1.0 github.com/sirupsen/logrus: v1.4.2 → v1.6.0 github.com/spf13/afero: v1.2.2 → v1.6.0 github.com/spf13/cast: v1.3.0 → v1.4.1 github.com/spf13/cobra: v0.0.5 → v1.3.0 github.com/spf13/viper: v1.7.0 → v1.10.0 github.com/stretchr/testify: v1.4.0 → v1.7.0 github.com/yuin/goldmark: v1.1.27 → v1.3.5 go.etcd.io/etcd: 262c939 → dd1b699 go.opencensus.io: v0.22.3 → v0.23.0 go.uber.org/atomic: v1.3.2 → v1.7.0 go.uber.org/multierr: v1.1.0 → v1.6.0 go.uber.org/zap: v1.10.0 → v1.17.0 golang.org/x/crypto: bac4c82 → 5e0467b golang.org/x/lint: 738671d → 6edffad golang.org/x/mod: v0.3.0 → v0.5.0 golang.org/x/net: ac852fb → 69e39ba golang.org/x/oauth2: 0f29369 → d3ed0bb golang.org/x/sync: cd5d95a → 036812b golang.org/x/sys: 742c48e → 1d35b9e golang.org/x/text: v0.3.2 → v0.3.7 golang.org/x/time: 9d24e82 → 3af7569 golang.org/x/tools: c1934b7 → v0.1.5 google.golang.org/api: v0.20.0 → v0.62.0 google.golang.org/appengine: v1.5.0 → v1.6.7 google.golang.org/genproto: 24fa4b2 → 3a66f56 google.golang.org/grpc: v1.27.1 → v1.27.0 google.golang.org/protobuf: v1.25.0 → v1.27.1 gopkg.in/check.v1: 8fa4692 → 41f04d3 gopkg.in/ini.v1: v1.51.0 → v1.66.2 gopkg.in/yaml.v2: v2.2.8 → v2.4.0 k8s.io/api: fcac651 → v0.20.15 k8s.io/apiextensions-apiserver: a7ee1ef → v0.20.15 k8s.io/apimachinery: 15c5dba → v0.20.15 k8s.io/apiserver: aed7ab0 → v0.20.15 k8s.io/cli-runtime: 2e4b259 → v0.20.15 k8s.io/client-go: e24efdc → v0.20.15 k8s.io/cloud-provider: 82fca6d → v0.20.15 k8s.io/cluster-bootstrap: 614b98e → v0.20.15 k8s.io/code-generator: v0.21.0-alpha.0 → v0.20.15 k8s.io/component-base: 1e84b32 → v0.20.15 k8s.io/component-helpers: 7cb42b6 → v0.20.15 k8s.io/controller-manager: b2c380a → v0.20.15 k8s.io/cri-api: v0.21.0-alpha.0 → v0.20.15 k8s.io/csi-translation-lib: 8333033 → v0.20.15 k8s.io/gengo: e0e292d → 83324d8 k8s.io/klog/v2: v2.4.0 → v2.9.0 k8s.io/kube-aggregator: 6c47de4 → v0.20.15 k8s.io/kube-controller-manager: 18c28a4 → v0.20.15 k8s.io/kube-openapi: d219536 → 83f114c k8s.io/kube-proxy: deb12d4 → v0.20.15 k8s.io/kube-scheduler: 0f62d39 → v0.20.15 k8s.io/kubectl: 5cfbd40 → v0.20.15 k8s.io/kubelet: 92ded5e → v0.20.15 k8s.io/kubernetes: f58c4d8 → v1.20.15 k8s.io/legacy-cloud-providers: 716c3da → v0.20.15 k8s.io/metrics: d70c0e0 → v0.20.15 k8s.io/mount-utils: v0.21.0-alpha.0 → v0.20.15 k8s.io/sample-apiserver: 1f4e6a9 → v0.20.15 k8s.io/utils: 6e3d28b → 67b214c sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.9 → v0.0.22 sigs.k8s.io/structured-merge-diff/v4: v4.0.2 → v4.1.2  Removed  github.com/coreos/go-etcd: v2.0.0+incompatible github.com/cpuguy83/go-md2man: v1.0.10 github.com/niemeyer/pretty: a10e7ca github.com/ugorji/go/codec: d75b2dc rsc.io/binaryregexp: v0.2.0  ","categories":"","description":"Cloud Provider Azure v0.7.13\n","excerpt":"Cloud Provider Azure v0.7.13\n","ref":"/cloud-provider-azure/blog/2022/02/07/v0.7.13/","tags":"","title":"v0.7.13"},{"body":"Changelog since v1.0.9 Changes by Kind Feature  Feat: support platformSubFaultDomain node label (#1082, @nilo19)  Bug or Regression  Fix: use zones in the pre-existing frontend IP configurations for internal LBs (#1095, @nilo19)  Dependencies Added  github.com/DataDog/datadog-go: v3.2.0+incompatible github.com/OneOfOne/xxhash: v1.2.2 github.com/antihax/optional: v1.0.0 github.com/cespare/xxhash: v1.1.0 github.com/circonus-labs/circonus-gometrics: v2.3.1+incompatible github.com/circonus-labs/circonusllhist: v0.1.3 github.com/cncf/udpa/go: 04548b0 github.com/cncf/xds/go: a8f9461 github.com/coreos/go-systemd/v22: v22.3.2 github.com/dgryski/go-sip13: e10d5fe github.com/go-gl/glfw: e6da0ac github.com/go-stack/stack: v1.8.0 github.com/go-task/slim-sprig: 348f09d github.com/godbus/dbus/v5: v5.0.4 github.com/golang-jwt/jwt/v4: v4.2.0 github.com/golang/snappy: v0.0.3 github.com/google/martian/v3: v3.2.1 github.com/hashicorp/go-hclog: v1.0.0 github.com/hashicorp/go-retryablehttp: v0.5.3 github.com/iancoleman/strcase: v0.2.0 github.com/konsorten/go-windows-terminal-sequences: v1.0.1 github.com/kr/fs: v0.1.0 github.com/kr/pty: v1.1.5 github.com/lyft/protoc-gen-star: v0.5.3 github.com/nxadm/tail: v1.4.8 github.com/oklog/ulid: v1.3.1 github.com/onsi/ginkgo/v2: v2.0.0 github.com/pkg/sftp: v1.10.1 github.com/prometheus/tsdb: v0.7.1 github.com/sagikazarmark/crypt: v0.3.0 github.com/spaolacci/murmur3: f09979e github.com/tv42/httpunix: b75d861 go.etcd.io/etcd/api/v3: v3.5.1 go.etcd.io/etcd/client/pkg/v3: v3.5.1 go.etcd.io/etcd/client/v2: v2.305.1 go.opentelemetry.io/proto/otlp: v0.7.0 google.golang.org/grpc/cmd/protoc-gen-go-grpc: v1.1.0 rsc.io/binaryregexp: v0.2.0  Changed  cloud.google.com/go/bigquery: v1.4.0 → v1.8.0 cloud.google.com/go/firestore: v1.1.0 → v1.6.1 cloud.google.com/go/pubsub: v1.2.0 → v1.3.1 cloud.google.com/go/storage: v1.6.0 → v1.10.0 cloud.google.com/go: v0.54.0 → v0.99.0 github.com/Azure/azure-sdk-for-go: v54.1.0+incompatible → v61.4.0+incompatible github.com/Azure/go-autorest/autorest/adal: v0.9.10 → v0.9.18 github.com/Azure/go-autorest/autorest/to: v0.2.0 → v0.4.0 github.com/Azure/go-autorest/autorest/validation: v0.1.0 → v0.3.1 github.com/Azure/go-autorest/autorest: v0.11.17 → v0.11.24 github.com/Azure/go-autorest/logger: v0.2.0 → v0.2.1 github.com/armon/go-metrics: f0300d1 → v0.3.10 github.com/armon/go-radix: 7fddfc3 → v1.0.0 github.com/census-instrumentation/opencensus-proto: v0.2.1 → v0.3.0 github.com/cespare/xxhash/v2: v2.1.1 → v2.1.2 github.com/cpuguy83/go-md2man/v2: v2.0.0 → v2.0.1 github.com/dnaeon/go-vcr: v1.1.0 → v1.2.0 github.com/envoyproxy/go-control-plane: 5f8ba28 → v0.10.1 github.com/envoyproxy/protoc-gen-validate: v0.1.0 → v0.6.2 github.com/evanphx/json-patch: v4.9.0+incompatible → v5.6.0+incompatible github.com/fatih/color: v1.7.0 → v1.13.0 github.com/fsnotify/fsnotify: v1.4.7 → v1.5.1 github.com/gofrs/uuid: v4.0.0+incompatible → v4.2.0+incompatible github.com/golang/groupcache: 8c9f03a → 41bb18b github.com/golang/mock: v1.4.1 → v1.6.0 github.com/golang/protobuf: v1.4.3 → v1.5.2 github.com/google/go-cmp: v0.5.2 → v0.5.6 github.com/google/pprof: 1ebb73c → 4bb14d4 github.com/googleapis/gax-go/v2: v2.0.5 → v2.1.1 github.com/grpc-ecosystem/grpc-gateway: v1.9.5 → v1.16.0 github.com/hashicorp/consul/api: v1.1.0 → v1.11.0 github.com/hashicorp/consul/sdk: v0.1.1 → v0.8.0 github.com/hashicorp/go-cleanhttp: v0.5.1 → v0.5.2 github.com/hashicorp/go-immutable-radix: v1.0.0 → v1.3.1 github.com/hashicorp/go-multierror: v1.0.0 → v1.1.0 github.com/hashicorp/go-rootcerts: v1.0.0 → v1.0.2 github.com/hashicorp/golang-lru: v0.5.1 → v0.5.4 github.com/hashicorp/mdns: v1.0.0 → v1.0.4 github.com/hashicorp/memberlist: v0.1.3 → v0.3.0 github.com/hashicorp/serf: v0.8.2 → v0.9.6 github.com/ianlancetaylor/demangle: 5e5cf60 → 28f6c0f github.com/json-iterator/go: v1.1.10 → v1.1.12 github.com/magiconair/properties: v1.8.1 → v1.8.5 github.com/mattn/go-colorable: v0.0.9 → v0.1.12 github.com/mattn/go-isatty: v0.0.4 → v0.0.14 github.com/miekg/dns: v1.0.14 → v1.1.41 github.com/mitchellh/cli: v1.0.0 → v1.1.0 github.com/mitchellh/mapstructure: v1.1.2 → v1.4.3 github.com/modern-go/reflect2: v1.0.1 → v1.0.2 github.com/onsi/ginkgo: v1.11.0 → v1.16.5 github.com/onsi/gomega: v1.8.1 → v1.18.1 github.com/pascaldekloe/goe: 57f6aae → v0.1.0 github.com/pelletier/go-toml: v1.2.0 → v1.9.4 github.com/posener/complete: v1.1.1 → v1.2.3 github.com/rogpeppe/fastuuid: 6724a57 → v1.2.0 github.com/rubiojr/go-vhd: 02e2102 → ccecf6c github.com/russross/blackfriday/v2: v2.0.1 → v2.1.0 github.com/sirupsen/logrus: v1.7.0 → v1.8.1 github.com/spf13/afero: v1.2.2 → v1.6.0 github.com/spf13/cast: v1.3.0 → v1.4.1 github.com/spf13/cobra: v1.1.1 → v1.3.0 github.com/spf13/jwalterweatherman: v1.0.0 → v1.1.0 github.com/spf13/viper: v1.7.0 → v1.10.0 github.com/stretchr/testify: v1.6.1 → v1.7.0 github.com/yuin/goldmark: v1.2.1 → v1.3.5 go.opencensus.io: v0.22.3 → v0.23.0 go.uber.org/atomic: v1.6.0 → v1.7.0 go.uber.org/multierr: v1.5.0 → v1.6.0 go.uber.org/zap: v1.16.0 → v1.17.0 golang.org/x/crypto: 5ea612d → 5e0467b golang.org/x/lint: 738671d → 6edffad golang.org/x/mod: ce943fd → v0.5.0 golang.org/x/net: 3d97a24 → 491a49a golang.org/x/oauth2: bf48bf1 → d3ed0bb golang.org/x/sync: 67f06af → 036812b golang.org/x/sys: a50acf3 → 1d35b9e golang.org/x/text: v0.3.4 → v0.3.7 golang.org/x/tools: v0.1.0 → v0.1.5 google.golang.org/api: v0.20.0 → v0.62.0 google.golang.org/appengine: v1.6.5 → v1.6.7 google.golang.org/genproto: 8816d57 → 3a66f56 google.golang.org/protobuf: v1.25.0 → v1.27.1 gopkg.in/ini.v1: v1.51.0 → v1.66.2 gopkg.in/yaml.v3: 9f266ea → 496545a honnef.co/go/tools: v0.0.1-2020.1.3 → v0.0.1-2020.1.4 k8s.io/api: 648b778 → v0.21.9 k8s.io/apimachinery: 8daf289 → v0.21.9 k8s.io/apiserver: 940c107 → v0.21.9 k8s.io/client-go: 8c8fa70 → v0.21.9 k8s.io/cloud-provider: 1ea896e → v0.21.9 k8s.io/component-base: 5860d9b → v0.21.9 k8s.io/controller-manager: 146a790 → v0.21.9 k8s.io/klog/v2: v2.8.0 → v2.9.0 k8s.io/kube-openapi: 591a79e → 3cc51fd k8s.io/utils: 67b214c → da69540 sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.15 → v0.0.27 sigs.k8s.io/structured-merge-diff/v4: v4.1.1 → v4.2.1  Removed  go.uber.org/tools: 2cfd321  ","categories":"","description":"Cloud Provider Azure v1.0.10\n","excerpt":"Cloud Provider Azure v1.0.10\n","ref":"/cloud-provider-azure/blog/2022/02/07/v1.0.10/","tags":"","title":"v1.0.10"},{"body":"Changelog since v1.1.5 Changes by Kind Feature  Feat: support platformSubFaultDomain node label (#1083, @nilo19)  Bug or Regression  Fix: use zones in the pre-existing frontend IP configurations for internal LBs (#1096, @nilo19)  Dependencies Added  github.com/DataDog/datadog-go: v3.2.0+incompatible github.com/circonus-labs/circonus-gometrics: v2.3.1+incompatible github.com/circonus-labs/circonusllhist: v0.1.3 github.com/cncf/xds/go: a8f9461 github.com/golang-jwt/jwt/v4: v4.2.0 github.com/golang/snappy: v0.0.3 github.com/hashicorp/go-hclog: v1.0.0 github.com/hashicorp/go-retryablehttp: v0.5.3 github.com/iancoleman/strcase: v0.2.0 github.com/lyft/protoc-gen-star: v0.5.3 github.com/onsi/ginkgo/v2: v2.0.0 github.com/sagikazarmark/crypt: v0.3.0 github.com/tv42/httpunix: b75d861 google.golang.org/grpc/cmd/protoc-gen-go-grpc: v1.1.0  Changed  cloud.google.com/go/firestore: v1.1.0 → v1.6.1 cloud.google.com/go: v0.81.0 → v0.99.0 github.com/Azure/azure-sdk-for-go: v55.8.0+incompatible → v61.3.0+incompatible github.com/Azure/go-autorest/autorest/adal: v0.9.14 → v0.9.18 github.com/Azure/go-autorest/autorest: v0.11.19 → v0.11.24 github.com/armon/go-metrics: f0300d1 → v0.3.10 github.com/armon/go-radix: 7fddfc3 → v1.0.0 github.com/bketelsen/crypt: v0.0.4 → 5cbc8cc github.com/census-instrumentation/opencensus-proto: v0.2.1 → v0.3.0 github.com/cespare/xxhash/v2: v2.1.1 → v2.1.2 github.com/cncf/udpa/go: 5459f2c → 04548b0 github.com/cpuguy83/go-md2man/v2: v2.0.0 → v2.0.1 github.com/envoyproxy/go-control-plane: 668b12f → v0.10.1 github.com/envoyproxy/protoc-gen-validate: v0.1.0 → v0.6.2 github.com/evanphx/json-patch: v4.11.0+incompatible → v5.6.0+incompatible github.com/fatih/color: v1.7.0 → v1.13.0 github.com/fsnotify/fsnotify: v1.4.9 → v1.5.1 github.com/google/go-cmp: v0.5.5 → v0.5.6 github.com/google/martian/v3: v3.1.0 → v3.2.1 github.com/google/pprof: cbba55b → 4bb14d4 github.com/googleapis/gax-go/v2: v2.0.5 → v2.1.1 github.com/hashicorp/consul/api: v1.1.0 → v1.11.0 github.com/hashicorp/consul/sdk: v0.1.1 → v0.8.0 github.com/hashicorp/go-cleanhttp: v0.5.1 → v0.5.2 github.com/hashicorp/go-immutable-radix: v1.0.0 → v1.3.1 github.com/hashicorp/go-multierror: v1.0.0 → v1.1.0 github.com/hashicorp/go-rootcerts: v1.0.0 → v1.0.2 github.com/hashicorp/golang-lru: v0.5.1 → v0.5.4 github.com/hashicorp/mdns: v1.0.0 → v1.0.4 github.com/hashicorp/memberlist: v0.1.3 → v0.3.0 github.com/hashicorp/serf: v0.8.2 → v0.9.6 github.com/json-iterator/go: v1.1.11 → v1.1.12 github.com/mattn/go-colorable: v0.0.9 → v0.1.12 github.com/mattn/go-isatty: v0.0.3 → v0.0.14 github.com/miekg/dns: v1.0.14 → v1.1.41 github.com/mitchellh/cli: v1.0.0 → v1.1.0 github.com/mitchellh/mapstructure: v1.4.1 → v1.4.3 github.com/modern-go/reflect2: v1.0.1 → v1.0.2 github.com/onsi/ginkgo: v1.16.4 → v1.16.5 github.com/onsi/gomega: v1.15.0 → v1.18.1 github.com/pascaldekloe/goe: 57f6aae → v0.1.0 github.com/pelletier/go-toml: v1.9.3 → v1.9.4 github.com/posener/complete: v1.1.1 → v1.2.3 github.com/rubiojr/go-vhd: 02e2102 → ccecf6c github.com/russross/blackfriday/v2: v2.0.1 → v2.1.0 github.com/spf13/cast: v1.3.1 → v1.4.1 github.com/spf13/cobra: v1.2.1 → v1.3.0 github.com/spf13/viper: v1.8.1 → v1.10.0 go.etcd.io/etcd/api/v3: v3.5.0 → v3.5.1 go.etcd.io/etcd/client/pkg/v3: v3.5.0 → v3.5.1 go.etcd.io/etcd/client/v2: v2.305.0 → v2.305.1 golang.org/x/crypto: 5ea612d → e495a2d golang.org/x/mod: v0.4.2 → v0.5.0 golang.org/x/net: 37e1c6a → 491a49a golang.org/x/oauth2: 2e8d934 → d3ed0bb golang.org/x/sys: 59db8d7 → 1d35b9e golang.org/x/text: v0.3.6 → v0.3.7 golang.org/x/tools: v0.1.2 → v0.1.5 google.golang.org/api: v0.44.0 → v0.62.0 google.golang.org/genproto: f16073e → 3a66f56 google.golang.org/grpc: v1.38.0 → v1.42.0 google.golang.org/protobuf: v1.26.0 → v1.27.1 gopkg.in/ini.v1: v1.62.0 → v1.66.2 k8s.io/api: v0.22.0 → v0.22.6 k8s.io/apimachinery: v0.22.0 → v0.22.6 k8s.io/apiserver: v0.22.0 → v0.22.6 k8s.io/client-go: v0.22.0 → v0.22.6 k8s.io/cloud-provider: v0.22.0 → v0.22.6 k8s.io/component-base: v0.22.0 → v0.22.6 k8s.io/controller-manager: v0.22.0 → v0.22.6 k8s.io/kube-openapi: 9528897 → 2043435 k8s.io/utils: 4b05e18 → bdf08cb sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.22 → v0.0.27 sigs.k8s.io/structured-merge-diff/v4: v4.1.2 → v4.2.1 sigs.k8s.io/yaml: v1.2.0 → v1.3.0  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.6\n","excerpt":"Cloud Provider Azure v1.1.6\n","ref":"/cloud-provider-azure/blog/2022/02/07/v1.1.6/","tags":"","title":"v1.1.6"},{"body":"Changelog since v1.23.2 Changes by Kind Feature  Feat: support platformSubFaultDomain node label (#1080, @k8s-infra-cherrypick-robot)  Bug or Regression  Fix: use zones in the pre-existing frontend IP configurations for internal LBs (#1097, @nilo19)  Dependencies Added  cloud.google.com/go/datastore: v1.1.0 cloud.google.com/go/pubsub: v1.3.1 github.com/DataDog/datadog-go: v3.2.0+incompatible github.com/alecthomas/template: fb15b89 github.com/alecthomas/units: f65c72e github.com/circonus-labs/circonus-gometrics: v2.3.1+incompatible github.com/circonus-labs/circonusllhist: v0.1.3 github.com/client9/misspell: v0.3.4 github.com/coreos/bbolt: v1.3.2 github.com/coreos/etcd: v3.3.13+incompatible github.com/coreos/go-systemd: 95778df github.com/coreos/pkg: 399ea9e github.com/dgrijalva/jwt-go: v3.2.0+incompatible github.com/dgryski/go-sip13: e10d5fe github.com/go-gl/glfw: e6da0ac github.com/hashicorp/go-hclog: v1.0.0 github.com/hashicorp/go-retryablehttp: v0.5.3 github.com/hpcloud/tail: v1.0.0 github.com/iancoleman/strcase: v0.2.0 github.com/jpillora/backoff: v1.0.0 github.com/konsorten/go-windows-terminal-sequences: v1.0.3 github.com/kr/logfmt: b84e30a github.com/kr/pty: v1.1.1 github.com/lyft/protoc-gen-star: v0.5.3 github.com/oklog/ulid: v1.3.1 github.com/onsi/ginkgo/v2: v2.0.0 github.com/prometheus/tsdb: v0.7.1 github.com/sagikazarmark/crypt: v0.3.0 github.com/tv42/httpunix: b75d861 gopkg.in/fsnotify.v1: v1.4.7 gopkg.in/resty.v1: v1.12.0  Changed  cloud.google.com/go/firestore: v1.1.0 → v1.6.1 github.com/Azure/azure-sdk-for-go: v55.8.0+incompatible → v61.4.0+incompatible github.com/Azure/go-autorest/autorest/adal: v0.9.17 → v0.9.18 github.com/Azure/go-autorest/autorest: v0.11.22 → v0.11.24 github.com/armon/go-metrics: f0300d1 → v0.3.10 github.com/armon/go-radix: 7fddfc3 → v1.0.0 github.com/census-instrumentation/opencensus-proto: v0.2.1 → v0.3.0 github.com/cespare/xxhash/v2: v2.1.1 → v2.1.2 github.com/cncf/udpa/go: 5459f2c → 04548b0 github.com/cncf/xds/go: fbca930 → a8f9461 github.com/cpuguy83/go-md2man/v2: v2.0.0 → v2.0.1 github.com/envoyproxy/go-control-plane: 63b5d3c → v0.10.1 github.com/envoyproxy/protoc-gen-validate: v0.1.0 → v0.6.2 github.com/evanphx/json-patch: v4.12.0+incompatible → v5.6.0+incompatible github.com/fatih/color: v1.7.0 → v1.13.0 github.com/golang-jwt/jwt/v4: v4.0.0 → v4.2.0 github.com/google/go-cmp: v0.5.5 → v0.5.6 github.com/google/pprof: cbba55b → 94a9f03 github.com/googleapis/gax-go/v2: v2.0.5 → v2.1.1 github.com/hashicorp/consul/api: v1.1.0 → v1.11.0 github.com/hashicorp/consul/sdk: v0.1.1 → v0.8.0 github.com/hashicorp/go-cleanhttp: v0.5.1 → v0.5.2 github.com/hashicorp/go-immutable-radix: v1.0.0 → v1.3.1 github.com/hashicorp/go-multierror: v1.0.0 → v1.1.0 github.com/hashicorp/go-rootcerts: v1.0.0 → v1.0.2 github.com/hashicorp/golang-lru: v0.5.1 → v0.5.4 github.com/hashicorp/mdns: v1.0.0 → v1.0.4 github.com/hashicorp/memberlist: v0.1.3 → v0.3.0 github.com/hashicorp/serf: v0.8.2 → v0.9.6 github.com/mattn/go-colorable: v0.0.9 → v0.1.12 github.com/mattn/go-isatty: v0.0.3 → v0.0.14 github.com/miekg/dns: v1.0.14 → v1.1.41 github.com/mitchellh/cli: v1.0.0 → v1.1.0 github.com/mitchellh/go-homedir: v1.0.0 → v1.1.0 github.com/mitchellh/mapstructure: v1.4.1 → v1.4.3 github.com/onsi/gomega: v1.16.0 → v1.18.1 github.com/pascaldekloe/goe: 57f6aae → v0.1.0 github.com/pelletier/go-toml: v1.9.3 → v1.9.4 github.com/posener/complete: v1.1.1 → v1.2.3 github.com/russross/blackfriday/v2: v2.0.1 → v2.1.0 github.com/spf13/cast: v1.3.1 → v1.4.1 github.com/spf13/cobra: v1.2.1 → v1.3.0 github.com/spf13/viper: v1.8.1 → v1.10.0 go.etcd.io/etcd/api/v3: v3.5.0 → v3.5.1 go.etcd.io/etcd/client/pkg/v3: v3.5.0 → v3.5.1 go.etcd.io/etcd/client/v2: v2.305.0 → v2.305.1 golang.org/x/crypto: 089bfa5 → 5e0467b golang.org/x/mod: v0.4.2 → v0.5.0 golang.org/x/net: e898025 → 491a49a golang.org/x/oauth2: 2bc19b1 → d3ed0bb golang.org/x/sys: f4d4317 → da31bd3 google.golang.org/api: v0.44.0 → v0.62.0 google.golang.org/genproto: fe13028 → 3a66f56 google.golang.org/grpc: v1.40.0 → v1.42.0 gopkg.in/ini.v1: v1.62.0 → v1.66.2 k8s.io/api: v0.23.0 → v0.23.3 k8s.io/apimachinery: v0.23.0 → v0.23.3 k8s.io/apiserver: v0.23.0 → v0.23.3 k8s.io/client-go: v0.23.0 → v0.23.3 k8s.io/cloud-provider: v0.23.0 → v0.23.3 k8s.io/component-base: v0.23.0 → v0.23.3 k8s.io/component-helpers: v0.23.0 → v0.23.3 k8s.io/controller-manager: v0.23.0 → v0.23.3 k8s.io/kubelet: v0.23.0 → v0.23.3 k8s.io/utils: cb0fa31 → 6203023 sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.25 → v0.0.27 sigs.k8s.io/structured-merge-diff/v4: v4.1.2 → v4.2.1  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.3\n","excerpt":"Cloud Provider Azure v1.23.3\n","ref":"/cloud-provider-azure/blog/2022/02/07/v1.23.3/","tags":"","title":"v1.23.3"},{"body":"Changelog since v0.7.11 Changes by Kind Feature  Chore: upgrade azure-sdk-for-go to v58.2.0 and compute sdk to 2021-07-01 (#991, @andyzhangx)  Bug or Regression  Fix: do not update tags on load balancer, security group and route table if both tags and tagsMap are empty (#1004, @nilo19) This PR helps solve failure in ci-kubernetes-kubemark-100-azure-test. It was using an old aks-engine that doesn’t support K8s v1.19. https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-kubemark-100-azure-test/1480408092028964864/build-log.txt (#985, @lzhecheng)  Uncategorized  Upgrade docker/dockerfile image (#990, @lzhecheng)  Dependencies Added Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.12\n","excerpt":"Cloud Provider Azure v0.7.12\n","ref":"/cloud-provider-azure/blog/2022/01/20/v0.7.12/","tags":"","title":"v0.7.12"},{"body":"Changelog since v0.7.10 Changes by Kind Bug or Regression  Fix: return all LBs in the resource group in ListManagedLBs when deleting the LB, so the LB deleting will not be skipped (#971, @nilo19)  Other (Cleanup or Flake)   Cherry-pick #607 to solve flaky tests:\n should add all nodes in different agent pools to backends [MultipleAgentPools]: fail to find backendAddressPoolID in lbBackendAddressPoolsIDMap Found no or more than 1 virtual network in resource group same as cluster name cannot obtain the master node  Failures e.g. https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/kubernetes-sigs_cloud-provider-azure/969/pull-cloud-provider-azure-e2e-ccm-capz-1-20/1479028839588827136\nCherry-pick #627 to solve “get outbound rules” issue Cherry-pick #841 to solve “not equal 0” issue\nFailures e.g. https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/kubernetes-sigs_cloud-provider-azure/982/pull-cloud-provider-azure-e2e-ccm-capz-1-20/1480370475220602880 (#982, @lzhecheng)\n  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.11\n","excerpt":"Cloud Provider Azure v0.7.11\n","ref":"/cloud-provider-azure/blog/2022/01/10/v0.7.11/","tags":"","title":"v0.7.11"},{"body":"Changelog since v1.0.7 Changes by Kind Feature  Feat: support json style tags (#895, @nilo19) Introduce a configuration option putVMSSVMBatchSize. If set, the sync requests will be sent concurrently in batches when putting vmss vms. (#966, @nilo19)  Bug or Regression  Fix: return all LBs in the resource group in ListManagedLBs when deleting the LB, so the LB deleting will not be skipped (#972, @nilo19)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.8\n","excerpt":"Cloud Provider Azure v1.0.8\n","ref":"/cloud-provider-azure/blog/2022/01/10/v1.0.8/","tags":"","title":"v1.0.8"},{"body":"Changelog since v1.0.8 Changes by Kind Bug or Regression  Fix: do not update tags on load balancer, security group and route table if both tags and tagsMap are empty (#1006, @nilo19)  Other (Cleanup or Flake)  Chore: reduce node LIST APIs in cloud-node-manager (#997, @feiskyer)  Uncategorized  Null (#974, @mainred)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.9\n","excerpt":"Cloud Provider Azure v1.0.9\n","ref":"/cloud-provider-azure/blog/2022/01/10/v1.0.9/","tags":"","title":"v1.0.9"},{"body":"Changelog since v1.1.3 Changes by Kind Feature  Introduce a configuration option putVMSSVMBatchSize. If set, the sync requests will be sent concurrently in batches when putting vmss vms. (#959, @nilo19)  Bug or Regression  Fix: return all LBs in the resource group in ListManagedLBs when deleting the LB, so the LB deleting will not be skipped (#973, @nilo19)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.4\n","excerpt":"Cloud Provider Azure v1.1.4\n","ref":"/cloud-provider-azure/blog/2022/01/10/v1.1.4/","tags":"","title":"v1.1.4"},{"body":"Changelog since v1.1.4 Changes by Kind Bug or Regression  Fix: do not update tags on load balancer, security group and route table if both tags and tagsMap are empty (#1007, @nilo19)  Other (Cleanup or Flake)  Chore: reduce node LIST APIs in cloud-node-manager (#998, @feiskyer)  Uncategorized  Null (#974, @mainred)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.5\n","excerpt":"Cloud Provider Azure v1.1.5\n","ref":"/cloud-provider-azure/blog/2022/01/10/v1.1.5/","tags":"","title":"v1.1.5"},{"body":"Changelog since v1.23.1 Changes by Kind Feature  Introduce a configuration option putVMSSVMBatchSize. If set, the sync requests will be sent concurrently in batches when putting vmss vms. (#964, @nilo19)  Bug or Regression  Fix: do not update tags on load balancer, security group and route table if both tags and tagsMap are empty (#1008, @nilo19)  Other (Cleanup or Flake)  Chore: reduce node LIST APIs in cloud-node-manager (#996, @feiskyer)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.2\n","excerpt":"Cloud Provider Azure v1.23.2\n","ref":"/cloud-provider-azure/blog/2022/01/10/v1.23.2/","tags":"","title":"v1.23.2"},{"body":"Changelog since v1.23.0 Changes by Kind Feature  Introduce a configuration option putVMSSVMBatchSize. If set, the sync requests will be sent concurrently in batches when putting vmss vms. (#964, @nilo19)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.23.1\n","excerpt":"Cloud Provider Azure v1.23.1\n","ref":"/cloud-provider-azure/blog/2022/01/06/v1.23.1/","tags":"","title":"v1.23.1"},{"body":"Changelog since v1.1.0 Changes by Kind Feature   Adds support for the Cloud Node Manager to run as a Windows Service. This can be enabled using the –windows-service flag. (#823, @JoelSpeed)\n  Chore: rename tag key for CSI drivers (#799, @andyzhangx)\n  Feat: Adds windows server 2022 build (#821, @nick5616)\n  Feat: add AccessTier in file share creation interface (#781, @andyzhangx)\n  Feat: add AllowBlobPublicAccess setting in storage account creation (#784, @andyzhangx)\n  Feat: add disk online resize support (#889, @andyzhangx)\n  Feat: only restart all controllers after stopping if needed (#832, @nilo19)\n  Feat: reduce vmss cache refresh in parallel disk attach/detach (#803, @andyzhangx)\n  Feat: support creating account if not exists when account name is provided (#786, @andyzhangx)\n  Feat: support json style tags (#892, @nilo19)\n  Feat: support reloading the cloud controller manager based on the changes of the config file (#769, @nilo19)\n  Introduce a new config loadBalancerBackendPoolConfigurationType and it can be set to nodeIPConfiguration (default) or nodeIP. If set to nodeIPConfiguration, everything will keep unchanged. If set to nodeIP, the cloud provider will call the LB API to attach the node private IPs to the LB instead of linking the NICs to the LB. (#918, @nilo19)\n  Prefix ‘k8s-azure-’ has been added to the following tags:\n   Legacy Tag New Tag Comment     service k8s-azure-service Applied on public IP   kubernetes-cluster-name k8s-azure-cluster-name Applied on public IP   kubernetes-dns-label-service k8s-azure-dns-label-service Applied on public IP    To keep backward compability, the legacy tags on existing public IP would not be removed, but newly created public IPs would only get the new tags. (#815, @feiskyer)\n  Documentation  Chore: add release notes for v0.7.9, v1.0.6 and v1.1.2 (#867, @nilo19)  Failing Test  Chore: fix implictly required vendor (#775, @nilo19)  Bug or Regression  Release note: (#780, @andyzhangx) 1 controllerserver.go:378] delete azure disk(/subscriptions/xxx/resourceGroups/aks55h93-nodegroup/providers/Microsoft.Compute/disks/pvc-ecfefbb9-102c-4f40-944f-d5d4b6183568) returned with Retriable: true, RetryAfter: 0s, HTTPStatusCode: 0, RawError: azure cloud provider rate limited(read) for operation “GetDisk” 1 utils.go:100] GRPC error: Retriable: true, RetryAfter: 0s, HTTPStatusCode: 0, RawError: azure cloud provider rate limited(read) for operation “GetDisk” (#757, @andyzhangx) Fix detach disk issue on deleting vmss node (#774, @andyzhangx) Fix: consolidate logs for instance not found error (#794, @feiskyer) Fix: detach disk should return error when throttled (#929, @andyzhangx) Fix: do not crash if the region does not support zones (#850, @nilo19) Fix: do not delete the lb that does not exist (#860, @nilo19) Fix: fix a potential data race issue in unit test (#848, @nilo19) Fix: ignore the case when comparing azure tags in service annotation (#791, @nilo19) Fix: multi accounts creation issue when private endpoint creation failed (#880, @andyzhangx) Fix: panic due to nil pointer (#899, @andyzhangx) Fix: remove VMSS instances from SLB backend pool only when they are explicitly labeled fix: remove VMSS from SLB backend pool only when necessary (#856, @feiskyer) Fix: remove outdated ipv4 route when the corresponding node is deleted (#876, @nilo19) Fix: use correct gitVersion in userAgent (#764, @feiskyer) Version.Info{Major:“1”, Minor:“0+”, GitVersion:“v1.0.1-116-g93dee8c35”, GitCommit:“93dee8c35bddbff6beabb764725c74d252130c3c”, GitTreeState:\"\", BuildDate:“2021-08-11T08:21:10Z”, GoVersion:“go1.16.6”, Compiler:“gc”, Platform:“darwin/amd64”} (#749, @feiskyer)  Other (Cleanup or Flake)  Chore: add mixed protocol service e2e test (#897, @nilo19) Chore: bump k8s.io/cloud-provider to v0.23.0 (#934, @nilo19) Chore: detect data race in unit tests (#849, @nilo19) Chore: expose some public functions (#754, @andyzhangx) Chore: logging the service body when service controller invokes the cloud provider (#778, @nilo19) Cleanup: remove blob disk controller blob disk is not supported any more (#909, @andyzhangx) Fix: report an error when route table name is not configured (#819, @feiskyer)  Uncategorized  Fix: skip instance not found when decoupling vmss from lb (#842, @nilo19) Fix: switch to sync detach disk (#790, @andyzhangx) Use docker buildx for multi-arch node image (#855, @mainred)  Dependencies Added  github.com/cncf/xds/go: fbca930 github.com/getkin/kin-openapi: v0.76.0 github.com/go-logr/zapr: v1.2.0 github.com/golang-jwt/jwt/v4: v4.0.0 github.com/gorilla/mux: v1.8.0 k8s.io/component-helpers: v0.23.0 k8s.io/kubelet: v0.23.0 sigs.k8s.io/json: c049b76  Changed  github.com/Azure/azure-sdk-for-go: v55.0.0+incompatible → v55.8.0+incompatible github.com/Azure/go-autorest/autorest/adal: v0.9.14 → v0.9.17 github.com/Azure/go-autorest/autorest/validation: v0.1.0 → v0.3.1 github.com/Azure/go-autorest/autorest: v0.11.19 → v0.11.22 github.com/benbjohnson/clock: v1.0.3 → v1.1.0 github.com/envoyproxy/go-control-plane: 668b12f → 63b5d3c github.com/evanphx/json-patch: v4.11.0+incompatible → v4.12.0+incompatible github.com/fsnotify/fsnotify: v1.4.9 → v1.5.1 github.com/go-logr/logr: v0.4.0 → v1.2.0 github.com/json-iterator/go: v1.1.11 → v1.1.12 github.com/mitchellh/go-homedir: v1.1.0 → v1.0.0 github.com/modern-go/reflect2: v1.0.1 → v1.0.2 github.com/onsi/ginkgo: v1.16.4 → v1.16.5 github.com/onsi/gomega: v1.15.0 → v1.16.0 github.com/prometheus/common: v0.26.0 → v0.28.0 github.com/stretchr/objx: v0.2.0 → v0.1.1 github.com/yuin/goldmark: v1.3.5 → v1.4.0 go.uber.org/zap: v1.17.0 → v1.19.0 golang.org/x/crypto: 5ea612d → 089bfa5 golang.org/x/net: 37e1c6a → e898025 golang.org/x/oauth2: 2e8d934 → 2bc19b1 golang.org/x/sys: 59db8d7 → f4d4317 golang.org/x/term: 6a3ed07 → 6886f2d golang.org/x/text: v0.3.6 → v0.3.7 golang.org/x/tools: v0.1.2 → d4cc65f google.golang.org/genproto: f16073e → fe13028 google.golang.org/grpc: v1.38.0 → v1.40.0 google.golang.org/protobuf: v1.26.0 → v1.27.1 k8s.io/api: v0.22.0 → v0.23.0 k8s.io/apimachinery: v0.22.0 → v0.23.0 k8s.io/apiserver: v0.22.0 → v0.23.0 k8s.io/client-go: v0.22.0 → v0.23.0 k8s.io/cloud-provider: v0.22.0 → v0.23.0 k8s.io/component-base: v0.22.0 → v0.23.0 k8s.io/controller-manager: v0.22.0 → v0.23.0 k8s.io/gengo: 3a45101 → 485abfe k8s.io/klog/v2: v2.10.0 → v2.30.0 k8s.io/kube-openapi: 9528897 → e816edb k8s.io/utils: 4b05e18 → cb0fa31 sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.22 → v0.0.25 sigs.k8s.io/yaml: v1.2.0 → v1.3.0  Removed  cloud.google.com/go/datastore: v1.1.0 cloud.google.com/go/pubsub: v1.3.1 github.com/alecthomas/template: fb15b89 github.com/alecthomas/units: f65c72e github.com/client9/misspell: v0.3.4 github.com/coreos/bbolt: v1.3.2 github.com/coreos/etcd: v3.3.13+incompatible github.com/coreos/go-systemd: 95778df github.com/coreos/pkg: 399ea9e github.com/dgrijalva/jwt-go: v3.2.0+incompatible github.com/dgryski/go-sip13: e10d5fe github.com/dnaeon/go-vcr: v1.1.0 github.com/go-gl/glfw: e6da0ac github.com/gofrs/uuid: v4.0.0+incompatible github.com/google/martian: v2.1.0+incompatible github.com/hpcloud/tail: v1.0.0 github.com/jpillora/backoff: v1.0.0 github.com/konsorten/go-windows-terminal-sequences: v1.0.3 github.com/kr/logfmt: b84e30a github.com/kr/pty: v1.1.1 github.com/modocache/gover: b58185e github.com/oklog/ulid: v1.3.1 github.com/prometheus/tsdb: v0.7.1 github.com/rubiojr/go-vhd: 02e2102 gopkg.in/fsnotify.v1: v1.4.7 gopkg.in/resty.v1: v1.12.0 rsc.io/binaryregexp: v0.2.0 rsc.io/quote/v3: v3.1.0 rsc.io/sampler: v1.3.0 ree/02e2102) gopkg.in/fsnotify.v1: v1.4.7 gopkg.in/resty.v1: v1.12.0 rsc.io/binaryregexp: v0.2.0 rsc.io/quote/v3: v3.1.0 rsc.io/sampler: v1.3.0  ","categories":"","description":"Cloud Provider Azure v1.23.0\n","excerpt":"Cloud Provider Azure v1.23.0\n","ref":"/cloud-provider-azure/blog/2021/12/16/v1.23.0/","tags":"","title":"v1.23.0"},{"body":"Changelog since v0.7.9 Changes by Kind Bug or Regression  Fix: do not delete the lb that does not exist (#864, @nilo19)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.10\n","excerpt":"Cloud Provider Azure v0.7.10\n","ref":"/cloud-provider-azure/blog/2021/11/23/v0.7.10/","tags":"","title":"v0.7.10"},{"body":"Changelog since v1.0.6 Changes by Kind Feature  Feat: support json style tags (#895, @nilo19)  Bug or Regression  Fix: do not delete the lb that does not exist (#865, @nilo19)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.7\n","excerpt":"Cloud Provider Azure v1.0.7\n","ref":"/cloud-provider-azure/blog/2021/11/23/v1.0.7/","tags":"","title":"v1.0.7"},{"body":"Changelog since v1.1.2 Changes by Kind Feature  Feat: support json style tags (#896, @nilo19)  Bug or Regression  Fix: do not delete the lb that does not exist (#866, @nilo19) Fix: remove outdated ipv4 route when the corresponding node is deleted (#891, @nilo19)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.3\n","excerpt":"Cloud Provider Azure v1.1.3\n","ref":"/cloud-provider-azure/blog/2021/11/23/v1.1.3/","tags":"","title":"v1.1.3"},{"body":"Changelog since v0.7.8 Changes by Kind Bug or Regression  Fix: avoid wrapping RawError twice (#805, @feiskyer) Fix: do not crash if the region does not support zones (#852, @nilo19) Fix: do not delete the lb that does not exist (#864, @nilo19) Fix: use correct gitVersion in userAgent (#766, @feiskyer)  Uncategorized  Chore: support multi-arch cloud-node-manager image (#837, @mainred) Fix: skip instance not found when decoupling vmss from lb (#843, @nilo19) Use docker buildx for multi-arch node image (#857, @mainred)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.9\n","excerpt":"Cloud Provider Azure v0.7.9\n","ref":"/cloud-provider-azure/blog/2021/10/21/v0.7.9/","tags":"","title":"v0.7.9"},{"body":"Changelog since v1.0.5 Changes by Kind Feature  Feat: only restart all controllers after stopping if needed (#846, @nilo19) Feat: reloading ccm when the config file changes (#777, @nilo19)  Bug or Regression  Fix: avoid wrapping RawError twice (#804, @feiskyer) Fix: do not delete the lb that does not exist (#865, @nilo19) Fix: ignore the case when comparing azure tags in service annotation (#811, @nilo19) Fix: skip not found nodes when reconciling LB backend address pools (#818, @feiskyer)  Uncategorized  Chore: support multi-arch cloud-node-manager image (#836, @mainred) Fix: do not crash if the region does not support zones (#853, @nilo19) Fix: skip instance not found when decoupling vmss from lb (#844, @nilo19) Use docker buildx for multi-arch node image (#858, @mainred)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.6\n","excerpt":"Cloud Provider Azure v1.0.6\n","ref":"/cloud-provider-azure/blog/2021/10/21/v1.0.6/","tags":"","title":"v1.0.6"},{"body":"Changelog since v1.1.1 Changes by Kind Feature  Feat: only restart all controllers after stopping if needed (#847, @nilo19) Feat: reloading ccm when the config file changes (#776, @nilo19)  Bug or Regression  Fix: do not crash if the region does not support zones (#854, @nilo19) Fix: do not delete the lb that does not exist (#866, @nilo19) Fix: ignore the case when comparing azure tags in service annotation (#812, @nilo19) Fix: skip not found nodes when reconciling LB backend address pools (#817, @feiskyer)  Uncategorized  Chore: support multi-arch cloud-node-manager image (#835, @mainred) Fix: skip instance not found when decoupling vmss from lb (#845, @nilo19) Use docker buildx for multi-arch node image (#859, @mainred)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.2\n","excerpt":"Cloud Provider Azure v1.1.2\n","ref":"/cloud-provider-azure/blog/2021/10/21/v1.1.2/","tags":"","title":"v1.1.2"},{"body":"Changelog since v0.7.7 Changes by Kind Bug or Regression  Fix: use correct gitVersion in userAgent (#766, @feiskyer)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.8\n","excerpt":"Cloud Provider Azure v0.7.8\n","ref":"/cloud-provider-azure/blog/2021/08/30/v0.7.8/","tags":"","title":"v0.7.8"},{"body":"Changelog since v1.0.4 Changes by Kind Feature  Feat: reloading ccm when the config file changes (#777, @nilo19)  Bug or Regression  Fix: use correct gitVersion in userAgent (#765, @feiskyer)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.5\n","excerpt":"Cloud Provider Azure v1.0.5\n","ref":"/cloud-provider-azure/blog/2021/08/30/v1.0.5/","tags":"","title":"v1.0.5"},{"body":"Changelog since v1.1.0 Changes by Kind Feature  Feat: reloading ccm when the config file changes (#776, @nilo19)  Bug or Regression  Fix: ignore GetDisk throttling in DeleteDisk (#757, @andyzhangx) Fix: use correct gitVersion in userAgent (#764, @feiskyer) Fix: ensure major and minor versions are set correctly (#749, @feiskyer)  Other (Cleanup or Flake)  Chore: expose some public functions (#754, @andyzhangx)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v55.0.0+incompatible → v55.8.0+incompatible  Removed Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.1.1\n","excerpt":"Cloud Provider Azure v1.1.1\n","ref":"/cloud-provider-azure/blog/2021/08/30/v1.1.1/","tags":"","title":"v1.1.1"},{"body":"Changelog since v0.7.6 Changes by Kind Bug or Regression  Fix: ensure NSG rules are handled regardless of case  fix: enable git command by default fix: ensure VM with provisioningState=Creating and powerState=Stopped is treated as Creating instead of Stopped (#747, @feiskyer)    Features  Feat: Provide IPv6 support for internal load balancer (#712, @feiskyer)  ","categories":"","description":"Cloud Provider Azure v0.7.7\n","excerpt":"Cloud Provider Azure v0.7.7\n","ref":"/cloud-provider-azure/blog/2021/08/13/v0.7.7/","tags":"","title":"v0.7.7"},{"body":"Changelog since v1.0.3 Changes by Kind Bug or Regression  Fix: ensure NSG rules are handled regardless of case  fix: enable git command by default fix: ensure VM with provisioningState=Creating and powerState=Stopped is treated as Creating instead of Stopped (#746, @feiskyer)    Features  Feat: Provide IPv6 support for internal load balancer (#713, @feiskyer)  ","categories":"","description":"Cloud Provider Azure v1.0.4\n","excerpt":"Cloud Provider Azure v1.0.4\n","ref":"/cloud-provider-azure/blog/2021/08/13/v1.0.4/","tags":"","title":"v1.0.4"},{"body":"Changelog since v1.0.0 Changes by Kind Feature  Added support for Windows Server 2004 and 20H2 to the azure-cloud-node-manager image. (#655, @claudiubelu) [SIG Windows] Azure_api_request_errors metric now has an added “code” label which provides more details on the errors encountered. (#733, @marwanad) Chore: upgrade TLS1.0 to TLS1.2 in account creation (#675, @andyzhangx) Feat: Provide IPv6 support for internal load balancer (#703, @tomkerkhove) Feat: add ListAll interface for PublicIP client (#695, @feiskyer) Feat: add support for additional public IPs via service annotation “service.beta.kubernetes.io/azure-additional-public-ips” (#691, @feiskyer) Feat: enable creation of private endpoint for storage account (#652, @nearora-msft) Feat: support async attach/detach disk (#677, @andyzhangx) Feat: support networkAccessPolicy (#701, @andyzhangx) UserAgents can now be passed as part of the cloud provider config (#734, @kassarl)  Documentation  Chore: update docs for service tags NSG (#647, @feiskyer) Docs: add ‘securityGroupResourceGroup’ cloud-config value (#668, @aslafy-z)  Failing Test  Fix: serviceOwnsFrontendIP shouldn’t report error when the public IP doesn’t match (#649, @feiskyer)  Bug or Regression  Do not set cached Sku when updating VMSS and VMSS instances (#630, @feiskyer) fixed PrivateEndpoint matching in storage account search (#707, @andyzhangx) Fix: cleanup outdated routes (#661, @nilo19) Fix: detach disk panic on Azure Stack (#688, @andyzhangx) Fix: enable git command by default so that images could get the correct version from git (#745, @feiskyer) Fix: ensure NSG rules are handled regardless of case (#741, @feiskyer) Fix: ensure VM with provisioningState=Creating and powerState=Stopped is treated as Creating instead of Stopped (#743, @feiskyer) Fix: ensure http connections reused for ARM clients (#711, @feiskyer) Fix: ignore the NodeCIDRMaskSize in dualstack clusters (#721, @nilo19) Fix: make tags case-insensitive for both keys and values (#669, @nilo19) Fix: remove GetDisk operation in AttachDisk (#678, @andyzhangx) Fix: respect VnetResourceGroup in private link creation (#719, @andyzhangx) Fix: return empty VMAS name if using standalone VM (#679, @nilo19) fix: reduce crp throttling in attach disk scenario (#621, @andyzhangx) fix: dangling volume issue (#622, @andyzhangx) fix: delete non existing disk issue (#623, @andyzhangx) Retry.GetError(response, err) (#718, @marwanad) fix: ensure major and minor versions are set correctly (#749, @feiskyer) Zones logic on Azure Stack Cloud platform was disabled because it does not supported on this platform. (#716, @lobziik)  Other (Cleanup or Flake)  Chore: add e2e test for byo public IP (#627, @nilo19) Chore: add more buckets for operation metrics (#656, @andyzhangx) Chore: enrich unit test for serviceOwnsFrontendIP (#710, @nilo19) Chore: only put pip if it is necessary (#686, @nilo19) Chore: only reconciling routes in cloud controller manager (#671, @nilo19) Chore: set default config secret name and namespace (#662, @nilo19) Fix: wait for the success of the initial run of syncRegionZonesMap (#646, @nilo19) Update Azure Go SDK to v55.0.0 (#643, @feiskyer) Upgrade to 2020-02-01/storage (#628, @andyzhangx) GetTestCloud properly initializes the disk controller fields enabling them to be used in unit tests and mocked. (#689, @edreed)  Uncategorized  Add CreateOrUpdateBackendPools() interface for LoadBalancer client (#620, @feiskyer) Feat: add NFSv3 account creation support (#633, @andyzhangx) Fix: leave the probe path empty for TCP probes (#680, @nilo19) Fix: no sleep when GetDisk is throttled (#629, @andyzhangx) GetTestCloud now initializes the SnapshotsClient field to a mock implementation to facilitate unit test development for consuming projects. (#732, @edreed)  Dependencies Added  github.com/OneOfOne/xxhash: v1.2.2 github.com/antihax/optional: v1.0.0 github.com/benbjohnson/clock: v1.0.3 github.com/certifi/gocertifi: 2c3bb06 github.com/cespare/xxhash: v1.1.0 github.com/client9/misspell: v0.3.4 github.com/cncf/udpa/go: 5459f2c github.com/cockroachdb/errors: v1.2.4 github.com/cockroachdb/logtags: eb05cc2 github.com/coreos/go-systemd/v22: v22.3.2 github.com/dgryski/go-sip13: e10d5fe github.com/felixge/httpsnoop: v1.0.1 github.com/getsentry/raven-go: v0.2.0 github.com/go-gl/glfw: e6da0ac github.com/go-kit/log: v0.1.0 github.com/go-stack/stack: v1.8.0 github.com/go-task/slim-sprig: 348f09d github.com/godbus/dbus/v5: v5.0.4 github.com/google/martian/v3: v3.1.0 github.com/josharian/intern: v1.0.0 github.com/jpillora/backoff: v1.0.0 github.com/konsorten/go-windows-terminal-sequences: v1.0.3 github.com/kr/fs: v0.1.0 github.com/kr/pty: v1.1.1 github.com/nxadm/tail: v1.4.8 github.com/oklog/ulid: v1.3.1 github.com/opentracing/opentracing-go: v1.1.0 github.com/pkg/sftp: v1.10.1 github.com/prometheus/tsdb: v0.7.1 github.com/spaolacci/murmur3: f09979e github.com/stoewer/go-strcase: v1.2.0 go.etcd.io/etcd/api/v3: v3.5.0 go.etcd.io/etcd/client/pkg/v3: v3.5.0 go.etcd.io/etcd/client/v2: v2.305.0 go.etcd.io/etcd/client/v3: v3.5.0 go.etcd.io/etcd/pkg/v3: v3.5.0 go.etcd.io/etcd/raft/v3: v3.5.0 go.etcd.io/etcd/server/v3: v3.5.0 go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc: v0.20.0 go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp: v0.20.0 go.opentelemetry.io/contrib: v0.20.0 go.opentelemetry.io/otel/exporters/otlp: v0.20.0 go.opentelemetry.io/otel/metric: v0.20.0 go.opentelemetry.io/otel/oteltest: v0.20.0 go.opentelemetry.io/otel/sdk/export/metric: v0.20.0 go.opentelemetry.io/otel/sdk/metric: v0.20.0 go.opentelemetry.io/otel/sdk: v0.20.0 go.opentelemetry.io/otel/trace: v0.20.0 go.opentelemetry.io/otel: v0.20.0 go.opentelemetry.io/proto/otlp: v0.7.0 go.uber.org/goleak: v1.1.10 rsc.io/binaryregexp: v0.2.0  Changed  cloud.google.com/go/bigquery: v1.4.0 → v1.8.0 cloud.google.com/go/pubsub: v1.2.0 → v1.3.1 cloud.google.com/go/storage: v1.6.0 → v1.10.0 cloud.google.com/go: v0.54.0 → v0.81.0 github.com/Azure/azure-sdk-for-go: v53.1.0+incompatible → v55.0.0+incompatible github.com/Azure/go-ansiterm: d6e3b33 → d185dfc github.com/Azure/go-autorest/autorest/adal: v0.9.10 → v0.9.14 github.com/Azure/go-autorest/autorest/to: v0.2.0 → v0.4.0 github.com/Azure/go-autorest/autorest: v0.11.17 → v0.11.19 github.com/Azure/go-autorest/logger: v0.2.0 → v0.2.1 github.com/alecthomas/units: c3de453 → f65c72e github.com/bketelsen/crypt: 5cbc8cc → v0.0.4 github.com/cockroachdb/datadriven: 80d97fb → bf6692d github.com/envoyproxy/go-control-plane: 5f8ba28 → 668b12f github.com/evanphx/json-patch: v4.9.0+incompatible → v4.11.0+incompatible github.com/form3tech-oss/jwt-go: v3.2.2+incompatible → v3.2.3+incompatible github.com/fsnotify/fsnotify: v1.4.7 → v1.4.9 github.com/go-logfmt/logfmt: v0.4.0 → v0.5.0 github.com/go-openapi/jsonpointer: v0.19.3 → v0.19.5 github.com/go-openapi/jsonreference: v0.19.3 → v0.19.5 github.com/go-openapi/swag: v0.19.5 → v0.19.14 github.com/golang/groupcache: 8c9f03a → 41bb18b github.com/golang/mock: v1.4.1 → v1.6.0 github.com/golang/protobuf: v1.4.3 → v1.5.2 github.com/google/btree: v1.0.0 → v1.0.1 github.com/google/go-cmp: v0.5.2 → v0.5.5 github.com/google/pprof: 1ebb73c → cbba55b github.com/googleapis/gnostic: v0.4.1 → v0.5.5 github.com/grpc-ecosystem/go-grpc-middleware: f849b54 → v1.3.0 github.com/grpc-ecosystem/grpc-gateway: v1.9.5 → v1.16.0 github.com/ianlancetaylor/demangle: 5e5cf60 → 28f6c0f github.com/jonboulle/clockwork: v0.1.0 → v0.2.2 github.com/json-iterator/go: v1.1.10 → v1.1.11 github.com/julienschmidt/httprouter: v1.2.0 → v1.3.0 github.com/magiconair/properties: v1.8.1 → v1.8.5 github.com/mailru/easyjson: v0.7.0 → v0.7.6 github.com/mattn/go-isatty: v0.0.4 → v0.0.3 github.com/mitchellh/mapstructure: v1.1.2 → v1.4.1 github.com/moby/term: df9cb8a → 9d4ed18 github.com/mwitkow/go-conntrack: cc309e4 → 2f06839 github.com/onsi/ginkgo: v1.11.0 → v1.16.4 github.com/onsi/gomega: v1.8.1 → v1.15.0 github.com/pelletier/go-toml: v1.2.0 → v1.9.3 github.com/prometheus/client_golang: v1.7.1 → v1.11.0 github.com/prometheus/common: v0.10.0 → v0.26.0 github.com/prometheus/procfs: v0.2.0 → v0.6.0 github.com/rogpeppe/fastuuid: 6724a57 → v1.2.0 github.com/sirupsen/logrus: v1.7.0 → v1.8.1 github.com/soheilhy/cmux: v0.1.4 → v0.1.5 github.com/spf13/afero: v1.2.2 → v1.6.0 github.com/spf13/cast: v1.3.0 → v1.3.1 github.com/spf13/cobra: v1.1.1 → v1.2.1 github.com/spf13/jwalterweatherman: v1.0.0 → v1.1.0 github.com/spf13/viper: v1.7.0 → v1.8.1 github.com/stretchr/testify: v1.6.1 → v1.7.0 github.com/tmc/grpc-websocket-proxy: 0ad062e → e5319fd github.com/yuin/goldmark: v1.2.1 → v1.3.5 go.etcd.io/bbolt: v1.3.5 → v1.3.6 go.opencensus.io: v0.22.3 → v0.23.0 go.uber.org/atomic: v1.6.0 → v1.7.0 go.uber.org/multierr: v1.5.0 → v1.6.0 go.uber.org/zap: v1.16.0 → v1.17.0 golang.org/x/lint: 738671d → 6edffad golang.org/x/mod: ce943fd → v0.4.2 golang.org/x/net: 3d97a24 → 37e1c6a golang.org/x/oauth2: bf48bf1 → 2e8d934 golang.org/x/sync: 67f06af → 036812b golang.org/x/sys: a50acf3 → 59db8d7 golang.org/x/text: v0.3.4 → v0.3.6 golang.org/x/time: f8bda1e → 1f47c86 golang.org/x/tools: v0.1.0 → v0.1.2 google.golang.org/api: v0.20.0 → v0.44.0 google.golang.org/appengine: v1.6.5 → v1.6.7 google.golang.org/genproto: 8816d57 → f16073e google.golang.org/grpc: v1.27.1 → v1.38.0 google.golang.org/protobuf: v1.25.0 → v1.26.0 gopkg.in/ini.v1: v1.51.0 → v1.62.0 gopkg.in/yaml.v3: 9f266ea → 496545a honnef.co/go/tools: v0.0.1-2020.1.3 → v0.0.1-2020.1.4 k8s.io/api: 648b778 → v0.22.0 k8s.io/apimachinery: 8daf289 → v0.22.0 k8s.io/apiserver: 940c107 → v0.22.0 k8s.io/client-go: 8c8fa70 → v0.22.0 k8s.io/cloud-provider: 1ea896e → v0.22.0 k8s.io/component-base: 5860d9b → v0.22.0 k8s.io/controller-manager: 146a790 → v0.22.0 k8s.io/klog/v2: v2.8.0 → v2.10.0 k8s.io/kube-openapi: 591a79e → 9528897 k8s.io/utils: 67b214c → 4b05e18 sigs.k8s.io/apiserver-network-proxy/konnectivity-client: v0.0.15 → v0.0.22 sigs.k8s.io/structured-merge-diff/v4: v4.1.1 → v4.1.2  Removed  github.com/go-openapi/spec: v0.19.5 github.com/mattn/go-runewidth: v0.0.2 github.com/olekukonko/tablewriter: a0225b3 github.com/urfave/cli: v1.20.0 go.etcd.io/etcd: dd1b699 go.uber.org/tools: 2cfd321 gopkg.in/cheggaaa/pb.v1: v1.0.25  ","categories":"","description":"Cloud Provider Azure v1.1.0\n","excerpt":"Cloud Provider Azure v1.1.0\n","ref":"/cloud-provider-azure/blog/2021/08/13/v1.1.0/","tags":"","title":"v1.1.0"},{"body":"Changelog since v0.7.5 Changes by Kind Feature  Feat: Provide IPv6 support for internal load balancer (#703, @tomkerkhove)  Bug or Regression  Fix: not send availability zones as part of create for edge zones (#709, @MirzaSikander)  ","categories":"","description":"Cloud Provider Azure v0.7.6\n","excerpt":"Cloud Provider Azure v0.7.6\n","ref":"/cloud-provider-azure/blog/2021/07/20/v0.7.6/","tags":"","title":"v0.7.6"},{"body":"Changelog since v1.0.2 Changes by Kind Feature  Feat: Provide IPv6 support for internal load balancer (#703, @tomkerkhove)  Bug or Regression  Fix: not send availability zones as part of create for edge zones (#709, @MirzaSikander)  ","categories":"","description":"Cloud Provider Azure v1.0.3\n","excerpt":"Cloud Provider Azure v1.0.3\n","ref":"/cloud-provider-azure/blog/2021/07/20/v1.0.3/","tags":"","title":"v1.0.3"},{"body":"Major changes since v0.7.4 Changes by Kind Feature  Chore: upgrade TLS1.0 to TLS1.2 in account creation (#675, @andyzhangx) Feat: Enable creation of storage accounts that support large file shares (#606, @nearora-msft) Feat: add support for additional public IPs via service annotation “service.beta.kubernetes.io/azure-additional-public-ips” (#691, @feiskyer) Feat: enable creation of private endpoint for storage account (#652, @nearora-msft) Feat: support reloading cloud controller manager from secret dynamically (#613, @nilo19)  Documentation  Chore: update docs for service tags NSG (#647, @feiskyer) Docs: add ‘securityGroupResourceGroup’ cloud-config value (#668, @aslafy-z)  Failing Test  Fix: serviceOwnsFrontendIP shouldn’t report error when the public IP doesn’t match (#649, @feiskyer)  Bug or Regression  Do not set cached Sku when updating VMSS and VMSS instances (#630, @feiskyer) Fix: avoid nil-pointer panic when checking the frontend IP configuration (#615, @nilo19) Fix: cleanup outdated routes (#661, @nilo19) Fix: detach disk panic on Azure Stack (#688, @andyzhangx) Fix: make tags case-insensitive for both keys and values (#669, @nilo19) Fix: not tagging static public IP (#616, @nilo19) Fix: remove GetDisk operation in AttachDisk (#678, @andyzhangx) Fix: return empty VMAS name if using standalone VM (#679, @nilo19)  Other (Cleanup or Flake)  Chore: add e2e test for byo public IP (#627, @nilo19) Chore: add more buckets for operation metrics (#656, @andyzhangx) Chore: completely decouple k/k (#601, @nilo19) Chore: only logs rate limiting configurations when rate limit is enabled (#608, @feiskyer) Chore: only put pip if it is necessary (#686, @nilo19) Chore: only reconciling routes in cloud controller manager (#671, @nilo19) Chore: set default config secret name and namespace (#662, @nilo19) Fix: wait for the success of the initial run of syncRegionZonesMap (#646, @nilo19) Update Azure Go SDK to v55.0.0 (#643, @feiskyer) Update cloud-provider vendor to v1.21 (#603, @feiskyer) Upgrade to 2020-02-01/storage (#628, @andyzhangx) GetTestCloud properly initializes the disk controller fields enabling them to be used in unit tests and mocked. (#689, @edreed)  Uncategorized  Add CreateOrUpdateBackendPools() interface for LoadBalancer client (#620, @feiskyer) Feat: add NFSv3 account creation support (#633, @andyzhangx) Fix: leave the probe path empty for TCP probes (#680, @nilo19) Fix: no sleep when GetDisk is throttled (#629, @andyzhangx)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v0.7.5\n","excerpt":"Cloud Provider Azure v0.7.5\n","ref":"/cloud-provider-azure/blog/2021/06/21/v0.7.5/","tags":"","title":"v0.7.5"},{"body":"Major changes since v1.0.0 Cloud Provider Azure v1.0.1 includes several critical bug fixes. The images are available at:\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v1.0.1 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.1  Changes by Kind Documentation  Chore: update docs for service tags NSG (#647, @feiskyer) Docs: add ‘securityGroupResourceGroup’ cloud-config value (#668, @aslafy-z)  Failing Test  Fix: serviceOwnsFrontendIP shouldn’t report error when the public IP doesn’t match (#649, @feiskyer)  Bug or Regression  Do not set cached Sku when updating VMSS and VMSS instances (#630, @feiskyer) Fix: cleanup outdated routes (#661, @nilo19) Fix: make tags case-insensitive for both keys and values (#669, @nilo19)  Other (Cleanup or Flake)  Chore: add e2e test for byo public IP (#627, @nilo19) Chore: add more buckets for operation metrics (#656, @andyzhangx) Chore: only reconciling routes in cloud controller manager (#671, @nilo19) Chore: set default config secret name and namespace (#662, @nilo19) Fix: wait for the success of the initial run of syncRegionZonesMap (#646, @nilo19) Update Azure Go SDK to v55.0.0 (#643, @feiskyer) Upgrade to 2020-02-01/storage (#628, @andyzhangx) Add CreateOrUpdateBackendPools() interface for LoadBalancer client (#620, @feiskyer) Feat: add NFSv3 account creation support (#633, @andyzhangx) Fix: no sleep when GetDisk is throttled (#629, @andyzhangx)  ","categories":"","description":"Cloud Provider Azure v1.0.1\n","excerpt":"Cloud Provider Azure v1.0.1\n","ref":"/cloud-provider-azure/blog/2021/06/21/v1.0.1/","tags":"","title":"v1.0.1"},{"body":"Major changes since v1.0.1 Changes by Kind Feature  Chore: upgrade TLS1.0 to TLS1.2 in account creation (#675, @andyzhangx) Feat: add support for additional public IPs via service annotation “service.beta.kubernetes.io/azure-additional-public-ips” (#691, @feiskyer) Feat: enable creation of private endpoint for storage account (#652, @nearora-msft)  Bug or Regression  Fix: detach disk panic on Azure Stack (#688, @andyzhangx) Fix: make tags case-insensitive for both keys and values (#669, @nilo19) Fix: remove GetDisk operation in AttachDisk (#678, @andyzhangx) Fix: return empty VMAS name if using standalone VM (#679, @nilo19)  Other (Cleanup or Flake)  Chore: only put pip if it is necessary (#686, @nilo19) GetTestCloud properly initializes the disk controller fields enabling them to be used in unit tests and mocked. (#689, @edreed)  Uncategorized  Fix: leave the probe path empty for TCP probes (#680, @nilo19)  Dependencies Nothing has changed.\n","categories":"","description":"Cloud Provider Azure v1.0.2\n","excerpt":"Cloud Provider Azure v1.0.2\n","ref":"/cloud-provider-azure/blog/2021/06/21/v1.0.2/","tags":"","title":"v1.0.2"},{"body":"Major changes since v0.7.4 Cloud Provider Azure v1.0.0 includes several critical bug fixes. The images are available at:\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v1.0.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0  Changes by Kind Feature  Feat: Enable creation of storage accounts that support large file shares (#606, @nearora-msft) Feat: support reloading cloud controller manager from secret dynamically (#613, @nilo19)  Bug or Regression  Fix: avoid nil-pointer panic when checking the frontend IP configuration (#615, @nilo19) Fix: not tagging static public IP (#616, @nilo19)  Other (Cleanup or Flake)  Chore: completely decouple k/k (#601, @nilo19) Chore: only logs rate limiting configurations when rate limit is enabled (#608, @feiskyer) Update cloud-provider vendor to v1.21 (#603, @feiskyer)  ","categories":"","description":"Cloud Provider Azure v1.0.0\n","excerpt":"Cloud Provider Azure v1.0.0\n","ref":"/cloud-provider-azure/blog/2021/05/07/v1.0.0/","tags":"","title":"v1.0.0"},{"body":"Major changes since v0.7.3 Cloud Provider Azure v0.7.4 includes several critical bug fixes. The images are available at:\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.7.4 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.7.4  Changes by Kind Documentation  Chore: enrich docs (#590, @nilo19)  Bug or Regression  Fix: arm node provider Windows initialization (#595, @JesusAlvarezTorres) Fix: call the counterpart function of availabilitySet when the instance is not a vmss vm (#597, @nilo19) Fix: potential race condition in detach disk (#593, @andyzhangx) Fix: support sharing the primary slb when there are both external and internal load balancers in the cluster (#588, @nilo19)  ","categories":"","description":"Cloud Provider Azure v0.7.4\n","excerpt":"Cloud Provider Azure v0.7.4\n","ref":"/cloud-provider-azure/blog/2021/04/23/v0.7.4/","tags":"","title":"v0.7.4"},{"body":"Major changes since v0.7.2 Cloud Provider Azure v0.7.3 supports out-of-tree node ipam controller, sharing the primary SLB with multiple vmSets, and a bunch of other features/bug fixes. The images are available at:\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.7.3 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.7.3  Changes by Kind Feature  Chore: remove get file in CreateFileShare (#534, @andyzhangx) Feat: add ARM node provider (#580, @JesusAlvarezTorres) Feat: add disable RetentionPolicy parameter (#545, @andyzhangx) Feat: add update vm interface (#592, @andyzhangx) Feat: implement cloud allocator for vmas (#555, @nilo19) Feat: implement cloud cidr allocator for VMSS (#539, @nilo19) Feat: support sharing the primary slb with multiple vmSets (#578, @nilo19) Feat: support system tag (#558, @nilo19) Feat: upgrade azure sdk to v53.1.0 (#589, @andyzhangx)  Documentation  Doc: add doc for out-of-tree node ipam controller (#553, @nilo19)  Bug or Regression  Allow disabling AzureStackCloud API versions when using AzureStackCloud config on public cloud (#525, @feiskyer) Avoid caching the VMSS instances whose network profile is nil (#583, @feiskyer) Azure_storageaccount.go:99] found skip-matching tag for account %!s(*string=0xc000cbd7b0), skip matching (#529, @andyzhangx) Ensure only LoadBalancer rule is created when HA mode is enabled (#536, @feiskyer) Ensure service deleted when the Azure resource group has been deleted (#584, @feiskyer) Fix node public IP fetching from instance metadata service when the node is part of standard load balancer backend pool. (#540, @feiskyer) Fix: avoid panic when RouteTablePropertiesFormat is nil (#568, @feiskyer) Fix: not delete existing pip when service is deleted (#574, @nilo19) Fix: support sharing the primary slb when there are both external and internal load balancers in the cluster (#588, @nilo19) Fixed routes not created issues before Pod scheduling. When using kubenet, 1) cloud-node-manager supports “–wait-routes=true” to indicate a node would wait for route updates before accepting Pod scheduling and 2) route controller would wait a while for new routes to take effect (default is 30s). (#528, @feiskyer) Ignore not a VMSS error for VMAS nodes in reconcileBackendPools (#551, @CecileRobertMichon)  Other (Cleanup or Flake)  Chore: move consts in azure_vmss.go to consts.go (#554, @nilo19) Chore: remove bazel support (#585, @nilo19) Chore: switch to network api 2020-08-01 (#569, @nilo19) Enable docker BuildKit and update Go to 1.15.8 (#548, @CecileRobertMichon) Update Azure compute API version to 2020-12-01 (#579, @feiskyer)  Uncategorized  Fix availability set cache in vmss cache (#537, @CecileRobertMichon) Fix: check disk state before attach disk (#564, @andyzhangx) Fix: convert backend pool id to lower case before using it (#561, @nilo19)  Dependencies Added  github.com/gofrs/uuid: v4.0.0+incompatible  Changed  github.com/Azure/azure-sdk-for-go: v51.2.0+incompatible → v53.1.0+incompatible  ","categories":"","description":"Cloud Provider Azure v0.7.3\n","excerpt":"Cloud Provider Azure v0.7.3\n","ref":"/cloud-provider-azure/blog/2021/04/19/v0.7.3/","tags":"","title":"v0.7.3"},{"body":"Cloud Provider Azure v0.7.2 allows to disable AzureStackCloud API versions when using AzureStackCloud config on public cloud (e.g. for customizing ARM endpoints). The images are available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.7.2 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.7.2  Changelog since v0.7.1 Changes by Kind Bug or Regression  Allow disabling AzureStackCloud API versions when using AzureStackCloud config on public cloud (#525, @feiskyer)  ","categories":"","description":"Cloud Provider Azure v0.7.2\n","excerpt":"Cloud Provider Azure v0.7.2\n","ref":"/cloud-provider-azure/blog/2021/02/28/v0.7.2/","tags":"","title":"v0.7.2"},{"body":"Changelog since v0.7.0 Changes by Kind Feature  Feat: Add service annotation ServiceAnnotationDenyAllExpectSourceRanges (#487, @nilo19) Feat: skip account matching with special tags (#490, @andyzhangx) Feat: vm client changes for Azure Stack Hub support (#477, @JesusAlvarezTorres) Implement cloudprovider.InstancesV2 interface (#466, @nilo19) Support etag when putting network interface. (#483, @nilo19) Updates all the references for azure network API to point to 2020-07-01 which is the latest API Version (#502, @MirzaSikander)  Bug or Regression  Aggregate errors when putting vmss (#482, @nilo19) Output the actual error when VMSS PUT fails rather than the error from the previous GET (#486, @devigned)  Other (Cleanup or Flake)  Add e2e test for annotation service.beta.kubernetes.io/azure-deny-all-except-load-balancer-source-ranges (#489, @nilo19) Add log level in armclient (#497, @nilo19)  ","categories":"","description":"Cloud Provider Azure v0.7.1\n","excerpt":"Cloud Provider Azure v0.7.1\n","ref":"/cloud-provider-azure/blog/2021/02/24/v0.7.1/","tags":"","title":"v0.7.1"},{"body":"Major changes since v0.6.0 Cloud Provider Azure v0.7.0 updates Kubernetes vendor to v1.20 and moves to beta. The images are available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.7.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.7.0  Enhancements  Features inherited from in-tree Azure cloud provider  Multiple standard load balancers in the same cluster. Multiple load balancer typed services sharing one IP address. Customized load balancer health probe. Tagging resources managed by provider azure.   Code improvements inherited from in-tree Azure cloud provider  Add metrics for cloud provider operations like “EnsureLoadBalancer”. Improve the unit/E2E test coverage in provider azure.   Upgrade Azure compute API version to 2020-06-30: (#444, @andyzhangx) Use batch operation for azure disk attach/detach: (#453, @andyzhangx)  Bug Fixes  Fix nil VMSS name when setting service to auto mode (#439, @nilo19) Fix readyz probe (#394, @nilo19) Ignore in-cluster config when --master or --kubeconfig is set explicitly (#397, @nilo19) Support change the LB selection mode on the existing services (#445, @nilo19) Use network.Interface.VirtualMachine.ID to get the VM (#443, @nilo19) Skip the exclude LB test on multi node pool cluster (#455, @nilo19)  Cleanups  Duplicate the in-tree cloud provider code to the out-of-tree repo (#433, @nilo19)  Dependencies Added  cloud.google.com/go/bigquery: v1.4.0 cloud.google.com/go/datastore: v1.1.0 cloud.google.com/go/firestore: v1.1.0 cloud.google.com/go/pubsub: v1.2.0 cloud.google.com/go/storage: v1.6.0 dmitri.shuralyov.com/gpu/mtl: 666a987 github.com/armon/go-metrics: f0300d1 github.com/armon/go-radix: 7fddfc3 github.com/bketelsen/crypt: 5cbc8cc github.com/checkpoint-restore/go-criu/v4: v4.1.0 github.com/chzyer/logex: v1.1.10 github.com/chzyer/readline: 2972be2 github.com/chzyer/test: a1ea475 github.com/cilium/ebpf: 1c8d4c9 github.com/containerd/cgroups: 0dbf7f0 github.com/containerd/console: v1.0.0 github.com/containerd/containerd: v1.4.1 github.com/containerd/continuity: aaeac12 github.com/containerd/fifo: a9fb20d github.com/containerd/go-runc: 5a6d9f3 github.com/containerd/ttrpc: v1.0.2 github.com/containerd/typeurl: v1.0.1 github.com/coreos/bbolt: v1.3.2 github.com/coreos/go-systemd/v22: v22.1.0 github.com/cyphar/filepath-securejoin: v0.2.2 github.com/euank/go-kmsg-parser: v2.0.0+incompatible github.com/fvbommel/sortorder: v1.0.1 github.com/globalsign/mgo: eeefdec github.com/go-gl/glfw/v3.3/glfw: 6f7a984 github.com/go-gl/glfw: e6da0ac github.com/godbus/dbus/v5: v5.0.3 github.com/gopherjs/gopherjs: 0766667 github.com/gorilla/mux: v1.8.0 github.com/hashicorp/consul/api: v1.1.0 github.com/hashicorp/consul/sdk: v0.1.1 github.com/hashicorp/errwrap: v1.0.0 github.com/hashicorp/go-cleanhttp: v0.5.1 github.com/hashicorp/go-immutable-radix: v1.0.0 github.com/hashicorp/go-msgpack: v0.5.3 github.com/hashicorp/go-multierror: v1.0.0 github.com/hashicorp/go-rootcerts: v1.0.0 github.com/hashicorp/go-sockaddr: v1.0.0 github.com/hashicorp/go-uuid: v1.0.1 github.com/hashicorp/go.net: v0.0.1 github.com/hashicorp/logutils: v1.0.0 github.com/hashicorp/mdns: v1.0.0 github.com/hashicorp/memberlist: v0.1.3 github.com/hashicorp/serf: v0.8.2 github.com/ianlancetaylor/demangle: 5e5cf60 github.com/jmespath/go-jmespath/internal/testify: v1.5.1 github.com/jtolds/gls: v4.20.0+incompatible github.com/karrick/godirwalk: v1.16.1 github.com/kr/logfmt: b84e30a github.com/mindprince/gonvml: 9ebdce4 github.com/mistifyio/go-zfs: f784269 github.com/mitchellh/cli: v1.0.0 github.com/mitchellh/go-testing-interface: v1.0.0 github.com/mitchellh/gox: v0.4.0 github.com/mitchellh/iochan: v1.0.0 github.com/moby/sys/mountinfo: v0.1.3 github.com/modocache/gover: b58185e github.com/morikuni/aec: v1.0.0 github.com/niemeyer/pretty: a10e7ca github.com/opencontainers/image-spec: v1.0.1 github.com/opencontainers/runtime-spec: 4d89ac9 github.com/pascaldekloe/goe: 57f6aae github.com/pborman/uuid: v1.2.0 github.com/posener/complete: v1.1.1 github.com/ryanuber/columnize: 9b3edd6 github.com/sean-/seed: e2103e2 github.com/seccomp/libseccomp-golang: v0.9.1 github.com/smartystreets/assertions: b2de0cb github.com/smartystreets/goconvey: v1.6.4 github.com/subosito/gotenv: v1.2.0 github.com/syndtr/gocapability: d983527 github.com/willf/bitset: d5bec33 github.com/yuin/goldmark: v1.1.27 golang.org/x/term: 7de9c90 gopkg.in/ini.v1: v1.51.0 k8s.io/api: fcac651 k8s.io/apiextensions-apiserver: a7ee1ef k8s.io/apimachinery: 15c5dba k8s.io/apiserver: aed7ab0 k8s.io/cli-runtime: 2e4b259 k8s.io/client-go: e24efdc k8s.io/cluster-bootstrap: 614b98e k8s.io/code-generator: v0.21.0-alpha.0 k8s.io/component-base: 1e84b32 k8s.io/component-helpers: 7cb42b6 k8s.io/controller-manager: b2c380a k8s.io/cri-api: v0.21.0-alpha.0 k8s.io/csi-translation-lib: 8333033 k8s.io/kube-aggregator: 6c47de4 k8s.io/kube-controller-manager: 18c28a4 k8s.io/kube-proxy: deb12d4 k8s.io/kube-scheduler: 0f62d39 k8s.io/kubectl: 5cfbd40 k8s.io/kubelet: 92ded5e k8s.io/legacy-cloud-providers: 716c3da k8s.io/metrics: d70c0e0 k8s.io/mount-utils: v0.21.0-alpha.0 k8s.io/sample-apiserver: 1f4e6a9 rsc.io/binaryregexp: v0.2.0  Updated  cloud.google.com/go: v0.38.0 → v0.54.0 github.com/Azure/azure-sdk-for-go: 8277be3 → v49.1.0+incompatible github.com/GoogleCloudPlatform/k8s-cloud-provider: 27a4ced → 7901bc8 github.com/Microsoft/go-winio: v0.4.14 → v0.4.15 github.com/Microsoft/hcsshim: 672e52e → 5eafd15 github.com/alecthomas/template: a0175ee → fb15b89 github.com/alecthomas/units: 2efee85 → c3de453 github.com/aws/aws-sdk-go: v1.28.2 → v1.35.24 github.com/containernetworking/cni: v0.7.1 → v0.8.0 github.com/coredns/corefile-migration: v1.0.6 → v1.0.10 github.com/coreos/etcd: v3.3.10+incompatible → v3.3.13+incompatible github.com/creack/pty: v1.1.7 → v1.1.9 github.com/dnaeon/go-vcr: v1.0.1 → v1.1.0 github.com/docker/docker: be7ac8b → bd33bbf github.com/docker/go-connections: v0.3.0 → v0.4.0 github.com/fsnotify/fsnotify: v1.4.7 → v1.4.9 github.com/go-kit/kit: v0.8.0 → v0.9.0 github.com/go-logfmt/logfmt: v0.3.0 → v0.4.0 github.com/google/cadvisor: v0.35.0 → v0.38.5 github.com/google/pprof: 3ea8567 → 1ebb73c github.com/googleapis/gax-go/v2: v2.0.4 → v2.0.5 github.com/gorilla/websocket: v1.4.0 → v1.4.2 github.com/jmespath/go-jmespath: c2b33e8 → v0.4.0 github.com/jstemmer/go-junit-report: af01ea7 → v0.9.1 github.com/kr/pretty: v0.1.0 → v0.2.0 github.com/kr/text: v0.1.0 → v0.2.0 github.com/mattn/go-isatty: v0.0.9 → v0.0.4 github.com/moby/ipvs: v1.0.0 → v1.0.1 github.com/mrunalp/fileutils: 7d4729f → abd8a0e github.com/opencontainers/runc: v1.0.0-rc10 → v1.0.0-rc92 github.com/opencontainers/selinux: 5215b18 → v1.6.0 github.com/quobyte/api: v0.1.2 → v0.1.8 github.com/spf13/viper: v1.3.2 → v1.7.0 github.com/storageos/go-api: 343b3ef → v2.2.0+incompatible github.com/tmc/grpc-websocket-proxy: 89b8d40 → 0ad062e github.com/urfave/cli: v1.20.0 → v1.22.2 github.com/vishvananda/netlink: v1.0.0 → v1.1.0 github.com/vishvananda/netns: be1fbed → db3c7e5 go.etcd.io/bbolt: v1.3.3 → v1.3.5 go.opencensus.io: v0.21.0 → v0.22.3 golang.org/x/exp: 4b39c73 → 6cc2880 golang.org/x/image: 0694c2d → cff245a golang.org/x/lint: 959b441 → 738671d golang.org/x/mobile: d3739f8 → d2bd2a2 golang.org/x/mod: 4bf6d31 → v0.3.0 golang.org/x/net: 13f9640 → ac852fb golang.org/x/tools: 5eefd05 → c1934b7 golang.org/x/xerrors: a985d34 → 5ec99f8 google.golang.org/api: 5213b80 → v0.20.0 google.golang.org/protobuf: v1.24.0 → v1.25.0 gopkg.in/check.v1: 788fd78 → 8fa4692 honnef.co/go/tools: v0.0.1-2019.2.2 → v0.0.1-2020.1.3 k8s.io/cloud-provider: 52e5381 → 82fca6d k8s.io/klog/v2: v2.2.0 → v2.4.0 k8s.io/kube-openapi: 6aeccd4 → d219536 k8s.io/kubernetes: bb8a5d2 → f58c4d8 k8s.io/system-validators: v1.1.2 → v1.2.0 sigs.k8s.io/structured-merge-diff/v4: v4.0.1 → v4.0.2  Removed  github.com/xlab/handysort: fb3537e k8s.io/kubernetes/staging/src/k8s.io/api: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/apiextensions-apiserver: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/apimachinery: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/apiserver: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/cli-runtime: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/client-go: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/cluster-bootstrap: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/code-generator: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/component-base: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/cri-api: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/csi-translation-lib: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kube-aggregator: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kube-controller-manager: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kube-proxy: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kube-scheduler: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kubectl: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kubelet: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/legacy-cloud-providers: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/metrics: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/sample-apiserver: 70a6823 vbom.ml/util: db5cfe1  ","categories":"","description":"Cloud Provider Azure v0.7.0\n","excerpt":"Cloud Provider Azure v0.7.0\n","ref":"/cloud-provider-azure/blog/2021/01/06/v0.7.0/","tags":"","title":"v0.7.0"},{"body":"Major changes since v0.5.0   Update vendor against k/k release-1.19(#385) Increase the e2e test coverage for cluster autoscaler(#364) Use hugo to generate doc website(#358) Update E2E test related docs and script(#355) Partly decouple k/k(#350) Update go module against k8s.io/cloud-provider(#348) Use distroless/static as base image(#333) Enable running ccm e2e test in a job(#345)  The image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.6.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.6.0  Since v0.5.0, our docs are moved to a dedicated website and the docs/ directory is deprecated.\n","categories":"","description":"Cloud Provider Azure v0.6.0\n","excerpt":"Cloud Provider Azure v0.6.0\n","ref":"/cloud-provider-azure/blog/2020/09/01/v0.6.0/","tags":"","title":"v0.6.0"},{"body":"Changes since v0.5.0   Update Kubernetes vendor to adopt bug fixes from in-tree cloud provider(#330) Use a service account for CCM (#329) Update images for out-of-tree examples (#328) Fix wrong init url for kubemark tests (#327)  The image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.5.1 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.5.1  ","categories":"","description":"Cloud Provider Azure v0.5.1\n","excerpt":"Cloud Provider Azure v0.5.1\n","ref":"/cloud-provider-azure/blog/2020/04/27/v0.5.1/","tags":"","title":"v0.5.1"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.18. It also adds Windows support for azure-cloud-node-manager.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.5.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.5.0  ","categories":"","description":"Cloud Provider Azure v0.5.0\n","excerpt":"Cloud Provider Azure v0.5.0\n","ref":"/cloud-provider-azure/blog/2020/03/27/v0.5.0/","tags":"","title":"v0.5.0"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which fixes the node address update issues.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.4.1 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.4.1  ","categories":"","description":"Cloud Provider Azure v0.4.1\n","excerpt":"Cloud Provider Azure v0.4.1\n","ref":"/cloud-provider-azure/blog/2019/12/30/v0.4.1/","tags":"","title":"v0.4.1"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.17.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.4.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.4.0  ","categories":"","description":"Cloud Provider Azure v0.4.0\n","excerpt":"Cloud Provider Azure v0.4.0\n","ref":"/cloud-provider-azure/blog/2019/12/17/v0.4.0/","tags":"","title":"v0.4.0"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.16.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.3.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.3.0  ","categories":"","description":"Cloud Provider Azure v0.3.0\n","excerpt":"Cloud Provider Azure v0.3.0\n","ref":"/cloud-provider-azure/blog/2019/09/24/v0.3.0/","tags":"","title":"v0.3.0"},{"body":"The alpha version of azure-cloud-controller-manager, which has upgraded Kubernetes version to v1.15.0.\nPlease see docs for documentation.\nThe image is available at mcr.microsoft.com/k8s/core/azure-cloud-controller-manager:v0.2.0.\n","categories":"","description":"Cloud Provider Azure v0.2.0\n","excerpt":"Cloud Provider Azure v0.2.0\n","ref":"/cloud-provider-azure/blog/2019/06/27/v0.2.0/","tags":"","title":"v0.2.0"},{"body":"The alpha version of azure-cloud-controller-manager. Please see docs for documentation.\nThe image is available at mcr.microsoft.com/k8s/core/azure-cloud-controller-manager:v0.1.0.\n","categories":"","description":"Cloud Provider Azure v0.1.0\n","excerpt":"Cloud Provider Azure v0.1.0\n","ref":"/cloud-provider-azure/blog/2019/03/26/v0.1.0/","tags":"","title":"v0.1.0"},{"body":"Overview Here provides some E2E tests only specific to Azure provider.\nPrerequisite Deploy a Kubernetes cluster with Azure CCM Refer step 1-3 in e2e-tests for deploying the Kubernetes cluster.\nSetup Azure credentials export AZURE_TENANT_ID=\u003ctenant-id\u003e # the tenant ID export AZURE_SUBSCRIPTION_ID=\u003csubscription-id\u003e # the subscription ID export AZURE_CLIENT_ID=\u003cservice-principal-id\u003e # the service principal ID export AZURE_CLIENT_SECRET=\u003cservice-principal-secret\u003e # the service principal secret export AZURE_ENVIRONMENT=\u003cAzurePublicCloud\u003e # the cloud environment (optional, default is AzurePublicCloud) export AZURE_LOCATION=\u003clocation\u003e # the location export AZURE_LOADBALANCER_SKU=\u003cloadbalancer-sku\u003e # the sku of load balancer (optional, default is basic) Setup KUBECONFIG   Locate your kubeconfig and set it as env variable export KUBECONFIG=\u003ckubeconfig\u003e or cp \u003ckubeconfig\u003e ~/.kube/config\n  Test it via kubectl version\n  Run Test Have installed ginkgo   Run ginkgo ./tests/e2e/ \nFor more usage of ginkgo, please follow ginkgo\n  Without ginkgo  Run go test ./tests/e2e/ -timeout 0  After a long time test, a JUnit report will be generated in a directory named by the cluster name\n","categories":"","description":"Azure E2E tests guidance.\n","excerpt":"Azure E2E tests guidance.\n","ref":"/cloud-provider-azure/development/e2e/e2e-tests-azure/","tags":"","title":"Azure E2E tests"},{"body":"Azure Private Link Service (PLS) is an infrastructure component that allows users to privately connect via a Private Endpoint (PE) in a VNET in Azure and a Frontend IP Configuration associated with an Azure Load Balancer (ALB). With Private Link, users as service providers can securely provide their services to consumers who can connect from within Azure or on-premises without data exfiltration risks.\nBefore Private Link Service for AKS Load Balancer, users who wanted private connectivity from on-premises or other VNETs to their services in the AKS cluster were required to create a Private Link Service (PLS) to reference the AKS Internal LoadBalancer. The user would then create a Private Endpoint (PE) to connect to the PLS to enable private connectivity. With this feature, the PLS to the LB would already be created in the MC_ resource group when the LB frontend is instantiated, and the user would only be required to create PE connections to it for private connectivity.\nCurrently, AKS managed private link service only works with Azure Internal Standard Load Balancer. Users who want to use private link service for their Kubernetes services must set annotation service.beta.kubernetes.io/azure-load-balancer-internal to be true (Doc).\nPrivateLinkService annotations Below is a list of annotations supported for Kubernetes services with Azure PLS created:\n   Annotation Value Description Required Default     service.beta.kubernetes.io/azure-pls-create \"true\" Boolean indicating whether a PLS needs to be created. Required    service.beta.kubernetes.io/azure-pls-name \u003cPLS name\u003e String specifying the name of the PLS resource to be created. Optional \"pls-\u003cLB frontend config id\u003e\"   service.beta.kubernetes.io/azure-pls-ip-configuration-subnet \u003cSubnet name\u003e String indicating the subnet to which the PLS will be deployed. This subnet must exist in the same VNET as the backend pool. PLS NAT IPs are allocated within this subnet. Required    service.beta.kubernetes.io/azure-pls-ip-configuration-ip-version \"ipv4\" or \"ipv6\" IP version of the private IP address. Optional \"ipv4\"   service.beta.kubernetes.io/azure-pls-ip-configuration-ip-address-count [1-8] Total number of private NAT IPs to allocate. Optional 1   service.beta.kubernetes.io/azure-pls-ip-configuration-ip-address \"10.0.0.7 ... 10.0.0.10\" A space separated list of static IPs to be allocated. Total number of IPs should not be greater than the ip count specifed in service.beta.kubernetes.io/azure-pls-ip-configuration-ip-address-count. If there are fewer IPs specified, the rest are dynamically allocated. The first IP in the list is set as Primary. Optional All IPs are dynamically allocated.   service.beta.kubernetes.io/azure-pls-fqdns \"fqdn1 fqdn2\" A space separated list of fqdns associated with the PLS. Optional []   service.beta.kubernetes.io/azure-pls-proxy-protocol \"true\" or \"false\" Boolean indicating whether the TCP PROXY protocol should be enabled on the PLS to pass through connection information, including the link ID and source IP address. Note that the backend service MUST support the PROXY protocol or the connections will fail. Optional false   service.beta.kubernetes.io/azure-pls-visibility \"sub1 sub2 sub3 … subN\" or \"*\" A space separated list of Azure subscription ids for which the private link service is visible. Use \"*\" to expose the PLS to all subs (Least restrictive). Optional Empty list [] indicating role-based access control only: This private link service will only be available to individuals with role-based access control permissions within your directory. (Most restrictive)   service.beta.kubernetes.io/azure-pls-auto-approval \"sub1 sub2 sub3 … subN\" A space separated list of Azure subscription ids. This allows PE connection requests from the subscriptions listed to the PLS to be automatically approved. Optional []    For more details about each configuration, please refer to Azure Private Link Service Documentation.\nDesign Details Creating AKS managed PrivateLinkService When a LoadBalancer typed service is created without the loadBalancerIP field specified, an LB frontend IP configuration is created with a dynamically generated IP. If the service has loadBalancerIP in its spec, an existing LB frontend IP configuration may be reused if one exists; otherwise a static configuration is created with the specified IP. When a service is created with annotation service.beta.kubernetes.io/azure-pls-create set to true or updated later with the annotation added, a PLS resource attached to the LB frontend is created in the MC_ resource group for the AKS cluster.\nThe Kubernetes service creating the PLS is assigned as the owner of the resource. Azure cloud provider tags the PLS with service name kubernetes-owner-service: \u003cnamespace\u003e/\u003cservice name\u003e. Only the owner service can later update the properties of the PLS resource.\nIf there’s an AKS managed PLS already created for the LB frontend, the same PLS is reused automatically since each LB frontend can be referenced by only one PLS. If the LB frontend is attached to a user defined PLS, service creation should fail with proper error logged.\nFor now, AKS does not manage any Private Link Endpoint resources. Once a PLS is created, users can create their own PEs to connect to the PLS.\nDeleting AKS managed PrivateLinkService Once a PLS is created, it shares the lifetime of the LB frontend IP configuration and is deleted only when its corresponding LB frontend gets deleted. As a result, a PLS may still exist even when its owner service is deleted. This is out of the consideration that multiple Kubernetes services can share the same LB frontend IP configuration and thus share the PLS automatically. More details are discussed in next section.\nIf there are active PE connections to the PLS, all connections are removed and the PEs become obsolete. Since PEs are not managed by AKS, AKS is not responsible for cleaning up the PE resources.\nSharing AKS managed PrivateLinkService Multiple Kubernetes services can share the same LB frontend by specifying the same loadBalancerIP (for more details, please refer to Multiple Services Sharing One IP Address). If a PLS is attached to the LB frontend, these services automatically share the PLS. Users can access these services via the same PE but different ports.\nAKS tags the service creating the PLS as the owner (kubernetes-owner-service: \u003cnamespace\u003e/\u003cservice name\u003e) and only allows that service to update the configurations of the PLS. If the owner service is deleted or if user wants some other service to take control, user can modify the tag value to a new service in \u003cnamespace\u003e/\u003cservice name\u003e pattern.\nPLS is only automatically deleted when the LB frontend IP configuration is deleted. One can delete a service while preserving the PLS by creating a temporary service referring to the same LB frontend.\nAKS managed PrivateLinkService Creation example Below we provide an example for creating a Kubernetes service object with Azure ILB and PLS created:\napiVersion:v1kind:Servicemetadata:name:myServiceannotations:service.beta.kubernetes.io/azure-load-balancer-internal:\"true\"# Right now PLS must be used with internal LBservice.beta.kubernetes.io/azure-pls-create:\"true\"service.beta.kubernetes.io/azure-pls-name:myServicePLSservice.beta.kubernetes.io/azure-pls-ip-configuration-subnet:aks-subnetservice.beta.kubernetes.io/azure-pls-ip-configuration-ip-version:ipv4service.beta.kubernetes.io/azure-pls-ip-configuration-ip-address-count:1service.beta.kubernetes.io/azure-pls-ip-configuration-ip-address:10.240.0.9# Must be available in aks-subnetservice.beta.kubernetes.io/azure-pls-fqdns:\"fqdn1 fqdn2\"service.beta.kubernetes.io/azure-pls-proxy-protocol:\"false\"service.beta.kubernetes.io/azure-pls-visibility:\"subId1 subId2\"service.beta.kubernetes.io/azure-pls-auto-approval:\"subId1\"spec:type:LoadBalancerselector:app:myAppports:- name:myAppPortprotocol:TCPport:80targetPort:80","categories":"","description":"Azure PLS Integration Design Document.\n","excerpt":"Azure PLS Integration Design Document.\n","ref":"/cloud-provider-azure/development/design-docs/pls-integration/","tags":"","title":"Azure Private Link Service Integration"},{"body":" azure disk plugin known issues  Recommended stable version for azure disk 1. disk attach error 2. disk unavailable after attach/detach a data disk on a node 3. Azure disk support on Sovereign Cloud 4. Time cost for Azure Disk PVC mount 5. Azure disk PVC Multi-Attach error, makes disk mount very slow or mount failure forever 6. WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax 7. uid and gid setting in azure disk 8. Addition of a blob based disk to VM with managed disks is not supported 9. dynamic azure disk PVC try to access wrong storage account (of other resource group) 10. data loss if using existing azure disk with partitions in disk mount 11. Delete azure disk PVC which is already in use by a pod 12. create azure disk PVC failed due to account creation failure 13. cannot find Lun for disk 14. azure disk attach/detach failure, mount issue, i/o error 15. azure disk could be not detached forever 16. potential race condition issue due to detach disk failure retry 17. very slow disk attach/detach issue when disk num is large 18. detach azure disk make VM run into a limbo state 19. disk attach/detach self-healing on VMAS 20. azure disk detach failure if node not exists 21. invalid disk URI error 22. vmss dirty cache issue 23. race condition when delete disk right after attach disk 24. attach disk costs 10min 25. Multi-Attach error 26. attached non-existing disk volume on agent node 27. failed to get azure instance id for node (not a vmss instance)    Recommended stable version for azure disk    k8s version stable version     v1.15 1.15.11+   v1.16 1.16.10+   v1.17 1.17.6+   v1.18 1.18.3+   v1.19 1.19.0+    1. disk attach error Issue details:\nIn some corner case(detaching multiple disks on a node simultaneously), when scheduling a pod with azure disk mount from one node to another, there could be lots of disk attach error(no recovery) due to the disk not being released in time from the previous node. This issue is due to lack of lock before DetachDisk operation, actually there should be a central lock for both AttachDisk and DetachDisk operations, only one AttachDisk or DetachDisk operation is allowed at one time.\nThe disk attach error could be like following:\nCannot attach data disk 'cdb-dynamic-pvc-92972088-11b9-11e8-888f-000d3a018174' to VM 'kn-edge-0' because the disk is currently being detached or the last detach operation failed. Please wait until the disk is completely detached and then try again or delete/detach the disk explicitly again. Related issues\n Azure Disk Detach are not working with multiple disk detach on the same Node Azure disk fails to attach and mount, causing rescheduled pod to stall following node disruption Since Intel CPU Azure update, new Azure Disks are not mounting, very critical…  Busy azure-disk regularly fail to mount causing K8S Pod deployments to halt  Mitigation:\n option#1: Update every agent node that has attached or detached the disk in problem  In Azure cloud shell, run\n$vm = Get-AzureRMVM -ResourceGroupName $rg -Name $vmname Update-AzureRmVM -ResourceGroupName $rg -VM $vm -verbose -debug In Azure cli, run\naz vm update -g \u003cgroup\u003e -n \u003cname\u003e  option#2:   kubectl cordon node #make sure no scheduling on this node kubectl drain node #schedule pod in current node to other node restart the Azure VM for node via the API or portal, wait until VM is “Running” kubectl uncordon node  Fix\n PR fix race condition issue when detaching azure disk has fixed this issue by add a lock before DetachDisk     k8s version fixed version     v1.6 no fix   v1.7 1.7.14   v1.8 1.8.9   v1.9 1.9.5   v1.10 1.10.0    2. disk unavailable after attach/detach a data disk on a node  💡 NOTE: Azure platform has fixed the host cache issue, the suggested host cache setting of data disk is ReadOnly now, more details about azure disk cache setting Issue details:\n From k8s v1.7, default host cache setting changed from None to ReadWrite, this change would lead to device name change after attach multiple disks on a node, finally lead to disk unavailable from pod. When access data disk inside a pod, will get following error:\n[root@admin-0 /]# ls /datadisk ls: reading directory .: Input/output error In my testing on Ubuntu 16.04 D2_V2 VM, when attaching the 6th data disk will cause device name change on agent node, e.g. following lun0 disk should be sdc other than sdk.\nazureuser@k8s-agentpool2-40588258-0:~$ tree /dev/disk/azure ... â””â”€â”€ scsi1 â”œâ”€â”€ lun0 -\u003e ../../../sdk â”œâ”€â”€ lun1 -\u003e ../../../sdj â”œâ”€â”€ lun2 -\u003e ../../../sde â”œâ”€â”€ lun3 -\u003e ../../../sdf â”œâ”€â”€ lun4 -\u003e ../../../sdg â”œâ”€â”€ lun5 -\u003e ../../../sdh â””â”€â”€ lun6 -\u003e ../../../sdi Related issues\n device name change due to azure disk host cache setting unable to use azure disk in StatefulSet since /dev/sd* changed after detach/attach disk Disk error when pods are mounting a certain amount of volumes on a node unable to use azure disk in StatefulSet since /dev/sd* changed after detach/attach disk Input/output error when accessing PV PersistentVolumeClaims changing to Read-only file system suddenly  Workaround:\n add cachingmode: None in azure disk storage class(default is ReadWrite), e.g.  kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:hddprovisioner:kubernetes.io/azure-diskparameters:skuname:Standard_LRSkind:Managedcachingmode:NoneFix\n PR fix device name change issue for azure disk could fix this issue too, it will change default cachingmode value from ReadWrite to None.     k8s version fixed version     v1.6 no such issue as cachingmode is already None by default   v1.7 1.7.14   v1.8 1.8.11   v1.9 1.9.4   v1.10 1.10.0    3. Azure disk support on Sovereign Cloud Fix\n PR Azure disk on Sovereign Cloud fixed this issue     k8s version fixed version     v1.7 1.7.9   v1.8 1.8.3   v1.9 1.9.0   v1.10 1.10.0    4. Time cost for Azure Disk PVC mount Original time cost for Azure Disk PVC mount on a standard node size(e.g. Standard_D2_V2) is around 1 minute, podAttachAndMountTimeout is 2 minutes, total waitForAttachTimeout is 10 minutes, so a disk remount(detach and attach in sequential) would possibly cost more than 2min, thus may fail.\n Note: for some smaller VM size which has only 1 CPU core, time cost would be much bigger(e.g. \u003e 10min) since container is hard to get CPU slot.\n Related issues\n ‘timeout expired waiting for volumes to attach/mount for pod when cluster’ when node-vm-size is Standard_B1s  Fix\n PR using cache fix fixed this issue, which could reduce the mount time cost to around 30s.     k8s version fixed version     v1.8 no fix   v1.9 1.9.2   v1.10 1.10.0    5. Azure disk PVC Multi-Attach error, makes disk mount very slow or mount failure forever  💡 NOTE: AKS and current aks-engine won’t have this issue since it’s not using containerized kubelet\n Issue details:\nWhen schedule a pod with azure disk volume from one node to another, total time cost of detach \u0026 attach is around 1 min from v1.9.2, while in v1.9.x, there is an UnmountDevice failure issue in containerized kubelet which makes disk mount very slow or mount failure forever, this issue only exists in v1.9.x due to PR Refactor nsenter, v1.10.0 won’t have this issue since devicePath is updated in v1.10 code\nerror logs:\n kubectl describe po POD-NAME  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m default-scheduler Successfully assigned deployment-azuredisk1-6cd8bc7945-kbkvz to k8s-agentpool-88970029-0 Warning FailedAttachVolume 3m attachdetach-controller Multi-Attach error for volume \"pvc-6f2d0788-3b0b-11e8-a378-000d3afe2762\" Volume is already exclusively attached to one node and can't be attached to another Normal SuccessfulMountVolume 3m kubelet, k8s-agentpool-88970029-0 MountVolume.SetUp succeeded for volume \"default-token-qt7h6\" Warning FailedMount 1m kubelet, k8s-agentpool-88970029-0 Unable to mount volumes for pod \"deployment-azuredisk1-6cd8bc7945-kbkvz_default(5346c040-3e4c-11e8-a378-000d3afe2762)\": timeout expired waiting for volumes to attach/mount for pod \"default\"/\"deployment-azuredisk1-6cd8bc7945-kbkvz\". list of unattached/unmounted volumes=[azuredisk]  kubelet logs from the new node  E0412 20:08:10.920284 7602 nestedpendingoperations.go:263] Operation for \"\\\"kubernetes.io/azure-disk//subscriptions/xxx/resourceGroups/MC_xxx_eastus/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\"\" failed. No retries permitted until 2018-04-12 20:08:12.920234762 +0000 UTC m=+1467.278612421 (durationBeforeRetry 2s). Error: \"Volume has not been added to the list of VolumesInUse in the node's volume status for volume \\\"pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\" (UniqueName: \\\"kubernetes.io/azure-disk//subscriptions/xxx/resourceGroups/MC_xxx_eastus/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\") pod \\\"symbiont-node-consul-0\\\" (UID: \\\"11043b12-3e8d-11e8-82ec-0a58ac1f04cf\\\") \" Related issues\n UnmountDevice would fail in containerized kubelet upgrade k8s process is broke  Mitigation:\nIf azure disk PVC mount successfully in the end, there is no action, while if it could not be mounted for more than 20min, following actions could be taken:\n check whether volumesInUse list has unmounted azure disks, run:  kubectl get no NODE-NAME -o yaml \u003e node.log all volumes in volumesInUse should be also in volumesAttached, otherwise there would be issue\n restart kubelet on the original node would solve this issue: sudo kubectl kubelet restart  Fix\n PR fix nsenter GetFileType issue in containerized kubelet fixed this issue     k8s version fixed version     v1.8 no such issue   v1.9 v1.9.7   v1.10 no such issue    After fix in v1.9.7, it took about 1 minute for scheduling one azure disk mount from one node to another, you could find details here.\nSince azure disk attach/detach operation on a VM cannot be parallel, scheduling 3 azure disk mounts from one node to another would cost about 3 minutes.\n6. WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax Issue details: MountVolume.WaitForAttach may fail in the azure disk remount\nerror logs:\nin v1.10.0 \u0026 v1.10.1, MountVolume.WaitForAttach will fail in the azure disk remount, error logs would be like following:\n incorrect DevicePath format on Linux  MountVolume.WaitForAttach failed for volume \"pvc-f1562ecb-3e5f-11e8-ab6b-000d3af9f967\" : azureDisk - Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun1 (strconv.Atoi: parsing \"/dev/disk/azure/scsi1/lun1\": invalid syntax) Warning FailedMount 1m (x10 over 21m) kubelet, k8s-agentpool-66825246-0 Unable to mount volumes for pod  wrong DevicePath(LUN) number on Windows   Warning FailedMount 1m kubelet, 15282k8s9010 MountVolume.WaitForAttach failed for volume \"disk01\" : azureDisk - WaitForAttach failed within timeout node (15282k8s9010) diskId:(andy-mghyb 1102-dynamic-pvc-6c526c51-4a18-11e8-ab5c-000d3af7b38e) lun:(4) Related issues\n WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax Pod unable to attach PV after being deleted (Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun0 (strconv.Atoi: parsing “/dev/disk/azure/scsi1/lun0”: invalid syntax)  Fix\n PR fix WaitForAttach failure issue for azure disk fixed this issue     k8s version fixed version     v1.8 no such issue   v1.9 no such issue   v1.10 1.10.2    7. uid and gid setting in azure disk Issue details: Unlike azure file mountOptions, you will get following failure if set mountOptions like uid=999,gid=999 in azure disk mount:\nWarning FailedMount 63s kubelet, aks-nodepool1-29460110-0 MountVolume.MountDevice failed for volume \"pvc-d783d0e4-85a1-11e9-8a90-369885447933\" : azureDisk - mountDevice:FormatAndMount failed with mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/azure-disk/mounts/m436970985 --scope -- mount -t xfs -o dir_mode=0777,file_mode=0777,uid=1000,gid=1000,defaults /dev/disk/azure/scsi1/lun2 /var/lib/kubelet/plugins/kubernetes.io/azure-disk/mounts/m436970985 Output: Running scope as unit run-rb21966413ab449b3a242ae9b0fbc9398.scope. mount: wrong fs type, bad option, bad superblock on /dev/sde, missing codepage or helper program, or other error That’s because azureDisk use ext4,xfs file system by default, mountOptions like [uid=x,gid=x] could not be set in mount time.\nRelated issues\n Timeout expired waiting for volumes to attach Pod failed mounting xfs format volume with mountOptions Allow volume ownership to be only set after fs formatting  Solution:\n option#1: Set uid in runAsUser and gid in fsGroup for pod: security context for a Pod  e.g. Following setting will set pod run as root, make it accessible to any file:\napiVersion:v1kind:Podmetadata:name:security-context-demospec:securityContext:runAsUser:0fsGroup:0 Note: Since gid \u0026 uid is mounted as 0(root) by default, if set as non-root(e.g. 1000), k8s will use chown to change all dir/files under that disk, this is a time consuming job, which would make mount device very slow, in this issue: Timeout expired waiting for volumes to attach, it costs about 10 min for chown operation complete.\n  option#2: use chown in initContainers  initContainers: - name: volume-mount image: busybox command: [\"sh\", \"-c\", \"chown -R 100:100 /data\"] volumeMounts: - name: \u003cyour data volume\u003e mountPath: /data  new upstream feature to address this: Allow volume ownership to be only set after fs formatting  8. Addition of a blob based disk to VM with managed disks is not supported Issue details:\nFollowing error may occur if attach a blob based(unmanaged) disk to VM with managed disks:\nWarning FailedMount 42s (x2 over 1m) attachdetach AttachVolume.Attach failed for volume \"pvc-f17e5e77-474e-11e8-a2ea-000d3a10df6d\" : Attach volume \"holo-k8s-dev-dynamic-pvc-f17e5e77-474e-11e8-a2ea-000d3a10df6d\" to instance \"k8s-master-92699158-0\" failed with compute.VirtualMachinesClient#CreateOrUpdate: Failure responding to request: StatusCode=409 -- Original Error: autorest/azure: Service returned an error. Status=409 Code=\"OperationNotAllowed\" Message=\"Addition of a blob based disk to VM with managed disks is not supported.\" This issue is by design as in Azure, there are two kinds of disks, blob based(unmanaged) disk and managed disk, an Azure VM could not attach both of these two kinds of disks.\nSolution:\nUse default azure disk storage class in acs-engine, as default will always be identical to the agent pool, that is, if VM is managed, it will be managed azure disk class, if unmanaged, then it’s unmanaged disk class.\n9. dynamic azure disk PVC try to access wrong storage account (of other resource group) Issue details:\nIn a k8s cluster with blob based VMs(won’t happen in AKS since AKS only use managed disk), create dynamic azure disk PVC may fail, error logs is like following:\nFailed to provision volume with StorageClass \"default\": azureDisk - account ds6c822a4d484211eXXXXXX does not exist while trying to create/ensure default container Related issues\n Multiple clusters - dynamic PVCs try to access wrong storage account (of other resource group)  Fix\n PR fix storage account not found issue: use ListByResourceGroup instead of List() fixed this issue     k8s version fixed version     v1.8 1.8.13   v1.9 1.9.9   v1.10 no such issue    Work around:\nthis bug only exists in blob based VM in v1.8.x, v1.9.x, so if specify ManagedDisks when creating k8s cluster in acs-engine(AKS is using managed disk by default), it won’t have this issue:\n\"agentPoolProfiles\": [ { ... \"storageProfile\" : \"ManagedDisks\", ... } 10. data loss if using existing azure disk with partitions in disk mount Issue details:\nWhen use an existing azure disk(also called static provisioning) in pod, if that disk has partitions, the disk will be formatted in the pod mounting process, actually k8s volume don’t support mount disk with partitions, disk mount would fail finally. While for mounting existing azure disk that has partitions, data will be lost since it will format that disk first. This issue happens only on Linux.\nRelated issues\n data loss if using existing azure disk with partitions in disk mount  Fix\n PR fix data loss issue if using existing azure disk with partitions in disk mount will let azure provider return error when mounting existing azure disk that has partitions     k8s version fixed version     v1.8 1.8.15   v1.9 1.9.11   v1.10 1.10.5   v1.11 1.11.0    Work around:\nDon’t use existing azure disk that has partitions, e.g. following disk in LUN 0 that has one partition:\nazureuser@aks-nodepool1-28371372-0:/$ ls -l /dev/disk/azure/scsi1/ total 0 lrwxrwxrwx 1 root root 12 Apr 27 08:04 lun0 -\u003e ../../../sdc lrwxrwxrwx 1 root root 13 Apr 27 08:04 lun0-part1 -\u003e ../../../sdc1 11. Delete azure disk PVC which is already in use by a pod Issue details:\nFollowing error may occur if delete azure disk PVC which is already in use by a pod:\nkubectl describe pv pvc-d8eebc1d-74d3-11e8-902b-e22b71bb1c06 ... Message: disk.DisksClient#Delete: Failure responding to request: StatusCode=409 -- Original Error: autorest/azure: Service returned an error. Status=409 Code=\"OperationNotAllowed\" Message=\"Disk kubernetes-dynamic-pvc-d8eebc1d-74d3-11e8-902b-e22b71bb1c06 is attached to VM /subscriptions/{subs-id}/resourceGroups/MC_markito-aks-pvc_markito-aks-pvc_westus/providers/Microsoft.Compute/virtualMachines/aks-agentpool-25259074-0.\" Fix:\nThis is a common k8s issue, other cloud provider would also has this issue. There is a PVC protection feature to prevent this, it’s alpha in v1.9, and beta(enabled by default) in v1.10\nWork around: delete pod first and then delete azure disk pvc after a few minutes\n12. create azure disk PVC failed due to account creation failure  please note this issue only happens on unmanaged k8s cluster\n Issue details: User may get Account property kind is invalid for the request error when trying to create a new unmanaged azure disk PVC, error would be like following:\nazureuser@k8s-master-17140924-0:/tmp$ kubectl describe pvc Name: pvc-azuredisk Namespace: default StorageClass: hdd Status: Bound ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 31m persistentvolume-controller Failed to provision volume with StorageClass \"hdd\": Create Storage Account: ds10e15ed89c5811e8a0a70, error: storage.AccountsClient#Create: Failure sending request: StatusCode=400 -- Original Error: Code=\"AccountPropertyIsInvalid\" Message=\"Account property kind is invalid for the request.\" Fix\n PR fix azure disk create failure due to sdk upgrade fixed this issue     k8s version fixed version     v1.9 no such issue   v1.10 no such issue   v1.11 1.11.3   v1.12 no such issue    Work around:\n create a storage account and specify that account in azure disk storage class, e.g.  kind:StorageClassapiVersion:storage.k8s.io/v1beta1metadata:name:ssdprovisioner:kubernetes.io/azure-diskparameters:skuname:Premium_LRSstorageAccount:customerstorageaccountkind:Dedicated13. cannot find Lun for disk Issue details:\nFollowing error may occur if attach a disk to a node:\nMountVolume.WaitForAttach failed for volume \"pvc-12b458f4-c23f-11e8-8d27-46799c22b7c6\" : Cannot find Lun for disk kubernetes-dynamic-pvc-12b458f4-c23f-11e8-8d27-46799c22b7c6 Related issues\n GetAzureDiskLun sometimes costs 1 min which is too long time  Fix\n PR fix azure disk attachment error on Linux will extract the LUN num from device path only on Linux     k8s version fixed version     v1.9 no such issue   v1.10 1.10.10   v1.11 1.11.5   v1.12 1.12.3   v1.13 no such issue    Work around:\nwait for a few more minutes should work\n14. azure disk attach/detach failure, mount issue, i/o error Issue details:\nWe found a disk attach/detach issue due to dirty vm cache PR introduced from v1.9.2, it would lead to following disk issues:\n disk attach/detach failure for a long time disk I/O error unexpected disk detachment from VM VM running into failed state due to attaching non-existing disk   Note: above error may only happen when there are multiple disk attach/detach operations in parallel and it’s not easy to repro since it happens on a little possibility.\n Related issues\n Azure Disks volume attach still times out on Kubernetes 1.10 Azure Disks occasionally mounted in a way leading to I/O errors  Fix\nWe changed the azure disk attach/detach retry logic in k8s v1.13, switch to use k8s attach-detach controller to do attach/detach disk retry and clean vm cache after every disk operation, this issue is proved to be fixed in our disk attach/detach stress test and also verified in customer env:\n PR remove retry operation on attach/detach azure disk in azure cloud provider PR fix azure disk attach/detach failed forever issue PR fix detach azure disk issue due to dirty cache     k8s version fixed version     v1.9 issue introduced in v1.9.2, no cherry-pick fix allowed   v1.10 1.10.12   v1.11 1.11.6   v1.12 1.12.4   v1.13 no such issue    Work around:\n if there is attach disk failure for long time, restart controller manager may work if there is disk not detached for long time, detach that disk manually  Related issues\n Multi Attach Error  15. azure disk could be not detached forever Issue details:\nIn some condition when first detach azure disk operation failed, it won’t retry and the azure disk would be still attached to the original VM node.\nFollowing error may occur when move one disk from one node to another(keyword: ConflictingUserInput):\n[Warning] AttachVolume.Attach failed for volume “pvc-7b7976d7-3a46-11e9-93d5-dee1946e6ce9” : Attach volume “kubernetes-dynamic-pvc-7b7976d7-3a46-11e9-93d5-dee1946e6ce9\" to instance “/subscriptions/XXX/resourceGroups/XXX/providers/Microsoft.Compute/virtualMachines/aks-agentpool-57634498-0” failed with compute.VirtualMachinesClient#CreateOrUpdate: Failure sending request: StatusCode=0 -- Original Error: autorest/azure: Service returned an error. Status= Code=“ConflictingUserInput” Message=“Disk ‘/subscriptions/XXX/resourceGroups/XXX/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-7b7976d7-3a46-11e9-93d5-dee1946e6ce9’ cannot be attached as the disk is already owned by VM ‘/subscriptions/XXX/resourceGroups/XXX/providers/Microsoft.Compute/virtualMachines/aks-agentpool-57634498-1’.” Fix\nWe added retry logic for detach azure disk:\n PR add retry for detach azure disk     k8s version fixed version     v1.10 N/A   v1.11 1.11.9   v1.12 1.12.7   v1.13 1.13.4   v1.14 1.14.0   v1.15 1.15.0    Work around:\n if there is disk not detached for long time, detach that disk manually  16. potential race condition issue due to detach disk failure retry Issue details:\nIn some error condition when detach azure disk failed, azure cloud provider will retry 6 times at most with exponential backoff, it will hold the data disk list for about 3 minutes with a node level lock, and in that time period, if customer update data disk list manually (e.g. need manual operationto attach/detach another disk since there is attach/detach error, ) , the data disk list will be obselete(dirty data), then weird VM status happens, e.g. attach a non-existing disk, we should split those retry operations, every retry should get a fresh data disk list in the beginning.\nFix\nFollowing PR refined detach azure disk retry operation, make every detach azure disk operation in a standalone function\n PR fix detach azure disk back off issue which has too big lock in failure retry condition PR fix azure disk list corruption issue     k8s version fixed version     v1.10 N/A   v1.11 no fix   v1.12 1.12.9   v1.13 1.13.6   v1.14 1.14.2   v1.15 1.15.0    Work around:\nDetach all the non-existing disks from VM (could do that in azure portal by bulk update)\n Detaching disk one by one using cli may fail since they are already non-existing disks.\n 17. very slow disk attach/detach issue when disk num is large Issue details:\nWe hit very slow disk attach/detach issue when disk num is large(\u003e 10 disks on one VM)\nFix\nAzure disk team are fixing this issue.\nWork around:\nNo workaround.\n18. detach azure disk make VM run into a limbo state Issue details:\nIn some corner condition, detach azure disk would sometimes make VM run into a limbo state\nFix\nFollowing two PRs would fix this issue by retry update VM if detach disk partially fail:\n fix azure retry issue when return 2XX with error fix: retry detach azure disk issue     k8s version fixed version     v1.11 no fix   v1.12 1.12.10   v1.13 1.13.8   v1.14 1.14.4   v1.15 1.15.0    Work around:\nUpdate VM status manually would solve the problem:\n Update Availability Set VM  az vm update -n \u003cVM_NAME\u003e -g \u003cRESOURCE_GROUP_NAME\u003e  Update Scale Set VM  az vmss update-instances -g \u003cRESOURCE_GROUP_NAME\u003e --name \u003cVMSS_NAME\u003e --instance-id \u003cID(number)\u003e 19. disk attach/detach self-healing on VMAS Issue details: There could be disk detach failure due to many reasons(e.g. disk RP busy, controller manager crash, etc.), and it would fail when attach one disk to other node if that disk is still attached to the old node, user needs to manually detach disk in problem in the before, with this fix, azure cloud provider would check and detach this disk if it’s already attached to the other node, that’s like self-healing. This PR could fix lots of such disk attachment issue.\nFix\nFollowing PR would first check whether current disk is already attached to other node, if so, it would trigger a dangling error and k8s controller would detach disk first, and then do the attach volume operation.\nThis PR would also fix a “disk not found” issue when detach azure disk due to disk URI case sensitive case, error logs are like following(without this PR):\nazure_controller_standard.go:134] detach azure disk: disk not found, diskURI: /subscriptions/xxx/resourceGroups/andy-mg1160alpha3/providers/Microsoft.Compute/disks/xxx-dynamic-pvc-41a31580-f5b9-4f08-b0ea-0adcba15b6db Fix\n Fix on VMAS  fix: detach azure disk issue using dangling error fix: azure disk name matching issue       k8s version fixed version     v1.12 no fix   v1.13 1.13.11   v1.14 1.14.7   v1.15 1.15.4   v1.15 1.16.0     Fix on VMSS  fix: azure disk dangling attach issue on VMSS which would cause API throttling       k8s version fixed version     v1.15 no fix   v1.16 1.16.9   v1.17 1.17.6   v1.18 1.18.3   v1.19 1.19.0    Work around:\nmanually detach disk in problem and wait for disk attachment happen automatically\n20. azure disk detach failure if node not exists Issue details: If a node with a Azure Disk attached is deleted (before the volume is detached), subsequent attempts by the attach/detach controller to detach it continuously fail, and prevent the controller from attaching the volume to another node.\nFix\n fix: azure disk detach failure if node not exists     k8s version fixed version     v1.12 no fix   v1.13 1.13.9   v1.14 1.14.8   v1.15 1.15.5   v1.16 1.16.1   v1.16 1.17.0    Work around:\nRestart kube-controller-manager on master node.\n21. invalid disk URI error Issue details:\nWhen user use an existing disk in static provisioning, may hit following error:\nAttachVolume.Attach failed for volume \"azure\" : invalid disk URI: /subscriptions/xxx/resourcegroups/xxx/providers/Microsoft.Compute/disks/Test_Resize_1/” Fix\n fix: make azure disk URI as case insensitive     k8s version fixed version     v1.13 no fix   v1.14 1.14.9   v1.15 1.15.6   v1.16 1.16.0   v1.17 1.17.0    Work around:\nUse resourceGroups instead of resourcegroups in disk PV configuration\n22. vmss dirty cache issue Issue details:\nclean vmss cache should happen after disk attach/detach operation, now it’s before those operations, which would lead to dirty cache. since update operation may cost 30s or more, and at that time period, if there is another get vmss operation, it would get the old data disk list\n VMSS disk attach/detach issues w/ v1.13.12, v1.14.8, v1.15.5, v1.16.2 Disk attachment/mounting problems, all pods with PVCs stuck in ContainerCreating  Fix\n fix vmss dirty cache issue     k8s version fixed version notes     v1.13 no fix regression since 1.13.12 (hotfixed in AKS release)   v1.14 1.14.10 regression only in 1.14.8, 1.14.9 (hotfixed in AKS release)   v1.15 1.15.7 regression only in 1.15.5, 1.15.6 (hotfixed in AKS release)   v1.16 1.16.4 regression only in 1.16.2, 1.16.3 (hotfixed in AKS release)   v1.17 1.17.0     Work around:\nDetach disk in problem manually\n23. race condition when delete disk right after attach disk Issue details:\nThere is condition that attach and delete disk happens in same time, azure CRP don’t check such race condition\n should not delete an azure disk when that disk is being attached  Fix\n fix race condition when delete azure disk right after attach azure disk     k8s version fixed version notes     v1.13 no fix hotfixed in AKS release since 1.13.12   v1.14 1.14.10 hotfixed in AKS release in 1.14.8, 1.14.9   v1.15 1.15.7 hotfixed in AKS release in 1.15.5, 1.15.6   v1.16 1.16.4 hotfixed in AKS release in 1.16.2, 1.16.3   v1.17 1.17.0     Work around:\nDetach disk in problem manually\n24. attach disk costs 10min Issue details:\nPR Fix aggressive VM calls for Azure VMSS change getVMSS cache TTL from 1min to 10min, getVMAS cache TTL from 5min to 10min, that will cause error WaitForAttach ... Cannot find Lun for disk, and it would make attach disk opeation costs 10min on VMSS and 15min on VMAS, detailed error would be like following:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 29m default-scheduler Successfully assigned authentication/authentication-mssql-statefulset-0 to aks-nodepool1-29122124-vmss000004 Normal SuccessfulAttachVolume 28m attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-8d9f0ade-1825-11ea-83a0-22ced17d4a3d\" Warning FailedMount 23m (x10 over 27m) kubelet, aks-nodepool1-29122124-vmss000004 MountVolume.WaitForAttach failed for volume \"pvc-8d9f0ade-1825-11ea-83a0-22ced17d4a3d\" : Cannot find Lun for disk kubernetes-dynamic-pvc-8d9f0ade-1825-11ea-83a0-22ced17d4a3d Warning FailedMount 23m (x3 over 27m) kubelet, aks-nodepool1-29122124-vmss000004 Unable to mount volumes for pod \"authentication-mssql-statefulset-0_authentication(8df467e7-1825-11ea-83a0-22ced17d4a3d)\": timeout expired waiting for volumes to attach or mount for pod \"authentication\"/\"authentication-mssql-statefulset-0\". list of unmounted volumes=[authentication-mssql-persistent-data-storage]. list of unattached volumes=[authentication-mssql-persistent-data-storage default-token-b7spv] Normal Pulled 21m kubelet, aks-nodepool1-29122124-vmss000004 Container image \"mcr.microsoft.com/mssql/server:2019-CTP3.2-ubuntu\" already present on machine Normal Created 21m kubelet, aks-nodepool1-29122124-vmss000004 Created container authentication-mssql Normal Started 21m kubelet, aks-nodepool1-29122124-vmss000004 Started container authentication-mssql This slow disk attachment issue only exists on 1.13.12+, 1.14.8+, fortunately, from k8s 1.15.0, this issue won’t happen, since getDiskLUN logic has already been refactored (already has PR:fix azure disk lun error, won’t depend on getVMSS operation to get disk LUN.\nRelate issues:\n GetAzureDiskLun sometimes costs 10min which is too long time  Fix\n fix azure disk lun error     k8s version fixed version notes     v1.13 no fix need to hotfix in AKS release since 1.13.12 (slow disk attachment exists on 1.13.12+)   v1.14 in cherry-pick need to hotfix in AKS release in 1.14.8, 1.14.9 (slow disk attachment exists on 1.14.8+)   v1.15 1.15.0    v1.16 1.16.0     Work around:\nWait for about 10min or 15min, MountVolume.WaitForAttach operation would retry and would finally succeed\n25. Multi-Attach error Issue details:\nIf two pods on different nodes are using same disk PVC(this issue may also happen when doing rollingUpdate in Deployment using one replica), would probably hit following error:\nEvents: Warning FailedAttachVolume 9m attachdetach-controller Multi-Attach error for volume \"pvc-fc0bed38-48bf-43f1-a7e4-255eef48ffb9\" Volume is already used by pod(s) sqlserver3-5b8449449-5chzx Warning FailedMount 42s (x4 over 7m) kubelet, aks-nodepool1-15915763-vmss000001 Unable to mount volumes for pod \"sqlserver3-55754785bb-jjr6d_default(55381f38-9640-43a9-888d-096387cbb780)\": timeout expired waiting for volumes to attach or mount for pod \"default\"/\"sqlserver3-55754785bb-jjr6d\". list of unmounted volumes=[mssqldb]. list of unattached volumes=[mssqldb default-token-q7cw9] The above issue is upstream issue(detailed error code), it could be due to following reasons:\n two pods are using same disk PVC, this issue could happen even using Deployment with one replica(see below workaround) one node is in Shutdown(deallocated) state, this is by design now and there is on-going upstream work to fix this issue  Propose to taint node “shutdown” condition add node shutdown KEP      workaround: user could use set terminationGracePeriodSeconds: 0 in deployment or kubectl delete pod PODNAME --grace-period=0 --force to delete pod on the deallocated node Azure cloud provider solution: delete shutdown node(in InstanceExistsByProviderID) like what other cloud provider does today, while it may lead to other problem(e.g. node label loss), see details: Common handling of stopped instances across cloud providers.    since azure disk PVC could not be attached to one node.\nRelate issues:\n Trouble attaching volume  Work around:\nWhen using disk PVC config in deployment, maxSurge: 0 could make sure there would not be no more than two pods in Running/ContainerCreating state when doing rollingUpdate:\ntemplate: ... strategy: rollingUpdate: maxSurge: 0 maxUnavailable: 1 type: RollingUpdate Refer to Rolling Updates with Kubernetes Deployments for more detailed rollingUpdate config, and you could find maxSurge: 0 setting example here\nNote\n error messages:  Multi-Attach error for volume \"pvc-e9b72e86-129a-11ea-9a02-9abdbf393c78\" Volume is already used by pod(s)    two pods are using same disk PVC, this issue could happen even using Deployment with one replica, check detailed explanation and workaround here with above explanation\n Multi-Attach error for volume \"pvc-0d7740b9-3a43-11e9-93d5-dee1946e6ce9\" Volume is already exclusively attached to one node and can't be attached to another  This could be a transient error when move volume from one node to another, use following command to find attached node:\nkubectl get no -o yaml | grep volumesAttached -A 15 | grep pvc-0d7740b9-3a43-11e9-93d5-dee1946e6ce9 -B 10 -A 15 related code: reportMultiAttachError\n26. attached non-existing disk volume on agent node Issue details:\nThere is little possibility that attach/detach disk and disk deletion happened in same time, that would cause race condition. This PR add remediation when attach/detach disk, if returned 404 error, it will filter out all non-existing disks and try attach/detach operation again.\nFix\n fix: add remediation in azure disk attach/detach fix: azure disk remediation issue     k8s version fixed version     v1.14 no fix   v1.15 1.15.11   v1.16 1.16.8   v1.17 1.17.4   v1.18 1.18.0    Work around:\nDetach disk in problem manually\n27. failed to get azure instance id for node (not a vmss instance) Issue details:\nPR#81266 does not convert the VMSS node name which causes error like this:\nfailed to get azure instance id for node \\\"k8s-agentpool1-32474172-vmss_1216\\\" (not a vmss instance) That will make dangling attach return error, and k8s volume attach/detach controller will getVmssInstance, and since the nodeName is in an incorrect format, it will always clean vmss cache if node not found, thus incur a get vmss API call storm.\nFix\n fix: azure disk dangling attach issue on VMSS which would cause API throttling     k8s version fixed version     v1.14 only hotfixed with image mcr.microsoft.com/oss/kubernetes/hyperkube:v1.14.8-hotfix.20200529.1   v1.15 only hotfixed with image mcr.microsoft.com/oss/kubernetes/hyperkube:v1.15.11-hotfix.20200529.1, mcr.microsoft.com/oss/kubernetes/hyperkube:v1.15.12-hotfix.20200603   v1.16 1.16.10 (also hotfixed with image mcr.microsoft.com/oss/kubernetes/hyperkube:v1.16.9-hotfix.20200529.1)   v1.17 1.17.6   v1.18 1.18.3   v1.19 1.19.0    Work around:\n Stop kube-controller-manager detach disk in problem from that vmss node manually  az vmss disk detach -g \u003cRESOURCE_GROUP_NAME\u003e --name \u003cVMSS_NAME\u003e --instance-id \u003cID(number)\u003e --lun number e.g. per below logs,\nE0501 11:15:40.981758 1 attacher.go:277] failed to detach azure disk \"/subscriptions/xxx/resourceGroups/rg/providers/Microsoft.Compute/disks/rg-dynamic-pvc-dc282131-b669-47db-8d57-cb3b9789ac3e\", err failed to get azure instance id for node \"k8s-agentpool1-32474172-vmss_1216\" (not a vmss instance)  find lun number of disk rg-dynamic-pvc-dc282131-b669-47db-8d57-cb3b9789ac3e:  az vmss show -g rg --name k8s-agentpool1-32474172-vmss --instance-id 1216  detach vmss disk manually:  az vmss disk detach -g rg --name k8s-agentpool1-32474172-vmss --instance-id 1216 --lun number Start kube-controller-manager  ","categories":"","description":"","excerpt":" azure disk plugin known issues  Recommended stable version for azure …","ref":"/cloud-provider-azure/faq/known-issues/azuredisk/","tags":"","title":"AzureDisk CSI Driver Known Issues"},{"body":" azure file plugin known issues  Recommended stable version for azure file 1. azure file mountOptions setting  file/dir mode setting: other useful mountOptions setting:   2. permission issue of azure file dynamic provision in acs-engine 3. Azure file support on Sovereign Cloud 4. azure file dynamic provision failed due to cluster name length issue 5. azure file dynamic provision failed due to no storage account in current resource group 6. azure file plugin on Windows does not work after node restart 7. file permission could not be changed using azure file, e.g. postgresql 8. Could not delete pod with AzureFile volume if storage account key changed 9. Long latency when handling lots of small files 10. allow access from selected network setting on storage account will break azure file dynamic provisioning 11. azure file remount on Windows in same node would fail 12. update azure file secret if azure storage account key changed 13. Create Azure Files PV AuthorizationFailure when using advanced networking 14. initial delay(5s) in mounting azure file    Recommended stable version for azure file    k8s version stable version     v1.7 1.7.14+   v1.8 1.8.11+   v1.9 1.9.7+   v1.10 1.10.2+   v1.11 1.11.8+   v1.12 1.12.6+   v1.13 1.13.4+   v1.14 1.14.0+    1. azure file mountOptions setting file/dir mode setting: Issue details:\n fileMode, dirMode value would be different in different versions, in latest master branch, it’s 0755 by default, to set a different value, follow this mount options support of azure file (available from v1.8.5). For version v1.8.0-v1.8.4, since mount options support of azure file is not available, as a workaround, securityContext could be specified for the pod, detailed pod example  securityContext:runAsUser:XXXfsGroup:XXX   version fileMode, dirMode value     v1.6.x, v1.7.x 0777   v1.8.0 ~ v1.8.5, v1.9.0 0700   v1.8.6 or later, v1.9.1 ~ v1.10.9, v1.11.0 ~ v1.11.3, v1.12.0 ~ v.12.1 0755   v1.10.10 or later 0777   v1.11.4 or later 0777   v1.12.2 or later 0777   v1.13.x 0777    other useful mountOptions setting:  mfsymlinks: make azure file(cifs) mount supports symbolic link nobrl: Do not send byte range lock requests to the server. This is necessary for certain applications that break with cifs style mandatory byte range locks (and most cifs servers do not yet support requesting advisory byte range locks). Error message could be like following:  Error: SQLITE_BUSY: database is locked Related issues\n azureFile volume mode too strict for container with non root user Unable to connect to SQL-lite db mounted on AzureFile/AzureDisks [SQLITE_BUSY: database is locked] Allow nobrl parameter like docker to use sqlite over network drive Error to deploy mongo with azure file storage  2. permission issue of azure file dynamic provision in acs-engine Issue details:\nFrom acs-engine v0.12.0, RBAC is enabled, azure file dynamic provision does not work from this version\nerror logs:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 8s persistentvolume-controller Failed to provision volume with StorageClass \"azurefile\": Couldn't create secret secrets is forbidden: User \"system:serviceaccount:kube-syste m:persistent-volume-binder\" cannot create secrets in the namespace \"default\" Warning ProvisioningFailed 8s persistentvolume-controller Failed to provision volume with StorageClass \"azurefile\": failed to find a matching storage account Related issues\n azure file PVC need secrets create permission for persistent-volume-binder  Workaround:\n Add a ClusterRole and ClusterRoleBinding for azure file dynamic privision  kubectl create -f https://raw.githubusercontent.com/andyzhangx/Demo/master/aks-engine/rbac/azure-cloud-provider-deployment.yaml  delete the original PVC and recreate PVC  Fix\n PR in acs-engine: fix azure file dynamic provision permission issue  3. Azure file support on Sovereign Cloud Azure file on Sovereign Cloud is supported from v1.7.11, v1.8.0\n4. azure file dynamic provision failed due to cluster name length issue Issue details: k8s cluster name length must be less than 16 characters, otherwise following error will be received when creating dynamic privisioning azure file pvc, this bug exists in [v1.7.0, v1.7.10]:\n Note: check cluster-name by running grep cluster-name /etc/kubernetes/manifests/kube-controller-manager.yaml on master node\n persistentvolume-controller Warning ProvisioningFailed Failed to provision volume with StorageClass \"azurefile\": failed to find a matching storage account Fix\n PR Fix share name generation in azure file provisioner     k8s version fixed version     v1.7 1.7.11   v1.8 1.8.0   v1.9 1.9.0    5. azure file dynamic provision failed due to no storage account in current resource group Issue details:\nWhen create an azure file PVC, there will be error if there is no storage account in current resource group, error info would be like following:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 10s (x5 over 1m) persistentvolume-controller Failed to provision volume with StorageClass \"azurefile-premium\": failed to find a matching storage account Related issues\n failed to create azure file pvc if there is no storage account in current resource group  Workaround: specify a storage account in azure file dynamic provision, you should make sure the specified storage account is in the same resource group as your k8s cluster. In AKS, the specified storage account should be in shadow resource group(naming as MC_+{RESOUCE-GROUP-NAME}+{CLUSTER-NAME}+{REGION}) which contains all resources of your aks cluster.\nFix\n PR fix the create azure file pvc failure if there is no storage account in current resource group     k8s version fixed version     v1.7 1.7.14   v1.8 1.8.9   v1.9 1.9.4   v1.10 1.10.0    6. azure file plugin on Windows does not work after node restart Issue details: azure file plugin on Windows does not work after node restart, this is due to New-SmbGlobalMapping cmdlet has lost account name/key after reboot\nRelated issues\n azure file plugin on Windows does not work after node restart  Workaround:\n delete the original pod with azure file mount create the pod again  Fix\n PR fix azure file plugin failure issue on Windows after node restart     k8s version fixed version     v1.7 not support in upstream   v1.8 1.8.10   v1.9 1.9.7   v1.10 1.10.0    7. file permission could not be changed using azure file, e.g. postgresql error logs when running postgresql on azure file plugin:\ninitdb: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted fixing permissions on existing directory /var/lib/postgresql/data Issue details: azure file plugin is using cifs/SMB protocol, file/dir permission could not be changed after mounting\nWorkaround:\nUse mountOptions with dir_mode, file_mode set as 0777:\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:azurefileprovisioner:kubernetes.io/azure-filemountOptions:- dir_mode=0777- file_mode=0777 follow detailed config here\n Related issues Persistent Volume Claim permissions\n8. Could not delete pod with AzureFile volume if storage account key changed Issue details:\n kubelet fails to umount azurefile volume when there is azure file connection, below is an easy repro:  create a pod with azure file mount regenerate the account key of the storage account delete the pod, and the pod will never be deleted due to UnmountVolume.TearDown error    error logs\nnestedpendingoperations.go:263] Operation for \"\\\"kubernetes.io/azure-file/cc5c86cd-422a-11e8-91d7-000d3a03ee84-myvolume\\\" (\\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\")\" failed. No retries permitted until 2018-04-17 10:35:40.240272223 +0000 UTC m=+1185722.391925424 (durationBeforeRetry 500ms). Error: \"UnmountVolume.TearDown failed for volume \\\"myvolume\\\" (UniqueName: \\\"kubernetes.io/azure-file/cc5c86cd-422a-11e8-91d7-000d3a03ee84-myvolume\\\") pod \\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\" (UID: \\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\") : Error checking if path exists: stat /var/lib/kubelet/pods/cc5c86cd-422a-11e8-91d7-000d3a03ee84/volumes/kubernetes.io~azure-file/myvolume: resource temporarily unavailable ... kubelet_volumes.go:128] Orphaned pod \"380b02f3-422b-11e8-91d7-000d3a03ee84\" found, but volume paths are still present on disk Workaround:\nmanually umount the azure file mount path on the agent node and then the pod will be deleted right after that\nsudo umount /var/lib/kubelet/pods/cc5c86cd-422a-11e8-91d7-000d3a03ee84/volumes/kubernetes.io~azure-file/myvolume Fix\n PR Fix bug:Kubelet failure to umount mount points     k8s version fixed version     v1.7 no fix(no cherry-pick fix is allowed)   v1.8 1.8.8   v1.9 1.9.7   v1.10 1.10.0    Related issues\n UnmountVolume.TearDown fails for AzureFile volume, locks up node Kubelet failure to umount glusterfs mount points  9. Long latency compared to disk when handling lots of small files Related issues\n azurefile is very slow Can’t roll out Wordpress chart with PV on AzureFile  10. allow access from selected network setting on storage account will break azure file dynamic provisioning When set allow access from selected network on storage account and will get following error when creating a file share by k8s:\npersistentvolume-controller (combined from similar events): Failed to provision volume with StorageClass \"azurefile\": failed to create share kubernetes-dynamic-pvc-xxx in account xxx: failed to create file share, err: storage: service returned error: StatusCode=403, ErrorCode=AuthorizationFailure, ErrorMessage=This request is not authorized to perform this operation. That’s because k8s persistentvolume-controller is on master node which is not in the selected network, and that’s why it could not create file share on that storage account.\nWorkaround:\nuse azure file static provisioning instead\n create azure file share in advance, and then provide storage account and file share name in k8s, here is an example  Related issues\n Azure Files PV AuthorizationFailure when using advanced networking   11. azure file remount on Windows in same node would fail Issue details:\nIf user delete a pod with azure file mount in deployment and it would probably schedule a pod on same node, azure file mount will fail since New-SmbGlobalMapping command would fail if file share is already mounted on the node.\nerror logs\nError logs would be like following:\nE0118 08:15:52.041014 2112 nestedpendingoperations.go:267] Operation for \"\\\"kubernetes.io/azure-file/42c0ea39-1af9-11e9-8941-000d3af95268-pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\\\" (\\\"42c0ea39-1af9-11e9-8941-000d3af95268\\\")\" failed. No retries permitted until 2019-01-18 08:15:53.0410149 +0000 GMT m=+732.446642701 (durationBeforeRetry 1s). Error: \"MountVolume.SetUp failed for volume \\\"pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\\\" (UniqueName: \\\"kubernetes.io/azure-file/42c0ea39-1af9-11e9-8941-000d3af95268-pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\\\") pod \\\"deployment-azurefile-697f98d559-6zrlf\\\" (UID: \\\"42c0ea39-1af9-11e9-8941-000d3af95268\\\") : azureMount: SmbGlobalMapping failed: exit status 1, only SMB mount is supported now, output: \\\"New-SmbGlobalMapping : Generic failure \\\\r\\\\nAt line:1 char:190\\\\r\\\\n+ ... ser, $PWord;New-SmbGlobalMapping -RemotePath $Env:smbremotepath -Cred ...\\\\r\\\\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\r\\\\n + CategoryInfo : NotSpecified: (MSFT_SmbGlobalMapping:ROOT/Microsoft/...mbGlobalMapping) [New-SmbGlobalMa \\\\r\\\\n pping], CimException\\\\r\\\\n + FullyQualifiedErrorId : HRESULT 0x80041001,New-SmbGlobalMapping\\\\r\\\\n \\\\r\\\\n\\\"\" Fix\n PR fix smb remount issue on Windows     k8s version fixed version     v1.10 no fix   v1.11 1.11.8   v1.12 1.12.6   v1.13 1.13.4   v1.14 1.14.0    Related issues\n azure file remount on Windows in same node would fail Mounting volume to pods fails randomly  12. update azure file secret if azure storage account key changed Issue details: There would be azure file mount failure if azure storage account key changed\nWorkaround: User needs to update azurestorageaccountkey field manually in azure file secret(secret name format: azure-storage-account-{storage-account-name}-secret in default namespace):\nkubectl delete secret azure-storage-account-{storage-account-name}-secret kubectl create secret generic azure-storage-account-{storage-account-name}-secret --from-literal azurestorageaccountname=... --from-literal azurestorageaccountkey=\"...\" --type=Opaque  make sure there is no \\r in the account name and key, here is a failed case\n  delete original pod(may use --force --grace-period=0) and wait a few minutes for new pod retry azure file mount  13. Create Azure Files PV AuthorizationFailure when using advanced networking Issue details:\nWhen create an azure file PV using advanced networking, user may hit following error:\nerr: storage: service returned error: StatusCode=403, ErrorCode=AuthorizationFailure, ErrorMessage=This request is not authorized to perform this operation Before api-version 2019-06-01, create file share action is considered as data-path operation, since 2019-06-01, it would be considered as control-path operation, not blocked by advanced networking any more.\nRelated issues\n Azure Files PV AuthorizationFailure when using advanced networking Azure Files PV AuthorizationFailure when using advanced networking  Fix\n PR Switch to use AzureFile management SDK     k8s version fixed version     v1.18 no fix   v1.19 1.19.0    Workaround:\nShut down the advanced networking when create azure file PV.\n14. initial delay(5s) in mounting azure file Issue details:\nWhen starting pods with AFS volumes, there is an initial delay of five seconds until the pod is transitioning from the “Scheduled” state. The reason for this is that currently the volume mounting happens inside a wait.Poll which will initially wait a specified interval(currently 5 seconds) before execution. This issue is introduced by PR fix: azure file mount timeout issue with v1.15.11+, v1.16.8+, v1.17.4+, v1.18.0+\nFix\n initial delay(5s) when starting Pods with Azure File volumes  Fix\n PR fix: initial delay in mounting azure disk \u0026 file     k8s version fixed version     v1.15 no fix   v1.16 1.16.14   v1.17 1.17.10   v1.18 1.18.7   v1.19 1.19.0    ","categories":"","description":"","excerpt":" azure file plugin known issues  Recommended stable version for azure …","ref":"/cloud-provider-azure/faq/known-issues/azurefile/","tags":"","title":"AzureFile CSI Driver Known Issues"},{"body":" Cloud Provider Azure An Azure implementation of the Kubernetes Cloud Provider. Get Started   Contribute              Contributions welcome!  We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Join our slack channel!  Please join #provider-azure in Kubernetes slack workspace.\nRead more …\n   Check out release notes!  For announcement of latest features, etc.\nRead more …\n    ","categories":"","description":"","excerpt":" Cloud Provider Azure An Azure implementation of the Kubernetes Cloud …","ref":"/cloud-provider-azure/","tags":"","title":"Cloud Provider Azure"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cloud-provider-azure/contribute/","tags":"","title":"Contribution"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cloud-provider-azure/development/","tags":"","title":"Development Guide"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cloud-provider-azure/example/","tags":"","title":"Example"},{"body":"What is Cloud Provider Azure? A Kubernetes Cloud Provider consists of two parts: a provider-specified cloud-controller-manager (or kube-controller-manager for in-tree version) and a provider-specified implementation of Kubernetes cloud provider interface. Currently, the Azure cloud-controller-manager is outside of Kubernetes repo and the cloud provider interface implementation is in pkg/provider.\nThe cloud-controller-manager is a Kubernetes control plane component which embeds cloud-specific control logic. It lets you link your cluster into your cloud provider’s API, and separates out the components that interact with that cloud platform from components that just interact with your cluster.\nBy decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.\nWhat is the difference between in-tree and out-of-tree cloud provider? In-tree cloud providers are the providers we develop \u0026 release in the main Kubernetes repository. This results in embedding the knowledge and context of each cloud provider into most of the Kubernetes components. This enables more native integrations such as the kubelet requesting information about itself via a metadata service from the cloud provider.\nOut-of-tree cloud providers are providers that can be developed, built, and released independent of Kubernetes core. This requires deploying a new component called the cloud-controller-manager which is responsible for running all the cloud specific controllers that were previously run in the kube-controller-manager.\nWhich one is recommended? We recommend using the in-tree cloud provider at this time because it’s out-of-tree counterpart is not 100% ready. However, out-of-tree cloud provider will become the No.1 pick in the near future.\n","categories":"","description":"","excerpt":"What is Cloud Provider Azure? A Kubernetes Cloud Provider consists of …","ref":"/cloud-provider-azure/faq/","tags":"","title":"FAQ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cloud-provider-azure/install/","tags":"","title":"Deploy Cloud Provider Azure"},{"body":"Prerequisite   An azure service principal\nPlease follow this guide for creating an azure service principal The service principal should either have:\n Contributor permission of a subscription Contributor permission of a resource group. In this case, please create the resource group first    Docker daemon enabled\n  How to run Kubernetes e2e tests locally  Prepare dependency project    aks-engine\nBinary downloads for the latest version of aks-engine for are available on Github. Download AKS Engine for your operating system, extract the binary and copy it to your $PATH.\nOn macOS, you can install aks-engine with Homebrew. Run the command brew install Azure/aks-engine/aks-engine to do so. You can install Homebrew following the instructions.\nOn Windows, you can install aks-engine via Chocolatey by executing the command choco install aks-engine. You can install Chocolatey following the instructions.\nOn Linux, it could also be installed by following commands:\n$ curl -o get-akse.sh https://raw.githubusercontent.com/Azure/aks-engine/master/scripts/get-akse.sh $ chmod 700 get-akse.sh $ ./get-akse.sh   Kubernetes\nThis serves as E2E tests case source, it should be located at $GOPATH/src/k8s.io/kubernetes.\ncd $GOPATH/src go get -d k8s.io/kubernetes   kubectl\nKubectl allows you to run command against Kubernetes cluster, which is also used for deploying CSI plugins. You can follow here to install kubectl. e.g. on Linux\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/    Build docker image azure-cloud-controller-manager and push it to your docker image repository.\ngit clone https://github.com/kubernetes-sigs/cloud-provider-azure $GOPATH/src/sigs.k8s.io/cloud-provider-azure cd $GOPATH/src/sigs.k8s.io/cloud-provider-azure export IMAGE_REGISTRY=\u003cusername\u003e export IMAGE_TAG=\u003ctag\u003e make image make push # or manually `docker push`   Deploy a Kubernetes cluster with the above azure-cloud-controller-manager image.\nTo deploy a cluster, export all the required environmental variables first and then invoke make deploy:\nexport RESOURCE_GROUP_NAME=\u003cresource group name\u003e export AZURE_LOCATION=\u003clocation\u003e export AZURE_SUBSCRIPTION_ID=\u003csubscription ID\u003e export AZURE_CLIENT_ID=\u003cclient id\u003e export AZURE_CLIENT_SECRET=\u003cclient secret\u003e export AZURE_TENANT_ID=\u003ctenant id\u003e export USE_CSI_DEFAULT_STORAGECLASS=\u003ctrue/false\u003e export K8S_RELEASE_VERSION=\u003ck8s release version\u003e export CCM_IMAGE=\u003cimage of the cloud controller manager\u003e export CNM_IMAGE=\u003cimage of the cloud node manager\u003e make deploy To connect the cluster:\nexport KUBECONFIG=$GOPATH/src/sigs.k8s.io/cloud-provider-azure/_output/$(ls -t _output | head -n 1)/kubeconfig/kubeconfig.$LOCATION.json kubectl cluster-info   To check out more of the deployed cluster , replace kubectl cluster-info with other kubectl commands. To further debug and diagnose cluster problems, use kubectl cluster-info dump\nGet kubetest binary  go get -u k8s.io/test-infra/kubetest Run E2E tests  Please first ensure the kubernetes project locates at $GOPATH/src/k8s.io/kubernetes, the e2e tests will be built from that location.\ncd $GOPATH/src/k8s.io/kubernetes make WHAT='test/e2e/e2e.test' make WHAT=cmd/kubectl make ginkgo export KUBERNETES_PROVIDER=azure export KUBERNETES_CONFORMANCE_TEST=y export KUBERNETES_CONFORMANCE_PROVIDER=azure export CLOUD_CONFIG=$GOPATH/src/sigs.k8s.io/cloud-provider-azure/tests/k8s-azure/manifest/azure.json # some test cases require ssh configurations export KUBE_SSH_KEY_PATH=path/to/ssh/privatekey export KUBE_SSH_USER={ssh_user} # Replace the test_args with your own. kubetest --test --provider=local --check-version-skew=false --test_args='--ginkgo.focus=Port\\sforwarding' ","categories":"","description":"Kubernetes E2E tests guidance.\n","excerpt":"Kubernetes E2E tests guidance.\n","ref":"/cloud-provider-azure/development/e2e/e2e-tests/","tags":"","title":"Kubernetes E2E tests"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cloud-provider-azure/blog/","tags":"","title":"Release Notes"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cloud-provider-azure/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/cloud-provider-azure/topics/","tags":"","title":"Topics"}]